import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. C·∫§U H√åNH T·ªêI ∆ØU (CH·ªêNG TREO CHO 27GB RAM)
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
TOP_1M_FILE = "top-1m.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"

# C·∫•u h√¨nh lu·ªìng c√¢n b·∫±ng ƒë·ªÉ kh√¥ng l√†m ngh·∫Ωn CPU Kali
NUM_PROCESSES = 10          
PAGES_PER_PROCESS = 6       
# GI·∫¢M TIMEOUT xu·ªëng 30s ƒë·ªÉ b·ªè qua nhanh c√°c URL ch·∫øt (tr√°nh ƒë·ª©ng m√°y)
TIMEOUT_MS = 30000          

# N·∫°p danh s√°ch Top 1M
TOP_1M_DOMAINS = set()
if os.path.exists(TOP_1M_FILE):
    try:
        top_df = pd.read_csv(TOP_1M_FILE, header=None)
        col_index = 1 if len(top_df.columns) > 1 else 0
        TOP_1M_DOMAINS = set(top_df[col_index].str.lower().dropna().tolist())
    except: pass

if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        f.write(f"--- PHI√äN L√ÄM VI·ªÜC: {datetime.now()} ---\n")

# =========================================================
# 2. LOGIC Tƒ®NH (8 Features) - GI·ªÆ NGUY√äN ƒê·∫¶Y ƒê·ª¶
# =========================================================
def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        subdomain_parts = domain.split('.')[:-2]
        return {
            'domainEntropy': calculate_entropy(domain),
            'V23_Entropy_Subdomain': calculate_entropy(".".join(subdomain_parts)) if subdomain_parts else 0,
            'hasIp': 1 if re.match(r'^\d{1,3}(\.\d{1,3}){3}$', domain) else 0,
            'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
            'domainLength': len(domain),
            'Subdomain_Level': len(subdomain_parts),
            'IsHTTPS': 1 if url.startswith('https') else 0,
            'Is_Top_1M_Domain': 1 if domain in TOP_1M_DOMAINS else 0
        }
    except: return {f: 0 for f in ['domainEntropy', 'V23_Entropy_Subdomain', 'hasIp', 'numHypRatio', 'domainLength', 'Subdomain_Level', 'IsHTTPS', 'Is_Top_1M_Domain']}

# =========================================================
# 3. LOGIC ƒê·ªòNG (10 Features) - GI·ªÆ NGUY√äN ƒê·∫¶Y ƒê·ª¶
# =========================================================
async def apply_stealth(page):
    await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => false});")

async def extract_dynamic(page, url):
    domain = urlparse(url).netloc.lower()
    try:
        content = (await page.content()).lower()
        title = (await page.title()) or ""
        
        # V9: Hidden IFrame
        v9_val = 0
        for f in page.frames[1:]:
            try:
                el = await f.frame_element()
                if el and not await el.is_visible(): v9_val = 1; break
            except: continue

        # Outlink & Social
        all_links = await page.query_selector_all('a')
        outlinks, has_social = 0, 0
        for link in all_links:
            try:
                href = await link.get_attribute('href')
                if href and href.startswith('http'):
                    if domain not in href: outlinks += 1
                    if any(s in href for s in ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com', 'youtube.com']):
                        has_social = 1
            except: continue
        
        return {
            'Outlink_Ratio': outlinks / len(all_links) if all_links else 0,
            'HasExternalFormSubmit': 1 if await page.query_selector(f'form[action*="http"]:not([action*="{domain}"])') else 0,
            'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
            'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
            'HasSocialNet': has_social,
            'HasCopyrightInfo': 1 if any(x in content for x in ['¬©', 'copyright', 'all rights reserved']) else 0,
            'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
            'V9_Has_Hidden_IFrame': v9_val,
            'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0,
            'V4_DNS_Volatility_Count': 1 if len(domain) > 25 or domain.count('-') > 1 else 0
        }
    except: return {f: 0 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}

# =========================================================
# 4. H√ÄM X·ª¨ L√ù (GI·∫¢M T·∫¢I ƒê·ªÇ CH·ªêNG TREO)
# =========================================================
async def process_url(url, label, context, semaphore):
    static_data = extract_static(url)
    status, reason, final_row = "FAILED", "UNKNOWN", {}

    async with semaphore:
        page = await context.new_page()
        # CH·∫∂N MEDIA ƒê·ªÇ GI·∫¢M T·∫¢I CPU/RAM TUY·ªÜT ƒê·ªêI
        await page.route("**/*.{png,jpg,jpeg,gif,svg,css,woff,woff2,ttf,otf,pdf,zip}", lambda route: route.abort())
        
        await apply_stealth(page)
        try:
            # wait_until="domcontentloaded" gi√∫p tho√°t s·ªõm khi c·∫•u tr√∫c trang ƒë√£ s·∫µn s√†ng
            await page.goto(url, timeout=TIMEOUT_MS, wait_until="domcontentloaded")
            
            content = (await page.content()).lower()
            if "just a moment" in content or "cloudflare" in content:
                reason = "Cloudflare_Blocked"
            else:
                await asyncio.sleep(2) # Ch·ªù 2s ƒë·ªÉ c√°c script c·∫ßn thi·∫øt th·ª±c thi
                dynamic_data = await extract_dynamic(page, url)
                final_row = {**static_data, **dynamic_data, 'URL': url, 'label': label}
                status, reason = "SUCCESS", "Done"
        except Exception as e:
            reason = f"Timeout/Err: {str(e)[:20]}"
        finally:
            await page.close()

    # Log ngay l·∫≠p t·ª©c
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {reason}\n")
        f.flush(); os.fsync(f.fileno())
    return final_row if status == "SUCCESS" else None

# =========================================================
# 5. ƒêI·ªÄU PH·ªêI TI·∫æN TR√åNH
# =========================================================
async def run_batch(df_batch):
    async with async_playwright() as p:
        # headless=True ƒë·ªÉ ch·∫°y ng·∫ßm, c·ª±c nh·∫π m√°y
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0")
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        tasks = [process_url(row['URL'], row['label'], context, semaphore) for _, row in df_batch.iterrows()]
        results = await asyncio.gather(*tasks)
        await browser.close()
        return [r for r in results if r is not None]

def process_entry(df_batch):
    try: return asyncio.run(run_batch(df_batch))
    except: return []

if __name__ == "__main__":
    start_time = time.time()
    if not os.path.exists(INPUT_FILE):
        print(f"‚ùå Thi·∫øu file {INPUT_FILE}!"); exit()

    df_all = pd.read_csv(INPUT_FILE)
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total = len(df_todo)
    print(f"üöÄ B·∫ÆT ƒê·∫¶U: {total} URL. RAM 27GB. ƒêang ch·∫°y ng·∫ßm ch·ªëng treo...")

    batch_size = NUM_PROCESSES * PAGES_PER_PROCESS
    for i in range(0, total, batch_size):
        chunk = df_todo.iloc[i : i + batch_size]
        sub_batches = [b for b in np.array_split(chunk, NUM_PROCESSES) if not b.empty]
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            batch_results = []
            for res in executor.map(process_entry, sub_batches):
                batch_results.extend(res)
        
        if batch_results:
            df_res = pd.DataFrame(batch_results)
            df_res.to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE), encoding="utf-8")
        
        done = min(i + batch_size, total)
        elapsed = time.time() - start_time
        speed = done / elapsed
        print(f"‚úÖ ƒê√£ l∆∞u {done}/{total} | T·ªëc ƒë·ªô: {speed:.2f} URL/s")
