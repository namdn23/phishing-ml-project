import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. Cáº¤U HÃŒNH & Náº P Dá»® LIá»†U TOP 1M (DÃ€NH CHO 27GB RAM)
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
TOP_1M_FILE = "top-1m.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"

NUM_PROCESSES = 8          # Sá»‘ tiáº¿n trÃ¬nh song song (CPU)
PAGES_PER_PROCESS = 5      # Sá»‘ tab má»—i tiáº¿n trÃ¬nh (RAM 27GB dÆ° sá»©c cÃ¢n)
TIMEOUT = 60000            # 60 giÃ¢y

# Náº¡p danh sÃ¡ch Top 1M vÃ o Set Ä‘á»ƒ tra cá»©u cá»±c nhanh O(1)
TOP_1M_DOMAINS = set()
if os.path.exists(TOP_1M_FILE):
    try:
        print(f"â³ Äang náº¡p {TOP_1M_FILE}...")
        # Náº¿u file cÃ³ dáº¡ng: 1,google.com -> dÃ¹ng index 1. Náº¿u chá»‰ cÃ³ domain -> dÃ¹ng index 0
        top_df = pd.read_csv(TOP_1M_FILE, header=None)
        col_index = 1 if len(top_df.columns) > 1 else 0
        TOP_1M_DOMAINS = set(top_df[col_index].str.lower().dropna().tolist())
        print(f"âœ… ÄÃ£ náº¡p {len(TOP_1M_DOMAINS)} domain uy tÃ­n.")
    except Exception as e:
        print(f"âš ï¸ Lá»—i náº¡p file Top 1M: {e}")
else:
    print(f"âŒ KhÃ´ng tÃ¬m tháº¥y {TOP_1M_FILE}. Äáº·c trÆ°ng Is_Top_1M sáº½ máº·c Ä‘á»‹nh lÃ  0.")

if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        f.write(f"--- PHIÃŠN LÃ€M VIá»†C: {datetime.now()} ---\n")

# =========================================================
# 2. LOGIC TRÃCH XUáº¤T TÄ¨NH (8 Features)
# =========================================================
def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        subdomain_parts = domain.split('.')[:-2]
        
        return {
            'domainEntropy': calculate_entropy(domain),
            'V23_Entropy_Subdomain': calculate_entropy(".".join(subdomain_parts)) if subdomain_parts else 0,
            'hasIp': 1 if re.match(r'^\d{1,3}(\.\d{1,3}){3}$', domain) else 0,
            'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
            'domainLength': len(domain),
            'Subdomain_Level': len(subdomain_parts),
            'IsHTTPS': 1 if url.startswith('https') else 0,
            'Is_Top_1M_Domain': 1 if domain in TOP_1M_DOMAINS else 0
        }
    except:
        return {f: 0 for f in ['domainEntropy', 'V23_Entropy_Subdomain', 'hasIp', 'numHypRatio', 'domainLength', 'Subdomain_Level', 'IsHTTPS', 'Is_Top_1M_Domain']}

# =========================================================
# 3. LOGIC TRÃCH XUáº¤T Äá»˜NG (10 Features)
# =========================================================
async def apply_stealth(page):
    await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => false});")

async def extract_dynamic(page, url):
    domain = urlparse(url).netloc.lower()
    try:
        content = (await page.content()).lower()
        title = (await page.title()) or ""
        
        # V9: Hidden IFrame
        v9_val = 0
        for f in page.frames[1:]:
            try:
                el = await f.frame_element()
                if el and not await el.is_visible(): v9_val = 1; break
            except: continue

        # Outlink Ratio & Social Net
        all_links = await page.query_selector_all('a')
        outlinks = 0
        has_social = 0
        for link in all_links:
            try:
                href = await link.get_attribute('href')
                if href and href.startswith('http'):
                    if domain not in href: outlinks += 1
                    if any(s in href for s in ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com', 'youtube.com']):
                        has_social = 1
            except: continue
        
        outlink_ratio = outlinks / len(all_links) if all_links else 0

        # Form & Password
        has_password = 1 if await page.query_selector('input[type="password"]') else 0
        ext_form = 1 if await page.query_selector(f'form[action*="http"]:not([action*="{domain}"])') else 0
        
        # Meta & Copyright
        has_desc = 1 if await page.query_selector('meta[name="description"]') else 0
        has_copy = 1 if any(x in content for x in ['Â©', 'copyright', 'all rights reserved']) else 0
        
        # Score Match
        title_match = 1 if domain.split('.')[0] in title.lower() else 0

        return {
            'Outlink_Ratio': outlink_ratio,
            'HasExternalFormSubmit': ext_form,
            'HasPasswordField': has_password,
            'DomainTitleMatchScore': title_match,
            'HasSocialNet': has_social,
            'HasCopyrightInfo': has_copy,
            'HasDescription': has_desc,
            'V9_Has_Hidden_IFrame': v9_val,
            'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0,
            'V4_DNS_Volatility_Count': 1 if len(domain) > 25 or domain.count('-') > 1 else 0
        }
    except:
        return {f: 0 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}

# =========================================================
# 4. Xá»¬ LÃ CHÃNH & ÄA LUá»’NG
# =========================================================
async def process_url(url, label, context, semaphore):
    static_data = extract_static(url)
    status, reason, final_row = "FAILED", "UNKNOWN", {}

    async with semaphore:
        page = await context.new_page()
        await apply_stealth(page)
        try:
            await page.set_extra_http_headers({"Referer": "https://www.google.com/"})
            response = await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
            
            content = (await page.content()).lower()
            if "just a moment" in content or "cloudflare" in content:
                reason = "Cloudflare_Blocked"
            elif response and response.status >= 400:
                reason = f"HTTP_{response.status}"
            else:
                await asyncio.sleep(random.uniform(2, 4))
                dynamic_data = await extract_dynamic(page, url)
                final_row = {**static_data, **dynamic_data, 'URL': url, 'label': label}
                status, reason = "SUCCESS", "Done"
        except Exception as e:
            reason = f"Err: {str(e).splitlines()[0][:30]}"
        finally:
            await page.close()

    # LOGGING CÆ¯á» NG Bá»¨C (Flush to disk)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {reason}\n")
        f.flush(); os.fsync(f.fileno())

    return final_row if status == "SUCCESS" else None

async def run_batch(df_batch):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False, args=['--no-sandbox', '--disable-setuid-sandbox'])
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0")
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        tasks = [process_url(row['URL'], row['label'], context, semaphore) for _, row in df_batch.iterrows()]
        results = await asyncio.gather(*tasks)
        await browser.close()
        return [r for r in results if r is not None]

def process_entry(df_batch):
    try: return asyncio.run(run_batch(df_batch))
    except: return []

if __name__ == "__main__":
    start_time = time.time()
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ Thiáº¿u file {INPUT_FILE}!"); exit()

    df_all = pd.read_csv(INPUT_FILE)
    
    # Checkpoint logic
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total = len(df_todo)
    print(f"ğŸš€ Báº¯t Ä‘áº§u: {total} URL. RAM 27GB tá»‘i Æ°u 40 tabs song song.")

    batch_size = NUM_PROCESSES * 5
    for i in range(0, total, batch_size):
        chunk = df_todo.iloc[i : i + batch_size]
        sub_batches = [b for b in np.array_split(chunk, NUM_PROCESSES) if not b.empty]
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            batch_results = []
            for res in executor.map(process_entry, sub_batches):
                batch_results.extend(res)
        
        if batch_results:
            df_res = pd.DataFrame(batch_results)
            file_exists = os.path.isfile(OUTPUT_FILE)
            df_res.to_csv(OUTPUT_FILE, mode='a', index=False, header=not file_exists, encoding="utf-8")
        
        done = i + len(chunk)
        speed = done / (time.time() - start_time)
        eta = str(timedelta(seconds=int((total - done) / speed))) if speed > 0 else "N/A"
        print(f"âœ… ÄÃ£ lÆ°u {done}/{total} | Tá»‘c Ä‘á»™: {speed:.2f} URL/s | ETA: {eta}")
