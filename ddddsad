import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import gc
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. Cáº¤U HÃŒNH & DANH SÃCH USER-AGENTS (VÆ¯á»¢T CHáº¶N)
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
TOP_1M_FILE = "top-1m.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"

NUM_PROCESSES = 20          
PAGES_PER_PROCESS = 10       
TIMEOUT_MS = 60000

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0"
]

TOP_1M_DOMAINS = set()
if os.path.exists(TOP_1M_FILE):
    try:
        top_df = pd.read_csv(TOP_1M_FILE, header=None)
        col = 1 if len(top_df.columns) > 1 else 0
        TOP_1M_DOMAINS = set(top_df[col].str.lower().dropna().tolist())
    except: pass

# =========================================================
# 2. LOGIC 18 FEATURES (GIá»® NGUYÃŠN 100%)
# =========================================================
def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        sub = domain.split('.')[:-2]
        return {
            'domainEntropy': calculate_entropy(domain),
            'V23_Entropy_Subdomain': calculate_entropy(".".join(sub)) if sub else 0,
            'hasIp': 1 if re.match(r'^\d{1,3}(\.\d{1,3}){3}$', domain) else 0,
            'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
            'domainLength': len(domain),
            'Subdomain_Level': len(sub),
            'IsHTTPS': 1 if url.startswith('https') else 0,
            'Is_Top_1M_Domain': 1 if domain in TOP_1M_DOMAINS else 0
        }
    except: return {f: 0 for f in range(8)}

async def extract_dynamic(page, url):
    domain = urlparse(url).netloc.lower()
    try:
        content = (await page.content()).lower()
        title = (await page.title()) or ""
        v9_val = 0
        for f in page.frames[1:]:
            try:
                el = await f.frame_element()
                if el and not await el.is_visible(): v9_val = 1; break
            except: continue
        all_links = await page.query_selector_all('a')
        outlinks, has_social = 0, 0
        for link in all_links:
            try:
                href = await link.get_attribute('href')
                if href and href.startswith('http'):
                    if domain not in href: outlinks += 1
                    if any(s in href for s in ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com', 'youtube.com']):
                        has_social = 1
            except: continue
        return {
            'Outlink_Ratio': outlinks / len(all_links) if all_links else 0,
            'HasExternalFormSubmit': 1 if await page.query_selector(f'form[action*="http"]:not([action*="{domain}"])') else 0,
            'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
            'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
            'HasSocialNet': has_social,
            'HasCopyrightInfo': 1 if any(x in content for x in ['Â©', 'copyright', 'all rights reserved']) else 0,
            'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
            'V9_Has_Hidden_IFrame': v9_val,
            'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0,
            'V4_DNS_Volatility_Count': 1 if len(domain) > 25 or domain.count('-') > 1 else 0
        }
    except: return {f: 0 for f in range(10)}

# =========================================================
# 3. HÃ€M Xá»¬ LÃ (VÆ¯á»¢T CHáº¶N NÃ‚NG CAO)
# =========================================================
async def process_url(url, label, context, semaphore, tab_idx):
    static_data = extract_static(url)
    await asyncio.sleep(tab_idx * 0.4) # TÄƒng giÃ£n cÃ¡ch lÃªn 0.4s cho an toÃ n máº¡ng

    async with semaphore:
        # Má»—i tab cÃ³ má»™t viewport (Ä‘á»™ phÃ¢n giáº£i) khÃ¡c nhau Ä‘á»ƒ trÃ¡nh fingerprinting
        page = await context.new_page()
        await page.set_viewport_size({"width": random.randint(1280, 1920), "height": random.randint(720, 1080)})
        
        # VÆ°á»£t cháº·n navigator.webdriver
        await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => false});")
        
        await page.route("**/*.{png,jpg,jpeg,gif,svg,css,woff,woff2,ttf,otf,pdf,zip,mp4,js.map,json}", lambda r: r.abort())
        
        status, reason, final_row = "FAILED", "Timeout", None
        try:
            await page.goto(url, timeout=TIMEOUT_MS, wait_until="domcontentloaded")
            await asyncio.sleep(random.uniform(2, 4)) # Nghá»‰ ngáº«u nhiÃªn giá»‘ng ngÆ°á»i tháº­t
            dynamic_data = await extract_dynamic(page, url)
            final_row = {**static_data, **dynamic_data, 'URL': url, 'label': label}
            status, reason = "SUCCESS", "Done"
        except Exception as e:
            reason = f"Err: {str(e)[:15]}"
        finally:
            await page.close()

    with open(LOG_FILE, "a") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {reason}\n")
    return final_row

async def run_batch(df_batch, start_offset):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=[
            '--no-sandbox', 
            '--disable-dev-shm-usage',
            '--disable-blink-features=AutomationControlled', # VÆ°á»£t cháº·n bot detection
            '--disable-infobars'
        ])
        # Sá»­ dá»¥ng ngáº«u nhiÃªn User-Agent cho má»—i cá»¥m Process
        context = await browser.new_context(user_agent=random.choice(USER_AGENTS))
        
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        tasks = [process_url(row['URL'], row['label'], context, semaphore, start_offset + i) 
                 for i, (_, row) in enumerate(df_batch.iterrows())]
        
        results = await asyncio.gather(*tasks)
        await browser.close()
        return [r for r in results if r is not None]

def process_entry(data_tuple):
    df_batch, start_offset = data_tuple
    try: return asyncio.run(run_batch(df_batch, start_offset))
    finally: gc.collect()

# =========================================================
# 4. CHÆ¯Æ NG TRÃŒNH CHÃNH + ETA (HOÃ€N CHá»ˆNH)
# =========================================================
if __name__ == "__main__":
    start_session_time = time.time()
    if not os.path.exists(INPUT_FILE): exit("âŒ Thiáº¿u file Ä‘áº§u vÃ o!")

    df_all = pd.read_csv(INPUT_FILE)
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total_todo = len(df_todo)
    print(f"ğŸš€ KHá»I CHáº Y: {total_todo} URL | 200 Tabs (Ultra Stealth) | RAM 27GB")

    batch_size = NUM_PROCESSES * PAGES_PER_PROCESS
    for i in range(0, total_todo, batch_size):
        chunk = df_todo.iloc[i : i + batch_size]
        sub_batches = []
        split_data = np.array_split(chunk, NUM_PROCESSES)
        offset = 0
        for s in split_data:
            if not s.empty:
                sub_batches.append((s, offset))
                offset += len(s)
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            batch_results = []
            for res in executor.map(process_entry, sub_batches):
                batch_results.extend(res)
        
        if batch_results:
            pd.DataFrame(batch_results).to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
        
        # Dá»n dáº¹p cache Kali Linux
        if os.name == 'posix': os.system('sync; echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null')
        
        done = min(i + batch_size, total_todo)
        speed = done / (time.time() - start_session_time)
        eta = str(timedelta(seconds=int((total_todo - done) / speed))) if speed > 0 else "0"
        
        print(f"âœ… Tiáº¿n Ä‘á»™: {done}/{total_todo} | Speed: {speed:.2f} URL/s | ETA: {eta}")

    print(f"ğŸ XONG! Tá»•ng thá»i gian: {str(timedelta(seconds=int(time.time() - start_session_time)))}")
