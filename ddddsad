import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. Cáº¤U HÃŒNH Tá»I Æ¯U (TÄ‚NG Tá»C CHO 27GB RAM)
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
TOP_1M_FILE = "top-1m.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"

# TÄƒng sá»‘ luá»“ng Ä‘á»ƒ táº­n dá»¥ng RAM dÆ° thá»«a
NUM_PROCESSES = 12          # TÄƒng tá»« 8 lÃªn 12
PAGES_PER_PROCESS = 8       # TÄƒng tá»« 5 lÃªn 8 (Tá»•ng ~96 tabs song song)
TIMEOUT = 45000             # Giáº£m xuá»‘ng 45s Ä‘á»ƒ bá» qua cÃ¡c web quÃ¡ cháº­m

# Náº¡p danh sÃ¡ch Top 1M
TOP_1M_DOMAINS = set()
if os.path.exists(TOP_1M_FILE):
    try:
        top_df = pd.read_csv(TOP_1M_FILE, header=None)
        col_index = 1 if len(top_df.columns) > 1 else 0
        TOP_1M_DOMAINS = set(top_df[col_index].str.lower().dropna().tolist())
        print(f"âœ… ÄÃ£ náº¡p {len(TOP_1M_DOMAINS)} domain uy tÃ­n.")
    except Exception as e: print(f"âš ï¸ Lá»—i náº¡p Top 1M: {e}")

if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        f.write(f"--- PHIÃŠN LÃ€M VIá»†C: {datetime.now()} ---\n")

# =========================================================
# 2. LOGIC TÄ¨NH (8 Features) - GIá»® NGUYÃŠN
# =========================================================
def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        subdomain_parts = domain.split('.')[:-2]
        return {
            'domainEntropy': calculate_entropy(domain),
            'V23_Entropy_Subdomain': calculate_entropy(".".join(subdomain_parts)) if subdomain_parts else 0,
            'hasIp': 1 if re.match(r'^\d{1,3}(\.\d{1,3}){3}$', domain) else 0,
            'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
            'domainLength': len(domain),
            'Subdomain_Level': len(subdomain_parts),
            'IsHTTPS': 1 if url.startswith('https') else 0,
            'Is_Top_1M_Domain': 1 if domain in TOP_1M_DOMAINS else 0
        }
    except: return {f: 0 for f in range(8)}

# =========================================================
# 3. LOGIC Äá»˜NG (10 Features) - KHÃ”NG LÆ¯á»¢C Bá»Ž
# =========================================================
async def apply_stealth(page):
    await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => false});")

async def extract_dynamic(page, url):
    domain = urlparse(url).netloc.lower()
    try:
        content = (await page.content()).lower()
        title = (await page.title()) or ""
        
        # V9: Hidden IFrame
        v9_val = 0
        for f in page.frames[1:]:
            try:
                el = await f.frame_element()
                if el and not await el.is_visible(): v9_val = 1; break
            except: continue

        # Outlink & Social
        all_links = await page.query_selector_all('a')
        outlinks = 0
        has_social = 0
        for link in all_links:
            try:
                href = await link.get_attribute('href')
                if href and href.startswith('http'):
                    if domain not in href: outlinks += 1
                    if any(s in href for s in ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com', 'youtube.com']):
                        has_social = 1
            except: continue
        
        return {
            'Outlink_Ratio': outlinks / len(all_links) if all_links else 0,
            'HasExternalFormSubmit': 1 if await page.query_selector(f'form[action*="http"]:not([action*="{domain}"])') else 0,
            'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
            'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
            'HasSocialNet': has_social,
            'HasCopyrightInfo': 1 if any(x in content for x in ['Â©', 'copyright', 'all rights reserved']) else 0,
            'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
            'V9_Has_Hidden_IFrame': v9_val,
            'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0,
            'V4_DNS_Volatility_Count': 1 if len(domain) > 25 or domain.count('-') > 1 else 0
        }
    except: return {f: 0 for f in range(10)}

# =========================================================
# 4. HÃ€M Xá»¬ LÃ (Tá»I Æ¯U Tá»C Äá»˜)
# =========================================================
async def process_url(url, label, context, semaphore):
    static_data = extract_static(url)
    status, reason, final_row = "FAILED", "UNKNOWN", {}

    async with semaphore:
        page = await context.new_page()
        # --- Cáº¢I TIáº¾N Tá»C Äá»˜: Cháº·n táº£i media/css Ä‘á»ƒ giáº£m táº£i ---
        await page.route("**/*.{png,jpg,jpeg,gif,svg,css,woff,woff2,ttf,otf}", lambda route: route.abort())
        
        await apply_stealth(page)
        try:
            # wait_until="commit" Ä‘á»ƒ báº¯t Ä‘áº§u xá»­ lÃ½ ngay khi server pháº£n há»“i
            response = await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
            
            content = (await page.content()).lower()
            if "just a moment" in content or "cloudflare" in content:
                reason = "Cloudflare_Blocked"
            else:
                # Giáº£m thá»i gian chá» xuá»‘ng 1.5s - 3s cho nhanh
                await asyncio.sleep(random.uniform(1.5, 3))
                dynamic_data = await extract_dynamic(page, url)
                final_row = {**static_data, **dynamic_data, 'URL': url, 'label': label}
                status, reason = "SUCCESS", "Done"
        except Exception as e: reason = f"Err: {str(e)[:20]}"
        finally: await page.close()

    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {reason}\n")
        f.flush(); os.fsync(f.fileno())
    return final_row if status == "SUCCESS" else None

# =========================================================
# 5. ÄIá»€U PHá»I (GIá»® NGUYÃŠN Cáº¤U TRÃšC Gá»C)
# =========================================================
async def run_batch(df_batch):
    async with async_playwright() as p:
        # headless=True Ä‘á»ƒ cháº¡y nhanh hÆ¡n ná»¯a, Ä‘á»•i False náº¿u muá»‘n xem trÃ¬nh duyá»‡t
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage'])
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0")
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        tasks = [process_url(row['URL'], row['label'], context, semaphore) for _, row in df_batch.iterrows()]
        results = await asyncio.gather(*tasks)
        await browser.close()
        return [r for r in results if r is not None]

def process_entry(df_batch):
    try: return asyncio.run(run_batch(df_batch))
    except: return []

if __name__ == "__main__":
    start_time = time.time()
    df_all = pd.read_csv(INPUT_FILE)
    
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total = len(df_todo)
    print(f"ðŸš€ Tá»C Äá»˜ CAO: {total} URL. RAM 27GB. Song song {NUM_PROCESSES * PAGES_PER_PROCESS} tabs.")

    # Batch lá»›n hÆ¡n Ä‘á»ƒ giáº£m thá»i gian khá»Ÿi Ä‘á»™ng browser
    batch_size = NUM_PROCESSES * PAGES_PER_PROCESS
    for i in range(0, total, batch_size):
        chunk = df_todo.iloc[i : i + batch_size]
        sub_batches = [b for b in np.array_split(chunk, NUM_PROCESSES) if not b.empty]
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            batch_results = []
            for res in executor.map(process_entry, sub_batches):
                batch_results.extend(res)
        
        if batch_results:
            df_res = pd.DataFrame(batch_results)
            df_res.to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE), encoding="utf-8")
        
        done = min(i + batch_size, total)
        speed = done / (time.time() - start_time)
        print(f"âœ… {done}/{total} | Speed: {speed:.2f} URL/s | ETA: {str(timedelta(seconds=int((total-done)/speed)) if speed>0 else 0)}")
