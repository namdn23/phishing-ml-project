import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. Cáº¤U HÃŒNH Tá»I Æ¯U (CHá»NG TREO CHO 27GB RAM)
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
TOP_1M_FILE = "top-1m.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"

# Cáº¥u hÃ¬nh song song tá»‘i Æ°u cho RAM 27GB
NUM_PROCESSES = 10          
PAGES_PER_PROCESS = 6       # Tá»•ng cá»™ng 60 tab cháº¡y cÃ¹ng lÃºc
TIMEOUT_MS = 30000          # Äá»£i tá»‘i Ä‘a 30s cho má»—i trang

# Náº¡p danh sÃ¡ch Top 1M Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng 'Is_Top_1M_Domain'
TOP_1M_DOMAINS = set()
if os.path.exists(TOP_1M_FILE):
    try:
        top_df = pd.read_csv(TOP_1M_FILE, header=None)
        col_index = 1 if len(top_df.columns) > 1 else 0
        TOP_1M_DOMAINS = set(top_df[col_index].str.lower().dropna().tolist())
    except: pass

if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        f.write(f"--- PHIÃŠN LÃ€M VIá»†C: {datetime.now()} ---\n")

# =========================================================
# 2. LOGIC TÄ¨NH (8 Features) - Äáº¦Y Äá»¦ 100%
# =========================================================
def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        subdomain_parts = domain.split('.')[:-2]
        return {
            'domainEntropy': calculate_entropy(domain),
            'V23_Entropy_Subdomain': calculate_entropy(".".join(subdomain_parts)) if subdomain_parts else 0,
            'hasIp': 1 if re.match(r'^\d{1,3}(\.\d{1,3}){3}$', domain) else 0,
            'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
            'domainLength': len(domain),
            'Subdomain_Level': len(subdomain_parts),
            'IsHTTPS': 1 if url.startswith('https') else 0,
            'Is_Top_1M_Domain': 1 if domain in TOP_1M_DOMAINS else 0
        }
    except: return {f: 0 for f in ['domainEntropy', 'V23_Entropy_Subdomain', 'hasIp', 'numHypRatio', 'domainLength', 'Subdomain_Level', 'IsHTTPS', 'Is_Top_1M_Domain']}

# =========================================================
# 3. LOGIC Äá»˜NG (10 Features) - Äáº¦Y Äá»¦ 100%
# =========================================================
async def apply_stealth(page):
    await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => false});")

async def extract_dynamic(page, url):
    domain = urlparse(url).netloc.lower()
    try:
        content = (await page.content()).lower()
        title = (await page.title()) or ""
        
        # 1. V9: Hidden IFrame
        v9_val = 0
        for f in page.frames[1:]:
            try:
                el = await f.frame_element()
                if el and not await el.is_visible(): v9_val = 1; break
            except: continue

        # 2. Outlink Ratio & 3. Social Net
        all_links = await page.query_selector_all('a')
        outlinks, has_social = 0, 0
        for link in all_links:
            try:
                href = await link.get_attribute('href')
                if href and href.startswith('http'):
                    if domain not in href: outlinks += 1
                    if any(s in href for s in ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com', 'youtube.com']):
                        has_social = 1
            except: continue
        
        return {
            'Outlink_Ratio': outlinks / len(all_links) if all_links else 0,
            'HasExternalFormSubmit': 1 if await page.query_selector(f'form[action*="http"]:not([action*="{domain}"])') else 0,
            'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
            'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
            'HasSocialNet': has_social,
            'HasCopyrightInfo': 1 if any(x in content for x in ['Â©', 'copyright', 'all rights reserved']) else 0,
            'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
            'V9_Has_Hidden_IFrame': v9_val,
            'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0,
            'V4_DNS_Volatility_Count': 1 if len(domain) > 25 or domain.count('-') > 1 else 0
        }
    except: return {f: 0 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}

# =========================================================
# 4. HÃ€M Xá»¬ LÃ URL (CHá»NG TREO & CHáº¶N RÃC)
# =========================================================
async def process_url(url, label, context, semaphore):
    static_data = extract_static(url)
    status, reason, final_row = "FAILED", "UNKNOWN", {}

    async with semaphore:
        page = await context.new_page()
        # CHáº¶N MEDIA Äá»‚ GIáº¢M Táº¢I CPU/RAM TUYá»†T Äá»I -> CHá»NG Äá»¨NG MÃY
        await page.route("**/*.{png,jpg,jpeg,gif,svg,css,woff,woff2,ttf,otf,pdf,zip}", lambda route: route.abort())
        
        await apply_stealth(page)
        try:
            # wait_until="domcontentloaded" Ä‘á»ƒ xá»­ lÃ½ ngay khi cÃ³ cáº¥u trÃºc HTML
            await page.goto(url, timeout=TIMEOUT_MS, wait_until="domcontentloaded")
            await asyncio.sleep(2) # Chá» script áº©n náº¡p
            dynamic_data = await extract_dynamic(page, url)
            final_row = {**static_data, **dynamic_data, 'URL': url, 'label': label}
            status, reason = "SUCCESS", "Done"
        except Exception as e:
            reason = f"Timeout/Err: {str(e)[:20]}"
        finally:
            await page.close()

    # Log ngay láº­p tá»©c Ä‘á»ƒ checkpoint
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {reason}\n")
        f.flush(); os.fsync(f.fileno())
    return final_row if status == "SUCCESS" else None

# =========================================================
# 5. ÄIá»€U PHá»I (BATCH PROCESSING)
# =========================================================
async def run_batch(df_batch):
    async with async_playwright() as p:
        # headless=True cháº¡y ngáº§m giÃºp tiáº¿t kiá»‡m RAM cá»±c lá»›n
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0")
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        tasks = [process_url(row['URL'], row['label'], context, semaphore) for _, row in df_batch.iterrows()]
        results = await asyncio.gather(*tasks)
        await browser.close()
        return [r for r in results if r is not None]

def process_entry(df_batch):
    try: return asyncio.run(run_batch(df_batch))
    except: return []

# =========================================================
# 6. CHÆ¯Æ NG TRÃŒNH CHÃNH + TÃNH ETA
# =========================================================
if __name__ == "__main__":
    start_session_time = time.time()
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file {INPUT_FILE}!"); exit()

    df_all = pd.read_csv(INPUT_FILE)
    
    # Äá»c checkpoint tá»« log
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total_todo = len(df_todo)
    print(f"ğŸš€ KHá»I CHáº Y: {total_todo} URL cÃ²n láº¡i. Äang cháº¡y ngáº§m tiáº¿t kiá»‡m tÃ i nguyÃªn...")

    batch_size = NUM_PROCESSES * PAGES_PER_PROCESS
    processed_count = 0

    for i in range(0, total_todo, batch_size):
        chunk = df_todo.iloc[i : i + batch_size]
        sub_batches = [b for b in np.array_split(chunk, NUM_PROCESSES) if not b.empty]
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            batch_results = []
            for res in executor.map(process_entry, sub_batches):
                batch_results.extend(res)
        
        if batch_results:
            df_res = pd.DataFrame(batch_results)
            df_res.to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE), encoding="utf-8")
        
        # Cáº­p nháº­t tá»‘c Ä‘á»™ vÃ  Dá»± Ä‘oÃ¡n thá»i gian (ETA)
        processed_count += len(chunk)
        elapsed = time.time() - start_session_time
        speed = processed_count / elapsed # URL/s
        
        remaining = total_todo - processed_count
        eta_sec = remaining / speed if speed > 0 else 0
        eta_str = str(timedelta(seconds=int(eta_sec)))
        
        print(f"âœ… Xong: {processed_count}/{total_todo} | Tá»‘c Ä‘á»™: {speed:.2f} URL/s | CÃ²n láº¡i khoáº£ng: {eta_str}")

    print(f"ğŸ HOÃ€N THÃ€NH! Tá»•ng thá»i gian: {str(timedelta(seconds=int(time.time() - start_session_time)))}")
