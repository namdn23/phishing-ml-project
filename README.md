import pandas as pd
import numpy as np
import os
import time
import math
import io
import socket
import re
import tldextract
import ssl
import subprocess
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright
import imagehash
from PIL import Image
from bs4 import BeautifulSoup
from collections import Counter

# =================================================================
# I. C·∫§U H√åNH H·ªÜ TH·ªêNG
# =================================================================
RAW_CSV_FILE = 'PhiUSIIL_Phishing_URL_Dataset.csv'
TEMP_LOG_FILE = 'extraction_checkpoint.csv'
FINAL_OUTPUT = 'PhiUSIIL_Extracted_Full.csv'

# TH√îNG S·ªê V·∫¨N H√ÄNH
MAX_WORKERS = 20    # S·ªë lu·ªìng x·ª≠ l√Ω song song
CHUNK_SIZE = 25     # Ghi file sau m·ªói 25 URL m·ªói lu·ªìng
TIMEOUT_MS = 10000  # 10 gi√¢y cho m·ªói URL
TARGET_PHASH = imagehash.hex_to_hash('9880e61f1c7e0c4f')

OLD_KEEP_COLS = [
    'URL', 'NoOfDegitsInURL', 'HasDescription', 'HasSocialNet', 'HasPasswordField', 
    'HasSubmitButton', 'HasExternalFormSubmit', 'DomainTitleMatchScore', 
    'IsHTTPS', 'HasCopyrightInfo', 'label'
]

# =================================================================
# II. C√îNG C·ª§ H·ªÜ TH·ªêNG
# =================================================================
def clear_linux_cache():
    """Gi·∫£i ph√≥ng b·ªô nh·ªõ ƒë·ªám RAM trong Kali Linux"""
    try:
        os.system('sync; echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null')
    except:
        pass

def get_entropy(text):
    if not text or len(text) == 0: return 0.0
    probs = [count/len(text) for count in Counter(text).values()]
    return -sum(p * math.log2(p) for p in probs) / 8.0

def get_tls_issuer(hostname):
    try:
        context = ssl.create_default_context()
        context.check_hostname, context.verify_mode = False, ssl.CERT_NONE
        with socket.create_connection((hostname, 443), timeout=3) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert = ssock.getpeercert()
                issuer = dict(x[0] for x in cert['issuer'])
                return issuer.get('organizationName', 'Unknown')
    except: return 'None'

# =================================================================
# III. LOGIC TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG (ƒê√É B·ªé ALARM)
# =================================================================
def extract_full_features(page, url):
    # Kh·ªüi t·∫°o 14 ƒë·∫∑c tr∆∞ng k·ªπ thu·∫≠t
    res = {k: 0.0 for k in [
        'V10_HTTP_Extraction_Success', 'V11_WHOIS_Extraction_Success', 'V1_PHash_Distance', 
        'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score', 
        'V8_Total_IFrames', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 
        'V3_Domain_Age_Days', 'V4_DNS_Volatility_Count', 'Is_Top_1M_Domain', 
        'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain'
    ]}
    try:
        # Ch·∫∑n c√°c t√†i nguy√™n n·∫∑ng ƒë·ªÉ tƒÉng t·ªëc
        page.route("**/*", lambda r: r.abort() if r.request.resource_type in ["image", "media", "font", "stylesheet"] else r.continue_())
        
        ext = tldextract.extract(url)
        domain = f"{ext.domain}.{ext.suffix}"
        
        res['V22_IP_Subdomain_Pattern'] = 1 if re.search(r'\d+\.\d+', ext.subdomain) else 0
        res['V23_Entropy_Subdomain'] = get_entropy(ext.subdomain)
        res['Is_Top_1M_Domain'] = 1 if ext.domain in ['google', 'facebook', 'microsoft', 'apple', 'amazon'] else 0
        
        try: res['V4_DNS_Volatility_Count'] = len(socket.gethostbyname_ex(domain)[2])
        except: pass
        
        res['V5_TLS_Issuer_Reputation'] = 1.0 if get_tls_issuer(domain) != 'None' else 0.0

        # Truy c·∫≠p trang web
        page.goto(url, timeout=TIMEOUT_MS, wait_until="commit") 
        res['V10_HTTP_Extraction_Success'] = 1
        
        # Ch·ª•p ·∫£nh v√† t√≠nh pHash (L·ªói th√¨ m·∫∑c ƒë·ªãnh 0.5)
        try:
            img_bytes = page.screenshot(timeout=3000)
            img = Image.open(io.BytesIO(img_bytes)).convert('L')
            res['V1_PHash_Distance'] = (imagehash.phash(img) - TARGET_PHASH) / 64.0
        except:
            res['V1_PHash_Distance'] = 0.5

        # Ph√¢n t√≠ch n·ªôi dung DOM
        content = page.content()
        soup = BeautifulSoup(content, 'html.parser')
        full_text = soup.get_text().strip()

        depths = [len(list(t.parents)) for t in soup.find_all(True)]
        res['V2_Layout_Similarity'] = np.clip(1.0 - (max(depths or [0])/40.0), 0, 1)
        
        js_code = "".join([s.text for s in soup.find_all('script')])
        res['V6_JS_Entropy'] = get_entropy(js_code)
        
        words, sentences = full_text.split(), re.split(r'[.!?]+', full_text)
        res['V7_Text_Readability_Score'] = np.clip(len(words)/(len(sentences) or 1) / 20.0, 0, 1)
        
        iframes = soup.find_all('iframe')
        res['V8_Total_IFrames'] = len(iframes)
        res['V9_Has_Hidden_IFrame'] = 1 if any('none' in str(f.get('style','')).lower() for f in iframes) else 0
        res['V11_WHOIS_Extraction_Success'] = 1
    except:
        res['V10_HTTP_Extraction_Success'] = 0
        res['V1_PHash_Distance'] = 0.5 
    return res

def thread_worker(chunk_df):
    results = []
    p = sync_playwright().start() 
    browser = None
    try:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox", "--disable-dev-shm-usage"])
        context = browser.new_context(viewport={'width': 1280, 'height': 720})
        page = context.new_page()
        for _, row in chunk_df.iterrows():
            data = extract_full_features(page, row['URL'])
            data['URL_KEY'] = str(row['URL'])
            results.append(data)
    finally:
        if browser: browser.close()
        p.stop()
    return results

# =================================================================
# IV. QU·∫¢N L√ù CH∆Ø∆†NG TR√åNH
# =================================================================
def main():
    start_session_time = time.time()
    
    # ƒê·ªçc file dataset g·ªëc
    df_raw = pd.read_csv(RAW_CSV_FILE, usecols=OLD_KEEP_COLS)
    
    # Checkpoint: ƒê·ªçc nh·ªØng URL ƒë√£ x·ª≠ l√Ω t·ª´ file checkpoint 15 c·ªôt
    if os.path.exists(TEMP_LOG_FILE):
        print("üîÑ ƒêang qu√©t file checkpoint...")
        check_df = pd.read_csv(TEMP_LOG_FILE, usecols=['URL_KEY'], on_bad_lines='skip').dropna()
        processed_urls = set(check_df['URL_KEY'].astype(str))
    else:
        processed_urls = set()

    df_todo = df_raw[~df_raw['URL'].astype(str).isin(processed_urls)]
    total_todo = len(df_todo)
    
    if total_todo == 0:
        print("‚úÖ To√†n b·ªô URL ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ho√†n t·∫•t!"); return

    print(f"üöÄ Ti·∫øp t·ª•c t·ª´: {len(processed_urls)} | C√≤n l·∫°i: {total_todo} URL")
    chunks = [df_todo[i:i + CHUNK_SIZE] for i in range(0, total_todo, CHUNK_SIZE)]
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(thread_worker, c): c for c in chunks}
        for i, future in enumerate(as_completed(futures), 1):
            try:
                batch = future.result()
                if batch:
                    with open(TEMP_LOG_FILE, 'a', encoding='utf-8') as f:
                        pd.DataFrame(batch).to_csv(f, header=f.tell()==0, index=False)
                        f.flush()
                        os.fsync(f.fileno())
                
                if i % 10 == 0:
                    clear_linux_cache()
                    os.system('pkill -f chromium > /dev/null 2>&1')

                done = min(i * CHUNK_SIZE, total_todo)
                elapsed = time.time() - start_session_time
                speed = done / elapsed if elapsed > 0 else 0
                remaining_sec = (total_todo - done) / speed if speed > 0 else 0
                
                print(f"‚ûú [{datetime.now().strftime('%H:%M:%S')}] {len(processed_urls)+done}/{len(df_raw)} "
                      f"| {speed:.1f} URL/s | C√≤n: {str(timedelta(seconds=int(remaining_sec)))}")
            except Exception as e:
                print(f"‚ö†Ô∏è Batch l·ªói: {e}")

    # G·ªôp file cu·ªëi c√πng
    print("\nüîÑ ƒêang ti·∫øn h√†nh g·ªôp d·ªØ li·ªáu cu·ªëi c√πng v√†o file Full...")
    df_new = pd.read_csv(TEMP_LOG_FILE, on_bad_lines='skip').drop_duplicates('URL_KEY')
    df_final = pd.merge(df_raw, df_new, left_on='URL', right_on='URL_KEY', how='inner')
    df_final.drop(columns=['URL_KEY']).to_csv(FINAL_OUTPUT, index=False)
    print(f"‚úÖ HO√ÄN T·∫§T TH√ÄNH C√îNG! File l∆∞u t·∫°i: {FINAL_OUTPUT}")

if __name__ == "__main__":
    main()
