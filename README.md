# =================================================================
# run_extraction_final_merge.py - S·ª¨ D·ª§NG PLAYWRIGHT
# =================================================================
import pandas as pd
import numpy as np
import os
import requests
from bs4 import BeautifulSoup
import tldextract
import time
import re
from datetime import datetime
# --- Thay th·∫ø Selenium b·∫±ng Playwright ---
from playwright.sync_api import sync_playwright, Playwright, Browser, Page, TimeoutError as PlaywrightTimeoutError
# ----------------------------------------
import imagehash
from PIL import Image
import io
import sys
import math
from collections import Counter
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
import socket
import ssl

# T·∫Øt c·∫£nh b√°o SSL v√† t·∫Øt ghi file bytecode
requests.packages.urllib3.disable_warnings()
sys.dont_write_bytecode = True

# --- 1. C·∫§U H√åNH V√Ä H·∫∞NG S·ªê ---
RAW_CSV_FILE = 'PhiUSIIL_Phishing_URL_Dataset.csv'
OUTPUT_CSV_FILE = 'merged_extracted_data_final_processed.csv' # File Output CU·ªêI C√ôNG ƒë√£ merge v√† x·ª≠ l√Ω l·ªói
DETAILED_LOG_FILE = 'temp_new_features_log.csv'

MAX_WORKERS = 8
BUFFER_SIZE = 500

# Hash m·∫´u (v√≠ d·ª•: Google Search Page)
TARGET_PHASH = imagehash.hex_to_hash('9880e61f1c7e0c4f')

# === ‚ùó KHAI B√ÅO PLAYWRIGHT (Kh√¥ng c·∫ßn ƒë∆∞·ªùng d·∫´n Binary) ‚ùó ===
# ƒê√É LO·∫†I B·ªé CHROME_DRIVER_PATH V√Ä CHROME_BINARY_PATH
# =====================================================

# C√°c ƒë·∫∑c tr∆∞ng T·ªíN T·∫†I trong file th√¥ c·∫ßn ƒë∆∞·ª£c C·∫¨P NH·∫¨T/GHI ƒê√à (T√≠nh l·∫°i cho ch√≠nh x√°c)
OVERWRITE_FEATURES = [
    'NoOfDegitsInURL', 'HasDescription', 'HasSocialNet', 'HasPasswordField', 'HasSubmitButton',
    'HasExternalFormSubmit', 'DomainTitleMatchScore', 'IsHTTPS', 'HasCopyrightInfo', 'label'
]

# C√°c ƒë·∫∑c tr∆∞ng M·ªöI c·∫ßn ƒë∆∞·ª£c tr√≠ch xu·∫•t (Kh√¥ng t·ªìn t·∫°i trong file th√¥)
NEW_FEATURES = [
    'V10_HTTP_Extraction_Success', 'V11_WHOIS_Extraction_Success', 'V1_PHash_Distance',
    'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score', 'V8_Total_IFrames',
    'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 'V3_Domain_Age_Days',
    'V4_DNS_Volatility_Count', 'Is_Top_1M_Domain', 'V22_IP_Subdomain_Pattern',
    'V23_Entropy_Subdomain'
]

# --- DANH S√ÅCH M·ªöI ƒê·ªÇ X·ª¨ L√ù L·ªñI (QUAN TR·ªåNG) ---
# C√°c ƒë·∫∑c tr∆∞ng ƒë·ªông c√≥ gi√° tr·ªã m·∫∑c ƒë·ªãnh 0.0 ho·∫∑c 0.5 khi HTTP/Content FAIL (V10=0)
DYNAMIC_CONTENT_FEATURES = [
    'V1_PHash_Distance', 'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score',
    'V8_Total_IFrames', 'V9_Has_Hidden_IFrame', 'HasDescription', 'HasSocialNet',
    'HasPasswordField', 'HasSubmitButton', 'HasExternalFormSubmit', 'DomainTitleMatchScore',
    'HasCopyrightInfo', 'V5_TLS_Issuer_Reputation'
]

# C√°c ƒë·∫∑c tr∆∞ng b·ªã ·∫£nh h∆∞·ªüng khi WHOIS FAIL (V11=0)
WHOIS_FEATURES = ['V3_Domain_Age_Days']
# ------------------------------------------------

# Th·ª© t·ª± Output trong file log m·ªõi (DETAILED_LOG_FILE)
FEATURE_ORDER_LOG = ['url'] + OVERWRITE_FEATURES + NEW_FEATURES

# USER-AGENTS CHO NG·ª§Y TRANG BOT
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1'
]
# -------------------------------------

# =================================================================
# II. L·ªöP TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG (FEATURE EXTRACTOR) - D√ôNG PLAYWRIGHT
# =================================================================

class FeatureExtractor:
    WHOIS_TIMEOUT: int = 5
    RENDER_TIMEOUT: int = 20 # 20 gi√¢y cho Playwright

    def __init__(self, url: str):
        self.url: str = self._normalize_url(url)
        self.features: Dict[str, Any] = {'url': url}
        self.response: Optional[requests.Response] = None
        self.soup: Optional[BeautifulSoup] = None
        self.current_domain: Optional[str] = None
        self.http_extraction_successful: bool = False
        self.visual_extraction_successful: bool = False
        # D·ªØ li·ªáu Top 1M (v√≠ d·ª•)
        self.top_1m_data: Dict[str, bool] = {'google': True, 'facebook': True, 'microsoft': True}

    def _normalize_url(self, url: str) -> str:
        if not url.startswith('http'):
            return 'http://' + url
        return url

    # --- Tƒ®NH: WHOIS, DNS, TLS (Gi·ªØ nguy√™n) ---
    def _parse_whois_date(self, date_data: Any) -> Optional[datetime]:
        if isinstance(date_data, list): date_data = date_data[0]
        if date_data is None or date_data == 'None': return None
        if isinstance(date_data, datetime): return date_data.replace(tzinfo=None)

        if isinstance(date_data, str):
            clean_date_data = re.sub(r'(\s+\w{3}|\s+\+\d{2}:\d{2})$', '', date_data).strip()
            formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%Y-%m-%dT%H:%M:%SZ', '%Y%m%d', '%d-%b-%Y', '%m/%d/%Y']
            for fmt in formats:
                try:
                    dt = datetime.strptime(clean_date_data, fmt)
                    return dt.replace(tzinfo=None)
                except ValueError:
                    continue
        return None

    def _calculate_entropy(self, text: str) -> float:
        if not text: return 0.0
        p, lns = Counter(text), float(len(text))
        entropy = -sum(count / lns * math.log2(count / lns) for count in p.values())
        # Chu·∫©n h√≥a v·ªÅ [0, 1] (chia cho log2(alphabet_size), gi·∫£ s·ª≠ 256 k√Ω t·ª±)
        return entropy / 8.0

    def _calculate_dns_volatility(self, domain: str) -> int:
        try:
            ip_list = socket.gethostbyname_ex(domain)[2]
            return len(set(ip_list)) - 1
        except socket.gaierror:
            return -1
        except Exception:
            return 0

    def _calculate_tls_issuer_rep(self) -> float:
        if not self.url.startswith('https://'):
            return 0.0

        hostname = tldextract.extract(self.url).fqdn
        if not hostname: return 0.5

        try:
            context = ssl.create_default_context()
            with socket.create_connection((hostname, 443), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert = ssock.getpeercert()

            issuer = next((item[0][1] for item in cert['issuer'] if item[0][0] == 'organizationName'), '').lower()

            TRUSTED_ISSUERS = ['google', 'amazon', 'digicert', 'cloudflare', 'globalsign']

            if 'lets encrypt' in issuer: return 0.7
            if any(name in issuer for name in TRUSTED_ISSUERS): return 0.95

            return 0.2

        except socket.gaierror: return 0.0
        except ssl.SSLError: return 0.1
        except TimeoutError: return 0.5
        except Exception: return 0.0


    # --- TR√çCH XU·∫§T URL & WHOIS (Gi·ªØ nguy√™n) ---
    def _get_url_domain_features(self) -> None:
        import whois

        # M·ªöI: V11_WHOIS_Extraction_Success
        self.features['V11_WHOIS_Extraction_Success'] = 0

        url_no_protocol = self.url.replace("http://", "").replace("https://", "")
        # GHI ƒê√à: NoOfDegitsInURL
        self.features['NoOfDegitsInURL'] = sum(c.isdigit() for c in url_no_protocol)

        domain_info = tldextract.extract(self.url)
        domain = f"{domain_info.domain}.{domain_info.suffix}"
        self.current_domain = domain_info.domain
        subdomain = domain_info.subdomain.lower()

        # M·ªöI: V22_IP_Subdomain_Pattern, V23_Entropy_Subdomain
        self.features['V22_IP_Subdomain_Pattern'] = 1 if re.search(r'\d+\.\d+\.\d+(\.\d+)?', subdomain) else 0
        self.features['V23_Entropy_Subdomain'] = self._calculate_entropy(subdomain)

        # M·ªöI: V4_DNS_Volatility_Count
        volatility_count = self._calculate_dns_volatility(domain)
        self.features['V4_DNS_Volatility_Count'] = max(0, volatility_count)

        domain_age_days = 0
        try:
            whois_info = whois.whois(domain, timeout=self.WHOIS_TIMEOUT)
            if isinstance(whois_info.domain_name, str) and 'not found' in whois_info.domain_name.lower():
                raise ValueError("Domain not found")

            creation_date = self._parse_whois_date(whois_info.creation_date)
            if creation_date:
                age = datetime.now().replace(tzinfo=None) - creation_date
                domain_age_days = age.days
                self.features['V11_WHOIS_Extraction_Success'] = 1
        except Exception:
            # N·∫øu WHOIS th·∫•t b·∫°i, ƒë·∫∑t tu·ªïi l√† 0 (ƒë·ªÉ tr√°nh thi√™n v·ªã Age, v√† V11=0 s·∫Ω b√°o hi·ªáu l·ªói)
            domain_age_days = 0
            self.features['V11_WHOIS_Extraction_Success'] = 0 # Ghi r√µ r√†ng V11=0 khi th·∫•t b·∫°i

        # M·ªöI: V3_Domain_Age_Days
        self.features['V3_Domain_Age_Days'] = max(0, domain_age_days)

        # GHI ƒê√à: IsHTTPS
        self.features['IsHTTPS'] = 1 if self.url.startswith('https://') else 0

        # M·ªöI: Is_Top_1M_Domain
        is_top_1m = 1 if self.current_domain and self.current_domain.lower() in self.top_1m_data else 0
        self.features['Is_Top_1M_Domain'] = is_top_1m

    # --- TRUY V·∫§N V√Ä PH√ÇN T√çCH N·ªòI DUNG (Gi·ªØ nguy√™n) ---
    def _fetch_url_content(self) -> None:
        # M·ªöI: V10_HTTP_Extraction_Success
        self.features['V10_HTTP_Extraction_Success'] = 0
        self.http_extraction_successful = False

        if '0.0.0.0' in self.url or '127.0.0.1' in self.url or '192.168.' in self.url:
            return

        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': 'https://www.google.com/'
        }

        try:
            # TƒÉng Timeout l√™n 40 gi√¢y ƒë·ªÉ x·ª≠ l√Ω c√°c trang ch·∫≠m
            self.response = requests.get(self.url, timeout=40, verify=False, allow_redirects=True, headers=headers)
            self.response.raise_for_status()
            self.soup = BeautifulSoup(self.response.content, 'html.parser')
            self.features['V10_HTTP_Extraction_Success'] = 1
            self.http_extraction_successful = True
        except requests.exceptions.RequestException:
            self.response = None
            self.soup = None

    # --- TR√çCH XU·∫§T C√ÅC ƒê·∫∂C TR∆ØNG HTML (Gi·ªØ nguy√™n) ---
    def _get_content_features(self) -> None:

        # ƒê·∫∂T GI√Å TR·ªä M·∫∂C ƒê·ªäNH cho c√°c ƒë·∫∑c tr∆∞ng ƒë·ªông c√≥ th·ªÉ b·ªã thi·∫øu (0.0 ho·∫∑c 0.5)
        # N·∫øu V10 = 0, c√°c gi√° tr·ªã n√†y s·∫Ω ƒë∆∞·ª£c gi·ªØ l·∫°i
        default_features = {
            # GHI ƒê√à
            'HasDescription': 0, 'HasSocialNet': 0, 'HasPasswordField': 0, 'HasSubmitButton': 0,
            'HasExternalFormSubmit': 0, 'DomainTitleMatchScore': 0.0, 'HasCopyrightInfo': 0,
            # M·ªöI
            'V8_Total_IFrames': 0, 'V9_Has_Hidden_IFrame': 0, 'V7_Text_Readability_Score': 0.0,
            'V6_JS_Entropy': 0.0,
            'V1_PHash_Distance': 0.5, # GI√Å TR·ªä M·∫∂C ƒê·ªäNH TH·∫§T B·∫†I RENDER
            'V2_Layout_Similarity': 0.5, # GI√Å TR·ªä M·∫∂C ƒê·ªäNH TH·∫§T B·∫†I RENDER
        }

        self.features.update(default_features)

        # M·ªöI: V5_TLS_Issuer_Reputation
        self.features['V5_TLS_Issuer_Reputation'] = self._calculate_tls_issuer_rep()

        if not self.soup:
            return # GI·ªÆ NGUY√äN GI√Å TR·ªä M·∫∂C ƒê·ªäNH

        # --- B·∫ÆT ƒê·∫¶U T√çNH TO√ÅN KHI HTTP TH√ÄNH C√îNG ---

        def _calculate_readability(text: str) -> float:
            sentences = len(re.split(r'[.!?]+', text))
            words = len(re.findall(r'\w+', text))
            syllables = words * 1.5
            if sentences == 0 or words == 0: return 50.0
            # Flesch-Kincaid Readability Score (ƒë∆°n gi·∫£n h√≥a)
            score = 206.835 - 1.015 * (words / sentences) - 84.6 * (syllables / words)
            return np.clip(score, 0.0, 100.0)

        def _extract_dom_form_features(soup: BeautifulSoup, current_domain: str) -> Dict[str, Any]:
            f: Dict[str, Any] = {}
            f['HasPasswordField'] = 1 if len(soup.find_all('input', type='password')) > 0 else 0
            f['HasSubmitButton'] = 1 if len(soup.find_all('input', type='submit') + soup.find_all('button', type='submit')) > 0 else 0

            external_form = 0
            for form in soup.find_all('form'):
                action = form.get('action')
                if action and action.startswith('http') and tldextract.extract(action).domain != current_domain:
                    external_form = 1
                    break
            f['HasExternalFormSubmit'] = external_form
            return f

        form_features_static = _extract_dom_form_features(self.soup, self.current_domain)
        self.features.update(form_features_static)

        # GHI ƒê√à: HasDescription, HasSocialNet
        description_tag = self.soup.find('meta', attrs={'name': 'description'})
        self.features['HasDescription'] = 1 if (description_tag and description_tag.get('content')) else 0
        social_links = self.soup.find_all('a', href=lambda href: href and ('facebook.com' in href or 'twitter.com' in href))
        self.features['HasSocialNet'] = 1 if len(social_links) > 0 else 0

        # GHI ƒê√à: DomainTitleMatchScore
        title_text = self.soup.title.string if self.soup.title and self.soup.title.string else ""
        domain_name = self.current_domain.lower() if self.current_domain else ""
        match_score = 0.0
        if domain_name and title_text:
            if domain_name in title_text.lower():
                match_score = 1.0
        self.features['DomainTitleMatchScore'] = np.clip(match_score, 0.0, 1.0)

        # GHI ƒê√à: HasCopyrightInfo
        copyright_text = self.soup.find(string=lambda text: text and 'copyright' in text.lower())
        self.features['HasCopyrightInfo'] = 1 if copyright_text else 0

        # M·ªöI: V8_Total_IFrames, V9_Has_Hidden_IFrame
        self.features['V8_Total_IFrames'] = len(self.soup.find_all('iframe'))
        hidden_iframe = self.soup.find('iframe', attrs={'style': lambda style: style and 'display:none' in style.lower()})
        if not hidden_iframe:
            hidden_iframe = self.soup.find('iframe', attrs={'width': '0', 'height': '0'})
        self.features['V9_Has_Hidden_IFrame'] = 1 if hidden_iframe else 0

        # M·ªöI: V7_Text_Readability_Score, V6_JS_Entropy
        page_text = self.soup.get_text(separator=' ', strip=True)
        self.features['V7_Text_Readability_Score'] = _calculate_readability(page_text)

        script_tags = self.soup.find_all('script')
        js_content = "".join(tag.string for tag in script_tags if tag.string)
        self.features['V6_JS_Entropy'] = self._calculate_entropy(js_content)


    # --- ƒê·ªòNG: TR√çCH XU·∫§T VISUAL V√Ä JAVASCRIPT (V1, V2) B·∫∞NG PLAYWRIGHT ---
    def _get_visual_and_complex_features(self) -> None:
        """S·ª≠ d·ª•ng Playwright ƒë·ªÉ render v√† tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng ƒë·ªông (V1, V2)."""

        # B·ªè qua n·∫øu HTTP tƒ©nh ƒë√£ th·∫•t b·∫°i
        if not self.http_extraction_successful:
            return

        # --- H√†m con (Gi·ªØ nguy√™n logic c·ªßa b·∫°n) ---
        def _calculate_phash_distance(image_data: bytes) -> float:
            try:
                image = Image.open(io.BytesIO(image_data)).convert('L')
                current_phash = imagehash.phash(image, hash_size=8)
                distance = current_phash - TARGET_PHASH
                return float(distance) / 64.0
            except Exception:
                return 0.5

        def _calculate_layout_similarity(dom_tree: BeautifulSoup) -> float:
            def find_max_depth(element: BeautifulSoup, current_depth: int = 0) -> int:
                max_d = current_depth
                for child in element.find_all(True, recursive=False):
                    max_d = max(max_d, find_max_depth(child, current_depth + 1))
                return max_d
            try:
                # T√¨m ƒë·ªô s√¢u DOM t·ªëi ƒëa, chu·∫©n h√≥a v√† tr·∫£ v·ªÅ (v√≠ d·ª•: 0.1 ƒë·∫øn 0.9)
                max_depth = find_max_depth(dom_tree)
                similarity = np.clip(1.0 - (max_depth / 20.0), 0.1, 0.9)
                return float(f"{similarity:.4f}")
            except Exception:
                return 0.5

        def _extract_dom_form_features_dynamic(soup: BeautifulSoup, current_domain: str) -> Dict[str, Any]:
            f: Dict[str, Any] = {}
            # GHI ƒê√à: HasPasswordField, HasSubmitButton
            f['HasPasswordField'] = 1 if len(soup.find_all('input', type='password')) > 0 else 0
            f['HasSubmitButton'] = 1 if len(soup.find_all('input', type='submit') + soup.find_all('button', type='submit')) > 0 else 0

            external_form = 0
            for form in soup.find_all('form'):
                action = form.get('action')
                if action and action.startswith('http') and tldextract.extract(action).domain != current_domain:
                    external_form = 1
                    break
            f['HasExternalFormSubmit'] = external_form
            return f
        # ----------------------------------------------------

        # --- B·∫ÆT ƒê·∫¶U V·ªöI PLAYWRIGHT ---
        browser: Optional[Browser] = None
        try:
            with sync_playwright() as p:
                # 1. Kh·ªüi t·∫°o Tr√¨nh duy·ªát (Chromium)
                # Playwright t·ª± ƒë·ªông x·ª≠ l√Ω c√°c c·ªù c·∫ßn thi·∫øt cho headless mode
                browser = p.chromium.launch(
                    headless=True,
                    args=[
                        "--disable-gpu",
                        "--no-sandbox", # Quan tr·ªçng cho m√¥i tr∆∞·ªùng Kali/Linux
                        f"--user-agent={random.choice(USER_AGENTS)}"
                    ]
                )

                # 2. M·ªü Trang v√† T·∫£i URL
                page: Page = browser.new_page()
                # Playwright d√πng mili gi√¢y
                page.set_default_timeout(self.RENDER_TIMEOUT * 1000)

                try:
                    # Ch·ªù cho ƒë·∫øn khi m·∫°ng kh√¥ng ho·∫°t ƒë·ªông (ho·∫∑c timeout)
                    # "networkidle" ƒë·∫£m b·∫£o trang web ƒë√£ load xong n·ªôi dung ƒë·ªông
                    page.goto(self.url, wait_until="networkidle")
                    self.visual_extraction_successful = True

                    # M·ªöI: V1_PHash_Distance
                    # Playwright get_screenshot_as_png() thay cho Selenium
                    screenshot_data: bytes = page.screenshot()
                    self.features['V1_PHash_Distance'] = _calculate_phash_distance(screenshot_data)

                    # M·ªöI: V2_Layout_Similarity
                    rendered_html: str = page.content()
                    rendered_soup: BeautifulSoup = BeautifulSoup(rendered_html, 'html.parser')
                    self.features['V2_Layout_Similarity'] = _calculate_layout_similarity(rendered_soup)

                    # C·∫≠p nh·∫≠t DOM features d·ª±a tr√™n Playwright (N·ªôi dung load sau JS)
                    dynamic_form_features = _extract_dom_form_features_dynamic(rendered_soup, self.current_domain)
                    self.features.update(dynamic_form_features)

                except PlaywrightTimeoutError as e:
                    # L·ªói Timeout
                    print(f"‚ö†Ô∏è Playwright Timeout khi x·ª≠ l√Ω {self.url} (qu√° {self.RENDER_TIMEOUT}s): {e}")
                    self.features['V1_PHash_Distance'] = 0.5
                    self.features['V2_Layout_Similarity'] = 0.5
                    pass

                except Exception as e:
                    # L·ªói chung trong qu√° tr√¨nh render (v√≠ d·ª•: Navigation Error, JS Error)
                    print(f"‚ö†Ô∏è L·ªói Playwright khi x·ª≠ l√Ω {self.url}: {e}")
                    self.features['V1_PHash_Distance'] = 0.5
                    self.features['V2_Layout_Similarity'] = 0.5
                    pass

                finally:
                    if browser: browser.close()

        except Exception as e_init:
            # L·ªói Kh·ªüi t·∫°o Playwright (v√≠ d·ª•: kh√¥ng t√¨m th·∫•y browser binary)
            print(f"‚ùå L·ªói Kh·ªüi t·∫°o Playwright/Browser: {e_init}")
            # ƒê·∫£m b·∫£o V1/V2 c√≥ gi√° tr·ªã m·∫∑c ƒë·ªãnh khi l·ªói n·∫∑ng
            self.features['V1_PHash_Distance'] = 0.5
            self.features['V2_Layout_Similarity'] = 0.5

    def get_all_features(self, label: int) -> Optional[Dict[str, Any]]:
        """Tr·∫£ v·ªÅ dictionary ch·ª©a c√°c ƒë·∫∑c tr∆∞ng M·ªöI v√† C·∫¶N GHI ƒê√à ƒë√£ tr√≠ch xu·∫•t ƒë∆∞·ª£c."""
        try:
            self.features['label'] = label # ƒê·∫∑t label ngay t·ª´ ƒë·∫ßu

            self._get_url_domain_features()
            self._fetch_url_content()
            self._get_content_features()
            self._get_visual_and_complex_features()


            # CH·ªà TR·∫¢ V·ªÄ C√ÅC C·ªòT C·∫¶N THI·∫æT CHO LOG V√Ä MERGE
            final_features = {key: self.features.get(key,
                                                     0.5 if key in ['V1_PHash_Distance', 'V2_Layout_Similarity'] else
                                                     (label if key == 'label' else 0.0)
                                                     ) for key in FEATURE_ORDER_LOG}

            return final_features
        except Exception as e:
            # N·∫øu c√≥ l·ªói qu√° l·ªõn, v·∫´n tr·∫£ v·ªÅ dictionary v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
            print(f"L·ªói nghi√™m tr·ªçng khi tr√≠ch xu·∫•t feature cho {self.url}: {e}")
            return {key: self.features.get(key,
                                            0.5 if key in ['V1_PHash_Distance', 'V2_Layout_Similarity'] else
                                            (label if key == 'label' else 0.0)
                                            ) for key in FEATURE_ORDER_LOG}

# =================================================================
# III. LOGIC CH·∫†Y ƒêA LU·ªíNG V√Ä MERGE (Gi·ªØ nguy√™n)
# =================================================================

def load_data_for_extraction(file_path: str) -> pd.DataFrame:
    """ƒê·ªçc d·ªØ li·ªáu th√¥ v√† l·ªçc b·ªè c√°c URL ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω (d·ª±a tr√™n DETAILED_LOG_FILE)."""
    if not os.path.exists(file_path):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file CSV: {file_path}")
        return pd.DataFrame()

    ENCODINGS_TO_TRY = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']
    df_raw = pd.DataFrame()
    success = False

    for enc in ENCODINGS_TO_TRY:
        try:
            # T·∫¢I T·∫§T C·∫¢ C√ÅC C·ªòT C√ì S·∫¥N
            df_raw = pd.read_csv(file_path, encoding=enc, encoding_errors='ignore')
            success = True
            print(f"‚úÖ ƒê·ªçc file CSV th√¥ th√†nh c√¥ng v·ªõi m√£ h√≥a: {enc} (ƒê√£ b·ªè qua l·ªói k√Ω t·ª±).")
            break
        except Exception:
            continue

    if not success:
        print(f"‚ùå Th·∫•t b·∫°i: Kh√¥ng th·ªÉ ƒë·ªçc file CSV v·ªõi b·∫•t k·ª≥ m√£ h√≥a n√†o.")
        return pd.DataFrame()

    df_raw.rename(columns={'URL': 'url'}, inplace=True)
    df_base = df_raw.copy()

    processed_urls = set()
    if os.path.exists(DETAILED_LOG_FILE):
        try:
            # ƒê·ªçc file log chi ti·∫øt (ch·ªâ ch·ª©a c√°c feature m·ªõi)
            df_log = pd.read_csv(DETAILED_LOG_FILE, usecols=['url'], encoding='utf-8', encoding_errors='ignore')
            processed_urls = set(df_log['url'].astype(str).tolist())
            print(f"‚úÖ T·∫£i log chi ti·∫øt th√†nh c√¥ng: {len(processed_urls)} URL ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω c√°c feature m·ªõi.")

        except Exception as e:
            print(f"‚ö†Ô∏è C·∫£nh b√°o: L·ªói khi ƒë·ªçc file log chi ti·∫øt {DETAILED_LOG_FILE}. ƒêang x√≥a log ƒë·ªÉ b·∫Øt ƒë·∫ßu l·∫°i. L·ªói: {e}")
            try:
                os.remove(DETAILED_LOG_FILE)
            except Exception:
                pass
            processed_urls = set()

    # L·∫•y c√°c h√†ng trong df_base m√† URL ch∆∞a c√≥ trong processed_urls
    df_remaining = df_base[~df_base['url'].isin(processed_urls)]

    total_count = len(df_base)
    remaining_count = len(df_remaining)

    if remaining_count < total_count:
        print(f"‚úÖ ƒê√£ t·∫£i: {total_count} URL. ƒê√£ x·ª≠ l√Ω feature m·ªõi: {total_count - remaining_count} URL. Ti·∫øp t·ª•c x·ª≠ l√Ω {remaining_count} URL c√≤n l·∫°i.")
    else:
        print(f"‚úÖ B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t feature m·ªõi: {total_count} URL c·∫ßn x·ª≠ l√Ω.")

    # Tr·∫£ v·ªÅ df_raw g·ªëc (ƒë·ªÉ merge sau) v√† df_remaining (ƒë·ªÉ x·ª≠ l√Ω)
    return df_base, df_remaining


def extract_features_worker(row: pd.Series) -> Optional[Tuple[str, Optional[Dict[str, Any]]]]:
    url = row['url']
    label = row['label']

    extractor = FeatureExtractor(url)

    # Ch·ªâ tr√≠ch xu·∫•t c√°c feature M·ªöI v√† C·∫¶N GHI ƒê√à
    result_dict = extractor.get_all_features(label)

    return (url, result_dict)


def append_to_csv_and_log(results_buffer: List[Tuple[str, Optional[Dict[str, Any]]]], output_file_exists: bool):
    successful_log_dicts = []

    for url, features_dict in results_buffer:
        if features_dict:
            # Ghi t·∫•t c·∫£ c√°c k·∫øt qu·∫£ feature M·ªöI v√†o log
            successful_log_dicts.append(features_dict)

    # 1. Ghi chi ti·∫øt t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng M·ªöI ƒë√£ tr√≠ch xu·∫•t v√†o file LOG (temp_new_features_log.csv)
    if successful_log_dicts:

        df_log = pd.DataFrame(successful_log_dicts, columns=FEATURE_ORDER_LOG)

        log_file_exists = os.path.exists(DETAILED_LOG_FILE)
        log_header = not log_file_exists

        df_log.to_csv(DETAILED_LOG_FILE, mode='a', header=log_header, index=False)

    # Ki·ªÉm tra s·ªë l∆∞·ª£ng th√†nh c√¥ng (ch·ªâ ƒë·∫øm V1, V2 kh√°c 0.5)
    successes = sum(1 for d in successful_log_dicts if round(d.get('V1_PHash_Distance', 0.5), 2) != 0.5 or round(d.get('V2_Layout_Similarity', 0.5), 2) != 0.5)

    return successes


# --- H√ÄM MERGE ƒê∆Ø·ª¢C C·∫¨P NH·∫¨T ƒê·ªÇ T·∫†O BI·∫æN B√ÅO HI·ªÜU ---
def merge_final_data(df_raw: pd.DataFrame):
    """Th·ª±c hi·ªán merge cu·ªëi c√πng, x·ª≠ l√Ω l·ªói b·∫±ng Bi·∫øn B√°o Hi·ªáu, v√† l∆∞u k·∫øt qu·∫£."""
    if not os.path.exists(DETAILED_LOG_FILE):
        print("‚ùå L·ªói: File log feature m·ªõi kh√¥ng t·ªìn t·∫°i ƒë·ªÉ merge. Ch∆∞a c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c tr√≠ch xu·∫•t.")
        return

    print("\n--- B·∫Øt ƒë·∫ßu giai ƒëo·∫°n 2: H·ª£p nh·∫•t d·ªØ li·ªáu v√† X·ª≠ l√Ω L·ªói (Bi·∫øn B√°o Hi·ªáu) ---")

    # 1. ƒê·ªçc l·∫°i to√†n b·ªô file log feature m·ªõi
    df_new_features = pd.read_csv(DETAILED_LOG_FILE, encoding='utf-8', encoding_errors='ignore')

    # 2. Lo·∫°i b·ªè c√°c ƒë·∫∑c tr∆∞ng ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t/ghi ƒë√® kh·ªèi file th√¥
    cols_to_drop = [col for col in OVERWRITE_FEATURES if col != 'label']
    df_final = df_raw.drop(columns=cols_to_drop, errors='ignore')

    # 3. Th·ª±c hi·ªán merge (Left Join: gi·ªØ l·∫°i t·∫•t c·∫£ c√°c h√†ng t·ª´ file th√¥)
    df_final = pd.merge(df_final, df_new_features, on='url', how='left', suffixes=('_old', '_new'))

    # 4. T·∫†O BI·∫æN B√ÅO HI·ªÜU (INDICATOR VARIABLES)
    print("    -> √Åp d·ª•ng Bi·∫øn B√°o Hi·ªáu cho c√°c l·ªói tr√≠ch xu·∫•t...")

    # T·∫°o bi·∫øn b√°o hi·ªáu cho c√°c l·ªói HTTP/Content (V10=0)
    # df_final['V10_HTTP_Extraction_Success'] l√† c·ªôt m·ªõi ƒë√£ ƒë∆∞·ª£c merge
    for col in DYNAMIC_CONTENT_FEATURES:
        indicator_col_name = f'Is_{col}_Missing_V10'
        # N·∫øu V10 = 0, t·ª©c l√† feature n√†y b·ªã thi·∫øu v√† ƒë∆∞·ª£c g√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh (0.0 ho·∫∑c 0.5)
        df_final[indicator_col_name] = np.where(df_final['V10_HTTP_Extraction_Success'] == 0, 1, 0)

    # T·∫°o bi·∫øn b√°o hi·ªáu cho c√°c l·ªói WHOIS (V11=0)
    # df_final['V11_WHOIS_Extraction_Success'] l√† c·ªôt m·ªõi ƒë√£ ƒë∆∞·ª£c merge
    for col in WHOIS_FEATURES:
        indicator_col_name = f'Is_{col}_Missing_V11'
        df_final[indicator_col_name] = np.where(df_final['V11_WHOIS_Extraction_Success'] == 0, 1, 0)

    # 5. D·ªçn d·∫πp c·ªôt v√† L∆∞u file

    # ƒê·∫£m b·∫£o c·ªôt label cu·ªëi c√πng l√† c·ªôt m·ªõi
    df_final.rename(columns={'label_new': 'label'}, inplace=True)
    if 'label_old' in df_final.columns:
          df_final.drop(columns=['label_old'], inplace=True)

    # Lo·∫°i b·ªè c√°c c·ªôt *old kh√¥ng c·∫ßn thi·∫øt
    cols_to_keep = [col for col in df_final.columns if not col.endswith('_old')]
    df_final = df_final[cols_to_keep]

    # Ghi ra file cu·ªëi c√πng
    df_final.to_csv(OUTPUT_CSV_FILE, index=False)
    print(f"‚úÖ H·ª£p nh·∫•t th√†nh c√¥ng. K·∫øt qu·∫£ cu·ªëi c√πng (ƒë√£ x·ª≠ l√Ω l·ªói) l∆∞u t·∫°i: {OUTPUT_CSV_FILE}")


def check_internet_connectivity():
    print("--- ü©∫ Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng...")
    try:
        requests.get("https://www.google.com", timeout=15)
        print("‚úÖ Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng: OK.")
    except requests.exceptions.RequestException:
        print("‚ùå KI·ªÇM TRA M·∫†NG TH·∫§T B·∫†I: Script kh√¥ng th·ªÉ k·∫øt n·ªëi Internet.")
        print("    Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t NAT/Proxy.")
        sys.exit(1)


def run_multiprocess_extraction():
    check_internet_connectivity()

    df_raw, df_remaining = load_data_for_extraction(RAW_CSV_FILE)

    if df_remaining.empty and os.path.exists(DETAILED_LOG_FILE):
        print("üéâ T·∫•t c·∫£ URL ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω c√°c feature m·ªõi. Chuy·ªÉn sang Merge...")
        merge_final_data(df_raw)
        return

    ALL_ROWS = [row for index, row in df_remaining.iterrows()]
    total_remaining = len(ALL_ROWS)

    print(f"--- B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t {total_remaining} URL feature m·ªõi v·ªõi {MAX_WORKERS} lu·ªìng ---")

    results_buffer = []
    processed_count_success = 0
    start_time = time.time()

    # S·ªë l∆∞·ª£ng URL ƒë√£ ho√†n th√†nh tr∆∞·ªõc ƒë√≥ (t√≠nh t·ª´ log)
    initial_processed_count = pd.read_csv(DETAILED_LOG_FILE).shape[0] if os.path.exists(DETAILED_LOG_FILE) else 0

    output_file_exists = os.path.exists(OUTPUT_CSV_FILE)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_row = {executor.submit(extract_features_worker, row): row for row in ALL_ROWS}

        for i, future in enumerate(as_completed(future_to_row)):

            url, result = future.result()
            results_buffer.append((url, result))

            if len(results_buffer) >= BUFFER_SIZE or (i + 1) == total_remaining:

                successes = append_to_csv_and_log(results_buffer, output_file_exists)
                processed_count_success += successes
                results_buffer = []

                elapsed_time = time.time() - start_time
                avg_speed = (i + 1) / elapsed_time if elapsed_time > 0 else 0

                total_complete_log = initial_processed_count + (i + 1)

                print(f"[{i + 1}/{total_remaining}] ƒê√£ x·ª≠ l√Ω (m·ªõi): {i + 1} URL. Th√†nh c√¥ng (V1/V2): {processed_count_success}. T·ªïng log: {total_complete_log}. T·ªëc ƒë·ªô: {avg_speed:.2f} URL/gi√¢y.")

    print(f"\n--- Giai ƒëo·∫°n 1: Tr√≠ch xu·∫•t Feature m·ªõi HO√ÄN TH√ÄNH ---")
    # Th·ª±c hi·ªán merge cu·ªëi c√πng sau khi t·∫•t c·∫£ c√°c lu·ªìng ƒë√£ ho√†n th√†nh
    merge_final_data(df_raw)

# =================================================================
# IV. KH·ªêI CH·∫†Y CH√çNH
# =================================================================
if __name__ == "__main__":
    run_multiprocess_extraction()
