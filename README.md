# =================================================================
# run_extraction_final_merge_FIXED.py - S·ª¨ D·ª§NG PLAYWRIGHT ƒê√É S·ª¨A L·ªñI M·∫†NG/TIMEOUT
# =================================================================
import pandas as pd
import numpy as np
import os
import requests
from bs4 import BeautifulSoup
import tldextract
import time
import re
from datetime import datetime
# --- Thay th·∫ø Selenium b·∫±ng Playwright ---
from playwright.sync_api import sync_playwright, Playwright, Browser, Page, TimeoutError as PlaywrightTimeoutError
# ----------------------------------------
import imagehash
from PIL import Image
import io
import sys
import math
from collections import Counter
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
import socket
import ssl

# T·∫Øt c·∫£nh b√°o SSL v√† t·∫Øt ghi file bytecode
requests.packages.urllib3.disable_warnings()
sys.dont_write_bytecode = True

# --- 1. C·∫§U H√åNH V√Ä H·∫∞NG S·ªê ---
RAW_CSV_FILE = 'PhiUSIIL_Phishing_URL_Dataset.csv'
OUTPUT_CSV_FILE = 'merged_extracted_data_final_processed.csv' # File Output CU·ªêI C√ôNG ƒë√£ merge v√† x·ª≠ l√Ω l·ªói
DETAILED_LOG_FILE = 'temp_new_features_log.csv'

# TƒÉng s·ªë lu·ªìng ƒë·ªÉ t·∫≠n d·ª•ng hi·ªáu su·∫•t I/O t·ªët h∆°n
MAX_WORKERS = 10
BUFFER_SIZE = 500

# Hash m·∫´u (v√≠ d·ª•: Google Search Page)
TARGET_PHASH = imagehash.hex_to_hash('9880e61f1c7e0c4f')

# C√°c ƒë·∫∑c tr∆∞ng T·ªíN T·∫†I trong file th√¥ c·∫ßn ƒë∆∞·ª£c C·∫¨P NH·∫¨T/GHI ƒê√à (T√≠nh l·∫°i cho ch√≠nh x√°c)
OVERWRITE_FEATURES = [
    'NoOfDegitsInURL', 'HasDescription', 'HasSocialNet', 'HasPasswordField', 'HasSubmitButton',
    'HasExternalFormSubmit', 'DomainTitleMatchScore', 'IsHTTPS', 'HasCopyrightInfo', 'label'
]

# C√°c ƒë·∫∑c tr∆∞ng M·ªöI c·∫ßn ƒë∆∞·ª£c tr√≠ch xu·∫•t (Kh√¥ng t·ªìn t·∫°i trong file th√¥)
NEW_FEATURES = [
    'V10_HTTP_Extraction_Success', 'V11_WHOIS_Extraction_Success', 'V1_PHash_Distance',
    'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score', 'V8_Total_IFrames',
    'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 'V3_Domain_Age_Days',
    'V4_DNS_Volatility_Count', 'Is_Top_1M_Domain', 'V22_IP_Subdomain_Pattern',
    'V23_Entropy_Subdomain'
]

# C√°c ƒë·∫∑c tr∆∞ng ƒë·ªông c√≥ gi√° tr·ªã m·∫∑c ƒë·ªãnh 0.0 ho·∫∑c 0.5 khi HTTP/Content FAIL (V10=0)
DYNAMIC_CONTENT_FEATURES = [
    'V1_PHash_Distance', 'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score',
    'V8_Total_IFrames', 'V9_Has_Hidden_IFrame', 'HasDescription', 'HasSocialNet',
    'HasPasswordField', 'HasSubmitButton', 'HasExternalFormSubmit', 'DomainTitleMatchScore',
    'HasCopyrightInfo', 'V5_TLS_Issuer_Reputation'
]

# C√°c ƒë·∫∑c tr∆∞ng b·ªã ·∫£nh h∆∞·ªüng khi WHOIS FAIL (V11=0)
WHOIS_FEATURES = ['V3_Domain_Age_Days']
# ------------------------------------------------

FEATURE_ORDER_LOG = ['url'] + OVERWRITE_FEATURES + NEW_FEATURES

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    # Th√™m nhi·ªÅu User Agent ƒë·ªÉ tƒÉng t√≠nh ng·∫´u nhi√™n
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
]
# -------------------------------------

# =================================================================
# II. L·ªöP TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG (FEATURE EXTRACTOR) - ƒê√É S·ª¨A L·ªñI
# =================================================================

class FeatureExtractor:
    WHOIS_TIMEOUT: int = 15 # TƒÉng WHOIS Timeout
    RENDER_TIMEOUT: int = 40 # TƒÉng Playwright Timeout l√™n 40s
    REQUESTS_TIMEOUT: int = 45 # TƒÉng requests Timeout l√™n 45s

    def __init__(self, url: str):
        self.url: str = self._normalize_url(url)
        self.features: Dict[str, Any] = {'url': url}
        self.response: Optional[requests.Response] = None
        self.soup: Optional[BeautifulSoup] = None
        self.current_domain: Optional[str] = None
        self.http_extraction_successful: bool = False
        self.top_1m_data: Dict[str, bool] = {'google.com': True, 'facebook.com': True, 'microsoft.com': True, 'amazon.com': True}

    def _normalize_url(self, url: str) -> str:
        if not url.startswith('http'):
            return 'http://' + url
        return url
        
    # --- C√°c h√†m t√≠nh to√°n Tƒ®NH (Gi·ªØ nguy√™n) ---
    def _calculate_entropy(self, text: str) -> float:
        if not text: return 0.0
        p, lns = Counter(text), float(len(text))
        entropy = -sum(count / lns * math.log2(count / lns) for count in p.values())
        return entropy / 8.0

    def _calculate_dns_volatility(self, domain: str) -> int:
        try:
            ip_list = socket.gethostbyname_ex(domain)[2]
            return len(set(ip_list)) - 1
        except socket.gaierror:
            return 0 # Tr·∫£ v·ªÅ 0 thay v√¨ -1 ƒë·ªÉ th·ªëng nh·∫•t
        except Exception:
            return 0
            
    def _parse_whois_date(self, date_data: Any) -> Optional[datetime]:
        if isinstance(date_data, list): date_data = date_data[0]
        if date_data is None or date_data == 'None': return None
        if isinstance(date_data, datetime): return date_data.replace(tzinfo=None)
        
        if isinstance(date_data, str):
            clean_date_data = re.sub(r'(\s+\w{3}|\s+\+\d{2}:\d{2})$', '', date_data).strip()
            formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%Y-%m-%dT%H:%M:%SZ', '%Y%m%d', '%d-%b-%Y', '%m/%d/%Y']
            for fmt in formats:
                try:
                    dt = datetime.strptime(clean_date_data, fmt)
                    return dt.replace(tzinfo=None)
                except ValueError:
                    continue
        return None
        
    def _calculate_tls_issuer_rep(self) -> float:
        # ... (Logic TLS/SSL gi·ªØ nguy√™n)
        if not self.url.startswith('https://'): return 0.0
        hostname = tldextract.extract(self.url).fqdn
        if not hostname: return 0.0 # ƒê√£ s·ª≠a m·∫∑c ƒë·ªãnh th√†nh 0.0 n·∫øu kh√¥ng c√≥ hostname
        try:
            context = ssl.create_default_context()
            with socket.create_connection((hostname, 443), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert = ssock.getpeercert()
            issuer = next((item[0][1] for item in cert['issuer'] if item[0][0] == 'organizationName'), '').lower()
            TRUSTED_ISSUERS = ['google', 'amazon', 'digicert', 'cloudflare', 'globalsign']
            if 'lets encrypt' in issuer: return 0.7
            if any(name in issuer for name in TRUSTED_ISSUERS): return 0.95
            return 0.2
        except Exception: return 0.0 # ƒê·∫£m b·∫£o tr·∫£ v·ªÅ 0.0 khi th·∫•t b·∫°i TLS

    # --- TR√çCH XU·∫§T URL & WHOIS (ƒê√£ s·ª≠a l·ªói try-except) ---
    def _get_url_domain_features(self) -> None:
        import whois
        self.features['V11_WHOIS_Extraction_Success'] = 0

        url_no_protocol = self.url.replace("http://", "").replace("https://", "")
        self.features['NoOfDegitsInURL'] = sum(c.isdigit() for c in url_no_protocol)

        domain_info = tldextract.extract(self.url)
        domain = f"{domain_info.domain}.{domain_info.suffix}"
        self.current_domain = domain_info.domain
        subdomain = domain_info.subdomain.lower()

        self.features['V22_IP_Subdomain_Pattern'] = 1 if re.search(r'\d+\.\d+\.\d+(\.\d+)?', subdomain) else 0
        self.features['V23_Entropy_Subdomain'] = self._calculate_entropy(subdomain)
        self.features['V4_DNS_Volatility_Count'] = max(0, self._calculate_dns_volatility(domain))

        domain_age_days = 0
        try:
            # S·ª¨A L·ªñI: TƒÉng timeout cho WHOIS
            whois_info = whois.whois(domain, timeout=self.WHOIS_TIMEOUT)
            
            # Ki·ªÉm tra n·∫øu whois_info l√† l·ªói
            if isinstance(whois_info.domain_name, str) and ('not found' in whois_info.domain_name.lower() or 'no match' in whois_info.domain_name.lower()):
                 raise ValueError("Domain Not Found")

            creation_date = self._parse_whois_date(whois_info.creation_date)
            if creation_date:
                age = datetime.now().replace(tzinfo=None) - creation_date
                domain_age_days = age.days
                self.features['V11_WHOIS_Extraction_Success'] = 1
        except Exception:
            # N·∫øu WHOIS th·∫•t b·∫°i, ƒë·∫∑t tu·ªïi l√† 0 v√† V11=0
            domain_age_days = 0
            self.features['V11_WHOIS_Extraction_Success'] = 0 

        self.features['V3_Domain_Age_Days'] = max(0, domain_age_days)
        self.features['IsHTTPS'] = 1 if self.url.startswith('https://') else 0

        # S·ª¨A L·ªñI: ƒê·∫£m b·∫£o ki·ªÉm tra to√†n b·ªô domain (bao g·ªìm suffix)
        is_top_1m = 1 if domain and domain.lower() in self.top_1m_data else 0
        self.features['Is_Top_1M_Domain'] = is_top_1m

    # --- TRUY V·∫§N V√Ä PH√ÇN T√çCH N·ªòI DUNG (ƒê√É S·ª¨A L·ªñI TIMEOUT) ---
    def _fetch_url_content(self) -> None:
        self.features['V10_HTTP_Extraction_Success'] = 0
        self.http_extraction_successful = False

        if '0.0.0.0' in self.url or '127.0.0.1' in self.url or '192.168.' in self.url:
            return

        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept-Language': 'en-US,en;q=0.9',
        }

        try:
            # S·ª¨A L·ªñI: TƒÉng Timeout requests
            self.response = requests.get(self.url, timeout=self.REQUESTS_TIMEOUT, verify=False, allow_redirects=True, headers=headers)
            self.response.raise_for_status()
            self.soup = BeautifulSoup(self.response.content, 'html.parser')
            self.features['V10_HTTP_Extraction_Success'] = 1
            self.http_extraction_successful = True
        except requests.exceptions.RequestException as e:
            # Ghi l·∫°i l·ªói request
            print(f"‚ö†Ô∏è Request Failed for {self.url}: {type(e).__name__}")
            self.response = None
            self.soup = None

    # --- TR√çCH XU·∫§T C√ÅC ƒê·∫∂C TR∆ØNG HTML (Gi·ªØ nguy√™n) ---
    def _get_content_features(self) -> None:
        # ƒê·∫∂T GI√Å TR·ªä M·∫∂C ƒê·ªäNH cho c√°c ƒë·∫∑c tr∆∞ng ƒë·ªông (R·∫•t quan tr·ªçng)
        default_features = {
            'HasDescription': 0, 'HasSocialNet': 0, 'HasPasswordField': 0, 'HasSubmitButton': 0,
            'HasExternalFormSubmit': 0, 'DomainTitleMatchScore': 0.0, 'HasCopyrightInfo': 0,
            'V8_Total_IFrames': 0, 'V9_Has_Hidden_IFrame': 0, 'V7_Text_Readability_Score': 0.0,
            'V6_JS_Entropy': 0.0,
            'V1_PHash_Distance': 0.5, 
            'V2_Layout_Similarity': 0.5,
        }

        self.features.update(default_features)
        self.features['V5_TLS_Issuer_Reputation'] = self._calculate_tls_issuer_rep()

        if not self.soup: return

        # --- B·∫ÆT ƒê·∫¶U T√çNH TO√ÅN KHI HTTP TH√ÄNH C√îNG ---
        # (Gi·ªØ nguy√™n logic t√≠nh to√°n Readability, Form, Title Match, IFrame)
        def _calculate_readability(text: str) -> float:
             sentences = len(re.split(r'[.!?]+', text))
             words = len(re.findall(r'\w+', text))
             syllables = words * 1.5
             if sentences == 0 or words == 0: return 50.0
             score = 206.835 - 1.015 * (words / sentences) - 84.6 * (syllables / words)
             return np.clip(score, 0.0, 100.0)

        def _extract_dom_form_features(soup: BeautifulSoup, current_domain: str) -> Dict[str, Any]:
            f: Dict[str, Any] = {}
            f['HasPasswordField'] = 1 if len(soup.find_all('input', type='password')) > 0 else 0
            f['HasSubmitButton'] = 1 if len(soup.find_all('input', type='submit') + soup.find_all('button', type='submit')) > 0 else 0
            external_form = 0
            for form in soup.find_all('form'):
                action = form.get('action')
                if action and action.startswith('http') and tldextract.extract(action).domain != current_domain:
                    external_form = 1
                    break
            f['HasExternalFormSubmit'] = external_form
            return f

        form_features_static = _extract_dom_form_features(self.soup, self.current_domain)
        self.features.update(form_features_static)

        # C·∫≠p nh·∫≠t c√°c features tƒ©nh (HasDescription, HasSocialNet, TitleMatch, Copyright, IFrame, Entropy, Readability)
        description_tag = self.soup.find('meta', attrs={'name': 'description'})
        self.features['HasDescription'] = 1 if (description_tag and description_tag.get('content')) else 0
        social_links = self.soup.find_all('a', href=lambda href: href and ('facebook.com' in href or 'twitter.com' in href))
        self.features['HasSocialNet'] = 1 if len(social_links) > 0 else 0
        
        title_text = self.soup.title.string if self.soup.title and self.soup.title.string else ""
        domain_name = self.current_domain.lower() if self.current_domain else ""
        match_score = 1.0 if domain_name and title_text and domain_name in title_text.lower() else 0.0
        self.features['DomainTitleMatchScore'] = np.clip(match_score, 0.0, 1.0)
        
        copyright_text = self.soup.find(string=lambda text: text and 'copyright' in text.lower())
        self.features['HasCopyrightInfo'] = 1 if copyright_text else 0
        
        self.features['V8_Total_IFrames'] = len(self.soup.find_all('iframe'))
        hidden_iframe = self.soup.find('iframe', attrs={'style': lambda style: style and 'display:none' in style.lower()})
        if not hidden_iframe: hidden_iframe = self.soup.find('iframe', attrs={'width': '0', 'height': '0'})
        self.features['V9_Has_Hidden_IFrame'] = 1 if hidden_iframe else 0
        
        page_text = self.soup.get_text(separator=' ', strip=True)
        self.features['V7_Text_Readability_Score'] = _calculate_readability(page_text)
        
        script_tags = self.soup.find_all('script')
        js_content = "".join(tag.string for tag in script_tags if tag.string)
        self.features['V6_JS_Entropy'] = self._calculate_entropy(js_content)


    # --- ƒê·ªòNG: TR√çCH XU·∫§T VISUAL V√Ä JAVASCRIPT (ƒê√É S·ª¨A L·ªñI PLAYWRIGHT) ---
    def _get_visual_and_complex_features(self, p: Playwright) -> None:
        """S·ª≠ d·ª•ng Playwright ƒë·ªÉ render v√† tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng ƒë·ªông (V1, V2)."""

        if not self.http_extraction_successful: return

        # --- H√†m con (Gi·ªØ nguy√™n logic c·ªßa b·∫°n) ---
        def _calculate_phash_distance(image_data: bytes) -> float:
            try:
                image = Image.open(io.BytesIO(image_data)).convert('L')
                current_phash = imagehash.phash(image, hash_size=8)
                distance = current_phash - TARGET_PHASH
                return float(distance) / 64.0
            except Exception: return 0.5

        def _calculate_layout_similarity(dom_tree: BeautifulSoup) -> float:
            def find_max_depth(element: BeautifulSoup, current_depth: int = 0) -> int:
                max_d = current_depth
                for child in element.find_all(True, recursive=False):
                    max_d = max(max_d, find_max_depth(child, current_depth + 1))
                return max_d
            try:
                max_depth = find_max_depth(dom_tree)
                similarity = np.clip(1.0 - (max_depth / 20.0), 0.1, 0.9)
                return float(f"{similarity:.4f}")
            except Exception: return 0.5

        def _extract_dom_form_features_dynamic(soup: BeautifulSoup, current_domain: str) -> Dict[str, Any]:
             f: Dict[str, Any] = {}
             f['HasPasswordField'] = 1 if len(soup.find_all('input', type='password')) > 0 else 0
             f['HasSubmitButton'] = 1 if len(soup.find_all('input', type='submit') + soup.find_all('button', type='submit')) > 0 else 0
             external_form = 0
             for form in soup.find_all('form'):
                 action = form.get('action')
                 if action and action.startswith('http') and tldextract.extract(action).domain != current_domain:
                     external_form = 1
                     break
             f['HasExternalFormSubmit'] = external_form
             return f
        # ----------------------------------------------------

        # --- B·∫ÆT ƒê·∫¶U V·ªöI PLAYWRIGHT ---
        browser: Optional[Browser] = None
        try:
            # T·ªëi ∆∞u h√≥a Playwright
            browser = p.chromium.launch(
                headless=True,
                # Th√™m c·ªù ƒë·ªÉ tƒÉng c∆∞·ªùng ·ªïn ƒë·ªãnh tr√™n Linux
                args=[
                    "--disable-gpu", 
                    "--no-sandbox", 
                    "--disable-setuid-sandbox", # R·∫•t quan tr·ªçng tr√™n Linux
                    f"--user-agent={random.choice(USER_AGENTS)}"
                ]
            )

            page: Page = browser.new_page()
            # S·ª¨A L·ªñI: TƒÉng timeout Playwright
            page.set_default_timeout(self.RENDER_TIMEOUT * 1000)

            try:
                # S·ª¨A L·ªñI: Ch·ªù networkidle
                page.goto(self.url, wait_until="networkidle")

                screenshot_data: bytes = page.screenshot()
                self.features['V1_PHash_Distance'] = _calculate_phash_distance(screenshot_data)

                rendered_html: str = page.content()
                rendered_soup: BeautifulSoup = BeautifulSoup(rendered_html, 'html.parser')
                self.features['V2_Layout_Similarity'] = _calculate_layout_similarity(rendered_soup)

                dynamic_form_features = _extract_dom_form_features_dynamic(rendered_soup, self.current_domain)
                self.features.update(dynamic_form_features)

            except PlaywrightTimeoutError as e:
                print(f"‚ö†Ô∏è Playwright Timeout (40s) khi x·ª≠ l√Ω {self.url}")
                # Gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh 0.5
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói Playwright khi x·ª≠ l√Ω {self.url}: {e}")
                # Gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh 0.5
            finally:
                if browser: browser.close()

        except Exception as e_init:
            print(f"‚ùå L·ªói Kh·ªüi t·∫°o Browser Playwright: {e_init}")
            # Gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh 0.5
            

    def get_all_features(self, label: int, p: Playwright) -> Optional[Dict[str, Any]]:
        """Tr·∫£ v·ªÅ dictionary ch·ª©a c√°c ƒë·∫∑c tr∆∞ng M·ªöI v√† C·∫¶N GHI ƒê√à ƒë√£ tr√≠ch xu·∫•t ƒë∆∞·ª£c."""
        try:
            self.features['label'] = label 
            
            # --- 1. FEATURE Tƒ®NH (Kh√¥ng c·∫ßn Playwright object) ---
            self._get_url_domain_features()
            self._fetch_url_content()
            self._get_content_features()
            
            # --- 2. FEATURE ƒê·ªòNG (C·∫ßn Playwright object) ---
            self._get_visual_and_complex_features(p)

            # CH·ªà TR·∫¢ V·ªÄ C√ÅC C·ªòT C·∫¶N THI·∫æT
            final_features = {key: self.features.get(key,
                                                     0.5 if key in ['V1_PHash_Distance', 'V2_Layout_Similarity'] else
                                                     (label if key == 'label' else 0.0)
                                                     ) for key in FEATURE_ORDER_LOG}

            return final_features
        except Exception as e:
            # N·∫øu c√≥ l·ªói qu√° l·ªõn, v·∫´n tr·∫£ v·ªÅ dictionary v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
            print(f"L·ªói nghi√™m tr·ªçng khi tr√≠ch xu·∫•t feature cho {self.url}: {e}")
            return {key: self.features.get(key,
                                            0.5 if key in ['V1_PHash_Distance', 'V2_Layout_Similarity'] else
                                            (label if key == 'label' else 0.0)
                                            ) for key in FEATURE_ORDER_LOG}


# =================================================================
# III. LOGIC CH·∫†Y ƒêA LU·ªíNG V√Ä MERGE (ƒê√£ s·ª≠a ƒë·ªïi ƒë·ªÉ d√πng Playwright object)
# =================================================================

# Th√™m h√†m run_extractor_with_playwright_context ƒë·ªÉ qu·∫£n l√Ω Playwright context
def run_extractor_with_playwright_context(rows: List[pd.Series], p: Playwright) -> Dict[str, Optional[Dict[str, Any]]]:
    """Ch·∫°y tr√≠ch xu·∫•t trong m·ªôt lu·ªìng ƒë∆°n, s·ª≠ d·ª•ng c√πng m·ªôt Playwright context."""
    results = {}
    for row in rows:
        url = row['url']
        label = row['label']
        extractor = FeatureExtractor(url)
        results[url] = extractor.get_all_features(label, p)
    return results

def append_to_csv_and_log(results_buffer: List[Tuple[str, Optional[Dict[str, Any]]]], output_file_exists: bool):
    successful_log_dicts = []
    
    for url, features_dict in results_buffer:
        if features_dict:
            # Ghi t·∫•t c·∫£ c√°c k·∫øt qu·∫£ feature M·ªöI v√†o log
            successful_log_dicts.append(features_dict)
    
    # 1. Ghi chi ti·∫øt t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng M·ªöI ƒë√£ tr√≠ch xu·∫•t v√†o file LOG (temp_new_features_log.csv)
    if successful_log_dicts:
        
        df_log = pd.DataFrame(successful_log_dicts, columns=FEATURE_ORDER_LOG)
        
        log_file_exists = os.path.exists(DETAILED_LOG_FILE)
        log_header = not log_file_exists
        
        df_log.to_csv(DETAILED_LOG_FILE, mode='a', header=log_header, index=False)
    
    # Ki·ªÉm tra s·ªë l∆∞·ª£ng th√†nh c√¥ng (ch·ªâ ƒë·∫øm V1, V2 kh√°c 0.5)
    successes = sum(1 for d in successful_log_dicts if round(d.get('V1_PHash_Distance', 0.5), 2) != 0.5 or round(d.get('V2_Layout_Similarity', 0.5), 2) != 0.5)
        
    return successes

def merge_final_data(df_raw: pd.DataFrame):
    """Th·ª±c hi·ªán merge cu·ªëi c√πng, x·ª≠ l√Ω l·ªói b·∫±ng Bi·∫øn B√°o Hi·ªáu, v√† l∆∞u k·∫øt qu·∫£."""
    if not os.path.exists(DETAILED_LOG_FILE):
        print("‚ùå L·ªói: File log feature m·ªõi kh√¥ng t·ªìn t·∫°i ƒë·ªÉ merge. Ch∆∞a c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c tr√≠ch xu·∫•t.")
        return

    print("\n--- B·∫Øt ƒë·∫ßu giai ƒëo·∫°n 2: H·ª£p nh·∫•t d·ªØ li·ªáu v√† X·ª≠ l√Ω L·ªói (Bi·∫øn B√°o Hi·ªáu) ---")
    
    # 1. ƒê·ªçc l·∫°i to√†n b·ªô file log feature m·ªõi
    df_new_features = pd.read_csv(DETAILED_LOG_FILE, encoding='utf-8', encoding_errors='ignore')
    
    # 2. Lo·∫°i b·ªè c√°c ƒë·∫∑c tr∆∞ng ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t/ghi ƒë√® kh·ªèi file th√¥
    cols_to_drop = [col for col in OVERWRITE_FEATURES if col != 'label']
    df_final = df_raw.drop(columns=cols_to_drop, errors='ignore')
    
    # 3. Th·ª±c hi·ªán merge (Left Join: gi·ªØ l·∫°i t·∫•t c·∫£ c√°c h√†ng t·ª´ file th√¥)
    df_final = pd.merge(df_final, df_new_features, on='url', how='left', suffixes=('_old', '_new'))
    
    # 4. T·∫†O BI·∫æN B√ÅO HI·ªÜU (INDICATOR VARIABLES)
    print("   -> √Åp d·ª•ng Bi·∫øn B√°o Hi·ªáu cho c√°c l·ªói tr√≠ch xu·∫•t...")
    
    # T·∫°o bi·∫øn b√°o hi·ªáu cho c√°c l·ªói HTTP/Content (V10=0)
    # df_final['V10_HTTP_Extraction_Success'] l√† c·ªôt m·ªõi ƒë√£ ƒë∆∞·ª£c merge
    for col in DYNAMIC_CONTENT_FEATURES:
        indicator_col_name = f'Is_{col}_Missing_V10'
        # N·∫øu V10 = 0, t·ª©c l√† feature n√†y b·ªã thi·∫øu v√† ƒë∆∞·ª£c g√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh (0.0 ho·∫∑c 0.5)
        df_final[indicator_col_name] = np.where(df_final['V10_HTTP_Extraction_Success'] == 0, 1, 0)

    # T·∫°o bi·∫øn b√°o hi·ªáu cho c√°c l·ªói WHOIS (V11=0)
    # df_final['V11_WHOIS_Extraction_Success'] l√† c·ªôt m·ªõi ƒë√£ ƒë∆∞·ª£c merge
    for col in WHOIS_FEATURES:
        indicator_col_name = f'Is_{col}_Missing_V11'
        df_final[indicator_col_name] = np.where(df_final['V11_WHOIS_Extraction_Success'] == 0, 1, 0)
        
    # 5. D·ªçn d·∫πp c·ªôt v√† L∆∞u file
    
    # ƒê·∫£m b·∫£o c·ªôt label cu·ªëi c√πng l√† c·ªôt m·ªõi
    df_final.rename(columns={'label_new': 'label'}, inplace=True)
    if 'label_old' in df_final.columns:
         df_final.drop(columns=['label_old'], inplace=True)

    # Lo·∫°i b·ªè c√°c c·ªôt *old kh√¥ng c·∫ßn thi·∫øt
    cols_to_keep = [col for col in df_final.columns if not col.endswith('_old')]
    df_final = df_final[cols_to_keep]

    # Ghi ra file cu·ªëi c√πng
    df_final.to_csv(OUTPUT_CSV_FILE, index=False)
    print(f"‚úÖ H·ª£p nh·∫•t th√†nh c√¥ng. K·∫øt qu·∫£ cu·ªëi c√πng (ƒë√£ x·ª≠ l√Ω l·ªói) l∆∞u t·∫°i: {OUTPUT_CSV_FILE}")

def load_data_for_extraction(file_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """ƒê·ªçc d·ªØ li·ªáu th√¥ v√† l·ªçc b·ªè c√°c URL ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω (d·ª±a tr√™n DETAILED_LOG_FILE)."""
    if not os.path.exists(file_path):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file CSV: {file_path}")
        # TR·∫¢ V·ªÄ DataFrame r·ªóng ƒë·ªÉ tr√°nh l·ªói TypeError NoneType
        return pd.DataFrame(), pd.DataFrame() 

    ENCODINGS_TO_TRY = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']
    df_raw = pd.DataFrame()
    success = False
    
    for enc in ENCODINGS_TO_TRY:
        try:
            # T·∫¢I T·∫§T C·∫¢ C√ÅC C·ªòT C√ì S·∫¥N
            df_raw = pd.read_csv(file_path, encoding=enc, encoding_errors='ignore')
            success = True
            print(f"‚úÖ ƒê·ªçc file CSV th√¥ th√†nh c√¥ng v·ªõi m√£ h√≥a: {enc} (ƒê√£ b·ªè qua l·ªói k√Ω t·ª±).")
            break
        except Exception:
            continue
    
    if not success:
        print(f"‚ùå Th·∫•t b·∫°i: Kh√¥ng th·ªÉ ƒë·ªçc file CSV v·ªõi b·∫•t k·ª≥ m√£ h√≥a n√†o.")
        return pd.DataFrame(), pd.DataFrame() # Tr·∫£ v·ªÅ DataFrame r·ªóng

    df_raw.rename(columns={'URL': 'url'}, inplace=True)
    df_base = df_raw.copy()

    processed_urls = set()
    if os.path.exists(DETAILED_LOG_FILE):
        try:
            # ƒê·ªçc file log chi ti·∫øt (ch·ªâ ch·ª©a c√°c feature m·ªõi)
            df_log = pd.read_csv(DETAILED_LOG_FILE, usecols=['url'], encoding='utf-8', encoding_errors='ignore')
            processed_urls = set(df_log['url'].astype(str).tolist())
            print(f"‚úÖ T·∫£i log chi ti·∫øt th√†nh c√¥ng: {len(processed_urls)} URL ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω c√°c feature m·ªõi.")
            
        except Exception as e:
            print(f"‚ö†Ô∏è C·∫£nh b√°o: L·ªói khi ƒë·ªçc file log chi ti·∫øt {DETAILED_LOG_FILE}. ƒêang x√≥a log ƒë·ªÉ b·∫Øt ƒë·∫ßu l·∫°i. L·ªói: {e}")
            try:
                os.remove(DETAILED_LOG_FILE) 
            except Exception:
                pass
            processed_urls = set()
    
    # L·∫•y c√°c h√†ng trong df_base m√† URL ch∆∞a c√≥ trong processed_urls
    df_remaining = df_base[~df_base['url'].isin(processed_urls)]
    
    total_count = len(df_base)
    remaining_count = len(df_remaining)
    
    if remaining_count < total_count:
        print(f"‚úÖ ƒê√£ t·∫£i: {total_count} URL. ƒê√£ x·ª≠ l√Ω feature m·ªõi: {total_count - remaining_count} URL. Ti·∫øp t·ª•c x·ª≠ l√Ω {remaining_count} URL c√≤n l·∫°i.")
    else:
        print(f"‚úÖ B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t feature m·ªõi: {total_count} URL c·∫ßn x·ª≠ l√Ω.")

    # Tr·∫£ v·ªÅ df_raw g·ªëc (ƒë·ªÉ merge sau) v√† df_remaining (ƒë·ªÉ x·ª≠ l√Ω)
    return df_base, df_remaining

def extract_features_worker(row: pd.Series) -> Optional[Tuple[str, Optional[Dict[str, Any]]]]:
    url = row['url']
    label = row['label']
    
    extractor = FeatureExtractor(url)
    
    # Ch·ªâ tr√≠ch xu·∫•t c√°c feature M·ªöI v√† C·∫¶N GHI ƒê√à
    # L∆∞u √Ω: H√†m n√†y c·∫ßn ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ s·ª≠ d·ª•ng Playwright instance ƒë∆∞·ª£c truy·ªÅn v√†o, 
    # nh∆∞ng trong ki·∫øn tr√∫c ƒëa lu·ªìng hi·ªán t·∫°i, Playwright instance ƒë∆∞·ª£c qu·∫£n l√Ω b·ªüi run_extractor_with_playwright_context
    # n√™n h√†m n√†y c√≥ th·ªÉ kh√¥ng c·∫ßn thi·∫øt ho·∫∑c c·∫ßn ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh.
    # Tuy nhi√™n, ƒë·ªÉ gi·ªØ c·∫•u tr√∫c, ta c√≥ th·ªÉ b·ªè qua ho·∫∑c c·∫≠p nh·∫≠t sau.
    pass

def check_internet_connectivity():
    print("--- ü©∫ Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng...")
    try:
        requests.get("https://www.google.com", timeout=15)
        print("‚úÖ Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng: OK.")
    except requests.exceptions.RequestException:
        print("‚ùå KI·ªÇM TRA M·∫†NG TH·∫§T B·∫†I: Script kh√¥ng th·ªÉ k·∫øt n·ªëi Internet.")
        print("    Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t NAT/Proxy.")
        sys.exit(1)


def run_multiprocess_extraction():
    
    check_internet_connectivity()
    
    # S·ª¨A L·ªñI: Thay th·∫ø ThreadPoolExecutor b·∫±ng logic qu·∫£n l√Ω Playwright context
    df_raw, df_remaining = load_data_for_extraction(RAW_CSV_FILE)

    # Ki·ªÉm tra n·∫øu df_remaining r·ªóng (do load_data tr·∫£ v·ªÅ r·ªóng ho·∫∑c ƒë√£ x·ª≠ l√Ω h·∫øt)
    if df_remaining.empty:
        if os.path.exists(DETAILED_LOG_FILE):
             print("üéâ T·∫•t c·∫£ URL ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω c√°c feature m·ªõi. Chuy·ªÉn sang Merge...")
             merge_final_data(df_raw)
        else:
             print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω v√† kh√¥ng t√¨m th·∫•y file log.")
        return

    ALL_ROWS = [row for index, row in df_remaining.iterrows()]
    total_remaining = len(ALL_ROWS)

    print(f"--- B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t {total_remaining} URL feature m·ªõi v·ªõi {MAX_WORKERS} lu·ªìng Playwright ---")

    # Chia nh·ªè c√¥ng vi·ªác cho c√°c lu·ªìng
    chunk_size = math.ceil(total_remaining / MAX_WORKERS)
    # ƒê·∫£m b·∫£o chunk_size √≠t nh·∫•t l√† 1 ƒë·ªÉ tr√°nh l·ªói range
    if chunk_size == 0: chunk_size = 1
    row_chunks = [ALL_ROWS[i:i + chunk_size] for i in range(0, total_remaining, chunk_size)]
    
    results_buffer = []
    processed_count_success = 0
    start_time = time.time()
    
    initial_processed_count = pd.read_csv(DETAILED_LOG_FILE).shape[0] if os.path.exists(DETAILED_LOG_FILE) else 0

    try:
        with sync_playwright() as p:
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                # G·ª≠i m·ªói chunk rows v√† ƒë·ªëi t∆∞·ª£ng Playwright (p) ƒë·∫øn h√†m run_extractor_with_playwright_context
                future_to_chunk = {executor.submit(run_extractor_with_playwright_context, chunk, p): chunk for chunk in row_chunks}
                
                total_completed_chunks = 0
                for future in as_completed(future_to_chunk):
                    try:
                        chunk_results = future.result() # Dict: {url: features}
                        
                        # Th√™m k·∫øt qu·∫£ v√†o buffer
                        for url, features_dict in chunk_results.items():
                            results_buffer.append((url, features_dict))

                        total_completed_chunks += 1
                        total_urls_processed = initial_processed_count + len(results_buffer) # Ch·ªâ s·ªë n√†y s·∫Ω kh√¥ng ch√≠nh x√°c tuy·ªát ƒë·ªëi do buffer

                        if len(results_buffer) >= BUFFER_SIZE or total_completed_chunks == len(row_chunks):
                            successes = append_to_csv_and_log(results_buffer, os.path.exists(OUTPUT_CSV_FILE))
                            processed_count_success += successes
                            results_buffer = []

                            elapsed_time = time.time() - start_time
                            avg_speed = total_urls_processed / elapsed_time if elapsed_time > 0 else 0

                            print(f"[Completed Chunks: {total_completed_chunks}/{len(row_chunks)}] ƒê√£ x·ª≠ l√Ω (t·ªïng): {total_urls_processed} URL. Th√†nh c√¥ng (V1/V2): {processed_count_success}. T·ªëc ƒë·ªô: {avg_speed:.2f} URL/gi√¢y.")
                    except Exception as e:
                        print(f"‚ùå L·ªói trong m·ªôt chunk: {e}")

    except Exception as e:
        print(f"‚ùå L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh ƒëa lu·ªìng/Playwright: {e}")
        
    print(f"\n--- Giai ƒëo·∫°n 1: Tr√≠ch xu·∫•t Feature m·ªõi HO√ÄN TH√ÄNH ---")
    merge_final_data(df_raw)


# =================================================================
# IV. KH·ªêI CH·∫†Y CH√çNH
# =================================================================
if __name__ == "__main__":
    # S·ª¨A L·ªñI: B·∫°n ph·∫£i x√≥a file log c≈© ƒë·ªÉ ch·∫°y l·∫°i to√†n b·ªô qu√° tr√¨nh
    try:
         if os.path.exists(DETAILED_LOG_FILE):
             os.remove(DETAILED_LOG_FILE)
             print(f"‚ö†Ô∏è ƒê√£ x√≥a file log c≈©: {DETAILED_LOG_FILE} ƒë·ªÉ b·∫Øt ƒë·∫ßu l·∫°i to√†n b·ªô qu√° tr√¨nh tr√≠ch xu·∫•t.")
    except Exception:
         pass
         
    run_multiprocess_extraction()
