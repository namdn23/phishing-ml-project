import pandas as pd
import numpy as np
import os
import time
import math
import io
import socket
import re
import tldextract
import ssl
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright
import imagehash
from PIL import Image
from bs4 import BeautifulSoup
from collections import Counter

# =================================================================
# I. C·∫§U H√åNH H·ªÜ TH·ªêNG (T·ªêI ∆ØU CHO 32GB RAM)
# =================================================================
RAW_CSV_FILE = 'PhiUSIIL_Phishing_URL_Dataset.csv'
TEMP_LOG_FILE = 'extraction_checkpoint.csv'
FINAL_OUTPUT = 'PhiUSIIL_Extracted_Full.csv'

# TH√îNG S·ªê V·∫¨N H√ÄNH AN TO√ÄN
MAX_WORKERS = 20    # S·ªë lu·ªìng ƒë·ªìng th·ªùi (Gi·∫£m t·ª´ 35 xu·ªëng 20 ƒë·ªÉ tr√°nh tr√†n RAM)
CHUNK_SIZE = 40     # S·ªë URL m·ªói lu·ªìng x·ª≠ l√Ω tr∆∞·ªõc khi reset tr√¨nh duy·ªát
TIMEOUT_MS = 10000  # 10 gi√¢y cho m·ªói trang web
TARGET_PHASH = imagehash.hex_to_hash('9880e61f1c7e0c4f')

# C√°c c·ªôt gi·ªØ l·∫°i t·ª´ file g·ªëc
OLD_KEEP_COLS = [
    'URL', 'NoOfDegitsInURL', 'HasDescription', 'HasSocialNet', 'HasPasswordField', 
    'HasSubmitButton', 'HasExternalFormSubmit', 'DomainTitleMatchScore', 
    'IsHTTPS', 'HasCopyrightInfo', 'label'
]

# =================================================================
# II. C√ÅC H√ÄM T√çNH TO√ÅN FEATURE (LOGIC GI·ªÆ NGUY√äN)
# =================================================================
def get_entropy(text):
    if not text or len(text) == 0: return 0.0
    probs = [count/len(text) for count in Counter(text).values()]
    return -sum(p * math.log2(p) for p in probs) / 8.0

def get_tls_issuer(hostname):
    try:
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        with socket.create_connection((hostname, 443), timeout=3) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert = ssock.getpeercert()
                issuer = dict(x[0] for x in cert['issuer'])
                return issuer.get('organizationName', 'Unknown')
    except: return 'None'

# =================================================================
# III. H√ÄM TR√çCH XU·∫§T CHI TI·∫æT
# =================================================================
def extract_full_features(page, url):
    res = {k: 0.0 for k in [
        'V10_HTTP_Extraction_Success', 'V11_WHOIS_Extraction_Success', 'V1_PHash_Distance', 
        'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score', 
        'V8_Total_IFrames', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 
        'V3_Domain_Age_Days', 'V4_DNS_Volatility_Count', 'Is_Top_1M_Domain', 
        'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain'
    ]}
    
    try:
        # T·ªëi ∆∞u t·ªëc ƒë·ªô: B·ªè qua c√°c t√†i nguy√™n kh√¥ng c·∫ßn thi·∫øt
        page.route("**/*", lambda route: route.abort() 
                   if route.request.resource_type in ["image", "media", "font", "other", "stylesheet"] 
                   else route.continue_())

        ext = tldextract.extract(url)
        domain = f"{ext.domain}.{ext.suffix}"
        
        # Features tƒ©nh
        res['V22_IP_Subdomain_Pattern'] = 1 if re.search(r'\d+\.\d+', ext.subdomain) else 0
        res['V23_Entropy_Subdomain'] = get_entropy(ext.subdomain)
        res['Is_Top_1M_Domain'] = 1 if ext.domain in ['google', 'facebook', 'microsoft', 'apple', 'amazon'] else 0
        try:
            res['V4_DNS_Volatility_Count'] = len(socket.gethostbyname_ex(domain)[2])
        except: pass
        res['V5_TLS_Issuer_Reputation'] = 1.0 if get_tls_issuer(domain) != 'None' else 0.0

        # Features ƒë·ªông (Playwright)
        page.goto(url, timeout=TIMEOUT_MS, wait_until="commit") 
        res['V10_HTTP_Extraction_Success'] = 1
        
        # pHash
        img_bytes = page.screenshot()
        img = Image.open(io.BytesIO(img_bytes)).convert('L')
        res['V1_PHash_Distance'] = (imagehash.phash(img) - TARGET_PHASH) / 64.0
        
        soup = BeautifulSoup(page.content(), 'html.parser')
        
        # DOM Depth
        depths = [len(list(t.parents)) for t in soup.find_all(True)]
        res['V2_Layout_Similarity'] = np.clip(1.0 - (max(depths or [0])/40.0), 0, 1)
        
        # JS & Text
        js_code = "".join([s.text for s in soup.find_all('script')])
        res['V6_JS_Entropy'] = get_entropy(js_code)
        
        full_text = soup.get_text()
        words = full_text.split()
        sentences = re.split(r'[.!?]+', full_text)
        res['V7_Text_Readability_Score'] = np.clip(len(words)/(len(sentences) or 1) / 20.0, 0, 1)
        
        # IFrames
        iframes = soup.find_all('iframe')
        res['V8_Total_IFrames'] = len(iframes)
        res['V9_Has_Hidden_IFrame'] = 1 if any('none' in str(f.get('style','')).lower() for f in iframes) else 0
        res['V11_WHOIS_Extraction_Success'] = 1
        
    except Exception:
        res['V10_HTTP_Extraction_Success'] = 0
        res['V1_PHash_Distance'] = 0.5 
    return res

# =================================================================
# IV. QU·∫¢N L√ù LU·ªíNG V√Ä D·ªåN D·∫∏P TI·∫æN TR√åNH
# =================================================================
def thread_worker(chunk_df):
    results = []
    p = sync_playwright().start() # Kh·ªüi ƒë·ªông Playwright driver cho ri√™ng lu·ªìng n√†y
    browser = None
    try:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox", "--disable-dev-shm-usage"])
        context = browser.new_context(viewport={'width': 1280, 'height': 720})
        page = context.new_page()
        
        for _, row in chunk_df.iterrows():
            data = extract_full_features(page, row['URL'])
            data['URL_KEY'] = str(row['URL'])
            results.append(data)
    except Exception as e:
        print(f"‚ö†Ô∏è Error in thread: {e}")
    finally:
        # B·∫ÆT BU·ªòC: D·ªçn d·∫πp ƒë·ªÉ kh√¥ng b·ªã tr√†n RAM
        if browser: browser.close()
        p.stop() 
    return results

def main():
    start_session_time = time.time()
    if not os.path.exists(RAW_CSV_FILE):
        print(f"‚ùå File {RAW_CSV_FILE} kh√¥ng t·ªìn t·∫°i."); return
        
    df_raw = pd.read_csv(RAW_CSV_FILE, usecols=OLD_KEEP_COLS)
    
    # Ki·ªÉm tra checkpoint
    if os.path.exists(TEMP_LOG_FILE):
        processed_urls = set(pd.read_csv(TEMP_LOG_FILE, usecols=['URL_KEY'])['URL_KEY'].astype(str))
    else:
        processed_urls = set()
    
    df_todo = df_raw[~df_raw['URL'].astype(str).isin(processed_urls)]
    total_todo = len(df_todo)
    
    if total_todo == 0:
        print("‚úÖ ƒê√£ x·ª≠ l√Ω xong to√†n b·ªô d·ªØ li·ªáu."); return

    print(f"üöÄ Ti·∫øp t·ª•c t·ª´: {len(processed_urls)} | C√≤n l·∫°i: {total_todo} | Ch·∫°y {MAX_WORKERS} lu·ªìng")

    chunks = [df_todo[i:i + CHUNK_SIZE] for i in range(0, total_todo, CHUNK_SIZE)]
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(thread_worker, c): c for c in chunks}
        
        for i, future in enumerate(as_completed(futures), 1):
            batch_data = future.result()
            if batch_data:
                pd.DataFrame(batch_data).to_csv(TEMP_LOG_FILE, mode='a', index=False, header=not os.path.exists(TEMP_LOG_FILE))
            
            # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
            done_in_session = i * CHUNK_SIZE
            speed = done_in_session / (time.time() - start_session_time)
            print(f"‚ûú [{datetime.now().strftime('%H:%M:%S')}] {len(processed_urls) + done_in_session}/{len(df_raw)} | T·ªëc ƒë·ªô: {speed:.2f} URL/s")

    # V. H·ª¢P NH·∫§T K·∫æT QU·∫¢ CU·ªêI C√ôNG
    print("\nüîÑ ƒêang t·∫°o file k·∫øt qu·∫£ cu·ªëi c√πng...")
    df_new = pd.read_csv(TEMP_LOG_FILE).drop_duplicates('URL_KEY')
    df_final = pd.merge(df_raw, df_new, left_on='URL', right_on='URL_KEY', how='inner')
    
    # T·∫°o c√°c c·ªôt Alarm b√°o l·ªói tr√≠ch xu·∫•t
    for col in ['V1_PHash_Distance', 'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score']:
        df_final[f'Alarm_{col}_Missing'] = (df_final['V10_HTTP_Extraction_Success'] == 0).astype(int)

    df_final.drop(columns=['URL_KEY'], inplace=True)
    df_final.to_csv(FINAL_OUTPUT, index=False)
    print(f"‚úÖ HO√ÄN T·∫§T! File: {FINAL_OUTPUT}")

if __name__ == "__main__":
    main()
