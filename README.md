import pandas as pd
import numpy as np
import os
import time
import math
import io
import socket
import re
import tldextract
import ssl
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright
import imagehash
from PIL import Image
from bs4 import BeautifulSoup
from collections import Counter

# =================================================================
# I. C·∫§U H√åNH T·ªêI ∆ØU (D√ÄNH CHO CPU 12 NH√ÇN & 25GB RAM)
# =================================================================
RAW_CSV_FILE = 'PhiUSIIL_Phishing_URL_Dataset.csv'
TEMP_LOG_FILE = 'extraction_checkpoint.csv'
FINAL_OUTPUT = 'PhiUSIIL_Extracted_Full.csv'

# TH√îNG S·ªê V·∫¨N H√ÄNH ƒê√É TINH CH·ªàNH ƒê·ªÇ KH√îNG B·ªä GI√ÅN ƒêO·∫†N
MAX_WORKERS = 35    # T·∫≠n d·ª•ng 25GB RAM ƒë·ªÉ ch·∫°y 35 lu·ªìng song song
CHUNK_SIZE = 30     # Ghi file theo c·ª•m 30 URL ƒë·ªÉ m√°y kh√¥ng b·ªã ƒë·ª©ng l√¢u
TIMEOUT_MS = 10000  # Gi·∫£m xu·ªëng 10s ƒë·ªÉ t·ª± ƒë·ªông b·ªè qua c√°c URL "treo"
TARGET_PHASH = imagehash.hex_to_hash('9880e61f1c7e0c4f')

# C√°c c·ªôt gi·ªØ l·∫°i t·ª´ file g·ªëc
OLD_KEEP_COLS = [
    'URL', 'NoOfDegitsInURL', 'HasDescription', 'HasSocialNet', 'HasPasswordField', 
    'HasSubmitButton', 'HasExternalFormSubmit', 'DomainTitleMatchScore', 
    'IsHTTPS', 'HasCopyrightInfo', 'label'
]

# =================================================================
# II. H√ÄM H·ªñ TR·ª¢ T√çNH TO√ÅN
# =================================================================
def get_entropy(text):
    if not text or len(text) == 0: return 0.0
    probs = [count/len(text) for count in Counter(text).values()]
    return -sum(p * math.log2(p) for p in probs) / 8.0

def get_tls_issuer(hostname):
    try:
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        with socket.create_connection((hostname, 443), timeout=3) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert = ssock.getpeercert()
                issuer = dict(x[0] for x in cert['issuer'])
                return issuer.get('organizationName', 'Unknown')
    except: return 'None'

# =================================================================
# III. LOGIC TR√çCH XU·∫§T (T·ªêI ∆ØU T·ªêC ƒê·ªò & TI·∫æT KI·ªÜM DATA)
# =================================================================
def extract_full_features(page, url):
    # Kh·ªüi t·∫°o t·∫•t c·∫£ 14 features ƒë·ªông
    res = {k: 0.0 for k in [
        'V10_HTTP_Extraction_Success', 'V11_WHOIS_Extraction_Success', 'V1_PHash_Distance', 
        'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score', 
        'V8_Total_IFrames', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 
        'V3_Domain_Age_Days', 'V4_DNS_Volatility_Count', 'Is_Top_1M_Domain', 
        'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain'
    ]}
    
    try:
        # CH·∫∂N T√ÄI NGUY√äN R√ÅC: Ch·∫∑n c·∫£ CSS (stylesheet) ƒë·ªÉ tƒÉng t·ªëc t·ªëi ƒëa
        page.route("**/*", lambda route: route.abort() 
                   if route.request.resource_type in ["image", "media", "font", "other", "stylesheet"] 
                   else route.continue_())

        ext = tldextract.extract(url)
        domain = f"{ext.domain}.{ext.suffix}"
        
        # 1. TR√çCH XU·∫§T Tƒ®NH (DNS & URL)
        res['V22_IP_Subdomain_Pattern'] = 1 if re.search(r'\d+\.\d+', ext.subdomain) else 0
        res['V23_Entropy_Subdomain'] = get_entropy(ext.subdomain)
        res['Is_Top_1M_Domain'] = 1 if ext.domain in ['google', 'facebook', 'microsoft', 'apple', 'amazon'] else 0
        try:
            res['V4_DNS_Volatility_Count'] = len(socket.gethostbyname_ex(domain)[2])
        except: res['V4_DNS_Volatility_Count'] = 0
        
        res['V5_TLS_Issuer_Reputation'] = 1.0 if get_tls_issuer(domain) != 'None' else 0.0

        # 2. TR√çCH XU·∫§T ƒê·ªòNG (PLAYWRIGHT)
        # S·ª≠ d·ª•ng wait_until="commit" ƒë·ªÉ l·∫•y d·ªØ li·ªáu ngay khi server ph·∫£n h·ªìi
        page.goto(url, timeout=TIMEOUT_MS, wait_until="commit") 
        res['V10_HTTP_Extraction_Success'] = 1
        
        # V1: PHash Distance
        img_bytes = page.screenshot()
        img = Image.open(io.BytesIO(img_bytes)).convert('L')
        res['V1_PHash_Distance'] = (imagehash.phash(img) - TARGET_PHASH) / 64.0
        
        soup = BeautifulSoup(page.content(), 'html.parser')
        
        # V2: Layout Similarity (ƒê·ªô s√¢u DOM)
        depths = [len(list(t.parents)) for t in soup.find_all(True)]
        res['V2_Layout_Similarity'] = np.clip(1.0 - (max(depths or [0])/40.0), 0, 1)
        
        # V6: JS Entropy
        js_code = "".join([s.text for s in soup.find_all('script')])
        res['V6_JS_Entropy'] = get_entropy(js_code)
        
        # V7: Text Readability Score
        full_text = soup.get_text()
        words = full_text.split()
        sentences = re.split(r'[.!?]+', full_text)
        res['V7_Text_Readability_Score'] = np.clip(len(words)/(len(sentences) or 1) / 20.0, 0, 1)
        
        # V8, V9: IFrames
        iframes = soup.find_all('iframe')
        res['V8_Total_IFrames'] = len(iframes)
        res['V9_Has_Hidden_IFrame'] = 1 if any('none' in str(f.get('style','')).lower() for f in iframes) else 0
        
        res['V11_WHOIS_Extraction_Success'] = 1
        
    except Exception:
        # N·∫øu l·ªói, ƒë√°nh d·∫•u th·∫•t b·∫°i v√† ƒë·∫∑t pHash trung b√¨nh
        res['V10_HTTP_Extraction_Success'] = 0
        res['V1_PHash_Distance'] = 0.5 
        
    return res

# =================================================================
# IV. QU·∫¢N L√ù ƒêA LU·ªíNG & CHECKPOINT
# =================================================================
def thread_worker(chunk_df):
    results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox", "--disable-gpu"])
        context = browser.new_context(viewport={'width': 1280, 'height': 720})
        page = context.new_page()
        
        for _, row in chunk_df.iterrows():
            data = extract_full_features(page, row['URL'])
            data['URL_KEY'] = row['URL']
            results.append(data)
            
        browser.close()
    return results

def main():
    start_session_time = time.time()
    if not os.path.exists(RAW_CSV_FILE):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {RAW_CSV_FILE}")
        return
        
    df_raw = pd.read_csv(RAW_CSV_FILE, usecols=OLD_KEEP_COLS)
    
    if os.path.exists(TEMP_LOG_FILE):
        df_checkpoint = pd.read_csv(TEMP_LOG_FILE, usecols=['URL_KEY'])
        processed_urls = set(df_checkpoint['URL_KEY'].astype(str))
    else:
        processed_urls = set()
    
    df_todo = df_raw[~df_raw['URL'].astype(str).isin(processed_urls)]
    num_already_done = len(processed_urls)
    total_todo = len(df_todo)
    
    if total_todo == 0:
        print("‚úÖ To√†n b·ªô URL ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω!"); return

    print(f"üöÄ Ti·∫øp t·ª•c t·ª´: {num_already_done} | C√≤n l·∫°i: {total_todo} | Ch·∫°y {MAX_WORKERS} lu·ªìng")

    chunks = [df_todo[i:i + CHUNK_SIZE] for i in range(0, total_todo, CHUNK_SIZE)]
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(thread_worker, c): c for c in chunks}
        
        for i, future in enumerate(as_completed(futures), 1):
            batch_data = future.result()
            # Ghi mode 'a' ƒë·ªÉ n·ªëi ti·∫øp v√†o file checkpoint
            pd.DataFrame(batch_data).to_csv(TEMP_LOG_FILE, mode='a', index=False, header=not os.path.exists(TEMP_LOG_FILE))
            
            # T√≠nh to√°n v√† hi·ªÉn th·ªã ti·∫øn ƒë·ªô
            current_done = num_already_done + (i * CHUNK_SIZE)
            elapsed = time.time() - start_session_time
            speed = (i * CHUNK_SIZE) / elapsed
            print(f"‚ûú [{datetime.now().strftime('%H:%M:%S')}] {current_done}/{len(df_raw)} "
                  f"| T·ªëc ƒë·ªô: {speed:.2f} URL/s | ETA: {(len(df_raw) - current_done)/speed/60:.1f} ph√∫t")

    # V. H·ª¢P NH·∫§T & T·∫†O C·ªòT ALARM (GI·ªêNG CODE C≈®)
    print("\nüîÑ ƒêang ho√†n t·∫•t file k·∫øt qu·∫£ cu·ªëi c√πng...")
    df_new = pd.read_csv(TEMP_LOG_FILE).drop_duplicates('URL_KEY')
    df_final = pd.merge(df_raw, df_new, left_on='URL', right_on='URL_KEY', how='inner')
    
    # Kh√¥i ph·ª•c ƒë·∫ßy ƒë·ªß c√°c c·ªôt Alarm
    for col in ['V1_PHash_Distance', 'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score']:
        df_final[f'Alarm_{col}_Missing'] = np.where(df_final['V10_HTTP_Extraction_Success'] == 0, 1, 0)

    df_final.drop(columns=['URL_KEY'], inplace=True)
    df_final.to_csv(FINAL_OUTPUT, index=False)
    print(f"‚úÖ HO√ÄN T·∫§T! File: {FINAL_OUTPUT}")

if __name__ == "__main__":
    main()
