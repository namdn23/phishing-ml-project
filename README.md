import pandas as pd
import tldextract
import time
import json
import os
import random
import threading
import concurrent.futures
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from tqdm import tqdm

# ==================== C·∫§U H√åNH SI√äU C·∫§P ====================
INPUT_FILE = 'DATA_TRAIN_FINAL.csv'
OUTPUT_FILE = 'DATA_TRAIN_ULTIMATE.csv'
CHECKPOINT_FILE = 'checkpoint_ultimate.json'

# üî• C·∫§U H√åNH CHO M√ÅY M·∫†NH (INTEL ULTRA 5H + 27GB RAM)
MAX_WORKERS = 10  # Ch·∫°y 10 tr√¨nh duy·ªát c√πng l√∫c

# Danh s√°ch Brand l·ªõn (S·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn b√π d·ªØ li·ªáu chu·∫©n, KH√îNG CRAWL ƒë·ªÉ tr√°nh b·ªã ch·∫∑n)
DOMAIN_PROFILES = {
    'google':    {'Total_IFrames': 3, 'Has_Hidden_IFrame': 0, 'Is_Trusted_Issuer': 1, 'Certificate_Age': 365, 'Has_External_Form': 0, 'Has_Obfuscated_JS': 0},
    'facebook':  {'Total_IFrames': 2, 'Has_Hidden_IFrame': 0, 'Is_Trusted_Issuer': 1, 'Certificate_Age': 365, 'Has_External_Form': 0, 'Has_Obfuscated_JS': 0},
    'shopee':    {'Total_IFrames': 5, 'Has_Hidden_IFrame': 0, 'Is_Trusted_Issuer': 1, 'Certificate_Age': 300, 'Has_External_Form': 0, 'Has_Obfuscated_JS': 0},
    'amazon':    {'Total_IFrames': 2, 'Has_Hidden_IFrame': 0, 'Is_Trusted_Issuer': 1, 'Certificate_Age': 365, 'Has_External_Form': 0, 'Has_Obfuscated_JS': 0},
    'microsoft': {'Total_IFrames': 1, 'Has_Hidden_IFrame': 0, 'Is_Trusted_Issuer': 1, 'Certificate_Age': 365, 'Has_External_Form': 0, 'Has_Obfuscated_JS': 0},
    'paypal':    {'Total_IFrames': 1, 'Has_Hidden_IFrame': 0, 'Is_Trusted_Issuer': 1, 'Certificate_Age': 365, 'Has_External_Form': 0, 'Has_Obfuscated_JS': 0},
    'apple':     {'Total_IFrames': 1, 'Has_Hidden_IFrame': 0, 'Is_Trusted_Issuer': 1, 'Certificate_Age': 365, 'Has_External_Form': 0, 'Has_Obfuscated_JS': 0},
    'netflix':   {'Total_IFrames': 1, 'Has_Hidden_IFrame': 0, 'Is_Trusted_Issuer': 1, 'Certificate_Age': 365, 'Has_External_Form': 0, 'Has_Obfuscated_JS': 0},
    'instagram': {'Total_IFrames': 1, 'Has_Hidden_IFrame': 0, 'Is_Trusted_Issuer': 1, 'Certificate_Age': 365, 'Has_External_Form': 0, 'Has_Obfuscated_JS': 0},
}

PROTECTED_BRANDS = list(DOMAIN_PROFILES.keys()) + ['tiki', 'lazada', 'vietcombank', 'techcombank']

# Kh√≥a an to√†n cho lu·ªìng
file_lock = threading.Lock()

# ==================== LOGIC X·ª¨ L√ù ====================

def get_domain_parts(url):
    try:
        ext = tldextract.extract(str(url))
        root = f"{ext.domain}.{ext.suffix}"
        return ext.domain, ext.suffix, ext.subdomain, root
    except: return "", "", "", ""

def fix_static_features(row):
    """B∆∞·ªõc 1: S·ª≠a logic tƒ©nh (Ch·∫°y si√™u nhanh)"""
    url = str(row['url'])
    domain, suffix, subdomain, root = get_domain_parts(url)
    
    # 1. S·ª≠a l·ªói Brand In Subdomain
    # Logic: Brand ·ªü sub nh∆∞ng Root c≈©ng l√† Brand -> AN TO√ÄN (0)
    has_brand_issue = 0
    for b in PROTECTED_BRANDS:
        if b in subdomain:
            if b == domain: has_brand_issue = 0 # Ch√≠nh ch·ªß
            else: has_brand_issue = 1           # Gi·∫£ m·∫°o
    row['Brand_In_Subdomain'] = has_brand_issue
    
    # 2. C·∫≠p nh·∫≠t TLD r√°c
    SUSPICIOUS_TLDS = ['tk', 'ml', 'ga', 'cf', 'gq', 'xyz', 'top', 'vip', 'work']
    row['Suspicious_TLD'] = 1 if suffix in SUSPICIOUS_TLDS else 0
    
    return row, domain # Tr·∫£ v·ªÅ domain body ƒë·ªÉ check ti·∫øp

def process_chunk_worker(df_chunk, worker_id):
    """H√†m x·ª≠ l√Ω cho t·ª´ng lu·ªìng"""
    results = []
    
    # Kh·ªüi t·∫°o tr√¨nh duy·ªát (Ch·ªâ m·ªü n·∫øu c·∫ßn crawl)
    driver = None
    
    # L·ªçc ra nh·ªØng URL c·∫ßn crawl (Kh√¥ng n·∫±m trong Profile v√† b·ªã thi·∫øu d·ªØ li·ªáu)
    urls_to_crawl = []
    for idx, row in df_chunk.iterrows():
        domain, _, _, _ = get_domain_parts(row['url'])
        # N·∫øu kh√¥ng ph·∫£i √¥ng l·ªõn V√Ä d·ªØ li·ªáu b·ªã thi·∫øu -> C·∫ßn crawl
        if domain not in DOMAIN_PROFILES and (row['Total_IFrames'] == 0 and row['Has_External_Form'] == 0):
            urls_to_crawl.append(idx)

    # N·∫øu c√≥ URL c·∫ßn crawl th√¨ m·ªõi m·ªü Chrome
    if len(urls_to_crawl) > 0:
        try:
            options = uc.ChromeOptions()
            options.add_argument("--headless=new")
            options.add_argument("--no-sandbox")
            options.add_argument("--mute-audio")
            driver = uc.Chrome(options=options)
            driver.set_page_load_timeout(15)
            # print(f"[Worker {worker_id}] üü¢ ƒê√£ m·ªü Chrome ({len(urls_to_crawl)} tasks)")
        except: pass

    # --- V√íNG L·∫∂P X·ª¨ L√ù T·ª™NG D√íNG ---
    for idx, row in df_chunk.iterrows():
        # 1. S·ª≠a l·ªói Tƒ©nh
        row, domain_body = fix_static_features(row)
        
        # 2. X·ª≠ l√Ω D·ªØ li·ªáu ƒê·ªông
        
        # TR∆Ø·ªúNG H·ª¢P A: L√Ä √îNG L·ªöN (Google, FB...) -> D√πng d·ªØ li·ªáu chu·∫©n (IMPUTATION)
        if domain_body in DOMAIN_PROFILES:
            profile = DOMAIN_PROFILES[domain_body]
            
            # N·∫øu d·ªØ li·ªáu c≈© b·ªã l·ªói (0 ho·∫∑c √¢m), ƒëi·ªÅn d·ªØ li·ªáu chu·∫©n v√†o
            if row['Total_IFrames'] <= 0: row['Total_IFrames'] = profile['Total_IFrames']
            if row['Is_Trusted_Issuer'] <= 0: 
                row['Is_Trusted_Issuer'] = profile['Is_Trusted_Issuer']
                row['Certificate_Age'] = profile['Certificate_Age']
                row['Certificate_Validity_Days'] = 365
            
            # Reset c√°c c·ªù ph·∫°t oan
            row['Has_Hidden_IFrame'] = 0  
            row['Has_External_Form'] = 0
            row['Has_Obfuscated_JS'] = 0
            row['Brand_Impersonation'] = 0

        # TR∆Ø·ªúNG H·ª¢P B: L√Ä WEB L·∫† V√Ä THI·∫æU D·ªÆ LI·ªÜU -> CRAWL TH·∫¨T
        elif idx in urls_to_crawl and driver:
            try:
                url = row['url']
                driver.get(url)
                time.sleep(random.uniform(2, 3))
                
                soup = BeautifulSoup(driver.page_source.lower(), 'html.parser')
                current_root = get_domain_parts(url)[3]
                
                # Tr√≠ch xu·∫•t External Form
                has_ext_form = 0
                for f in soup.find_all('form'):
                    act = f.get('action', '').lower()
                    if act.startswith('http'):
                        act_root = get_domain_parts(act)[3]
                        if act_root and act_root != current_root: has_ext_form = 1; break
                row['Has_External_Form'] = has_ext_form

                # Tr√≠ch xu·∫•t Iframe
                iframes = soup.find_all('iframe')
                row['Total_IFrames'] = len(iframes)
                
                # Check Hidden Iframe
                has_bad_iframe = 0
                for i in iframes:
                    src = i.get('src', '').lower()
                    style = str(i.get('style', '')).lower()
                    if ('display:none' in style or 'visibility:hidden' in style) and src.startswith('http'):
                        src_root = get_domain_parts(src)[3]
                        if src_root and src_root != current_root and 'google' not in src_root:
                            has_bad_iframe = 1
                row['Has_Hidden_IFrame'] = has_bad_iframe
                
                # Check JS
                row['Has_Obfuscated_JS'] = 1 if 'eval(' in str(soup) else 0

            except: pass # N·∫øu crawl l·ªói th√¨ gi·ªØ nguy√™n s·ªë 0 c≈©

        # TR∆Ø·ªúNG H·ª¢P C: WEB L·∫† NH∆ØNG ƒê√É C√ì D·ªÆ LI·ªÜU -> GI·ªÆ NGUY√äN (Kh√¥ng t·ªën time crawl l·∫°i)
        else:
            pass 

        results.append((idx, row))

    if driver: 
        try: driver.quit()
        except: pass
        
    return results

# ==================== MAIN ====================
def main():
    print(f"üöÄ DATASET REPAIR KIT ULTIMATE (10 WORKERS)")
    print(f"üéØ Chi·∫øn thu·∫≠t: Imputation (Google/FB) + Crawling (Web l·∫°)")
    
    # 1. Load Data
    try: full_df = pd.read_csv(INPUT_FILE)
    except: print("‚ùå Kh√¥ng th·∫•y file input"); return

    # 2. Load Checkpoint
    processed_indices = set()
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            processed_indices = set(json.load(f))
    
    # L·ªçc d√≤ng ch∆∞a x·ª≠ l√Ω
    full_df['temp_idx'] = full_df.index
    remaining_df = full_df[~full_df.index.isin(processed_indices)]
    
    print(f"üìä T·ªïng: {len(full_df)} | ƒê√£ xong: {len(processed_indices)} | C√≤n l·∫°i: {len(remaining_df)}")
    if len(remaining_df) == 0: return

    # 3. Chia Chunk (M·ªói worker l√†m 50 d√≤ng r·ªìi ngh·ªâ ƒë·ªÉ gi·∫£i ph√≥ng RAM)
    CHUNK_SIZE = 50 
    chunks = [remaining_df[i:i + CHUNK_SIZE] for i in range(0, len(remaining_df), CHUNK_SIZE)]
    
    print(f"üì¶ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(chunks)} g√≥i...")
    pbar = tqdm(total=len(remaining_df))
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_chunk_worker, chunk, i % MAX_WORKERS): chunk for i, chunk in enumerate(chunks)}
        
        for future in concurrent.futures.as_completed(futures):
            try:
                results = future.result()
                if not results: continue

                with file_lock:
                    for idx, row in results:
                        full_df.loc[idx] = row
                        processed_indices.add(int(idx))
                    
                    # L∆∞u checkpoint
                    full_df.drop(columns=['temp_idx'], errors='ignore').to_csv(OUTPUT_FILE, index=False)
                    with open(CHECKPOINT_FILE, 'w') as f:
                        json.dump(list(processed_indices), f)
                
                pbar.update(len(results))
            except Exception as e:
                print(f"‚ùå L·ªói: {e}")

    pbar.close()
    if 'temp_idx' in full_df.columns: full_df.drop(columns=['temp_idx'], inplace=True)
    
    print("\n‚úÖ HO√ÄN T·∫§T! File 'DATA_TRAIN_ULTIMATE.csv' ƒë√£ s·∫µn s√†ng ƒë·ªÉ Train.")

if __name__ == "__main__":
    main()
