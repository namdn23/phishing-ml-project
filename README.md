import pandas as pd
import numpy as np
import os
import time
import math
import io
import socket
import re
import tldextract
import ssl
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright
import imagehash
from PIL import Image
from bs4 import BeautifulSoup
from collections import Counter

# =================================================================
# I. C·∫§U H√åNH & DANH S√ÅCH FEATURES
# =================================================================
RAW_CSV_FILE = 'PhiUSIIL_Phishing_URL_Dataset.csv'
TEMP_LOG_FILE = 'extraction_checkpoint.csv'
FINAL_OUTPUT = 'PhiUSIIL_Extracted_Full.csv'

# T·ªêI ∆ØU CHO M√ÅY ·∫¢O 8 NH√ÇN VT-x
MAX_WORKERS = 20  # TƒÉng l√™n 20 lu·ªìng ƒë·ªÉ t·∫≠n d·ª•ng CPU Intel Ultra 5H
CHUNK_SIZE = 10   # M·ªói lu·ªìng x·ª≠ l√Ω 10 URL r·ªìi reset tr√¨nh duy·ªát ƒë·ªÉ gi·∫£i ph√≥ng RAM
TARGET_PHASH = imagehash.hex_to_hash('9880e61f1c7e0c4f')

# C√°c c·ªôt l·∫•y t·ª´ file c≈©
OLD_KEEP_COLS = [
    'URL', 'NoOfDegitsInURL', 'HasDescription', 'HasSocialNet', 'HasPasswordField', 
    'HasSubmitButton', 'HasExternalFormSubmit', 'DomainTitleMatchScore', 
    'IsHTTPS', 'HasCopyrightInfo', 'label'
]

# =================================================================
# II. H√ÄM H·ªñ TR·ª¢ T√çNH TO√ÅN
# =================================================================
def get_entropy(text):
    if not text or len(text) == 0: return 0.0
    probs = [count/len(text) for count in Counter(text).values()]
    return -sum(p * math.log2(p) for p in probs) / 8.0

def get_tls_issuer(hostname):
    try:
        context = ssl.create_default_context()
        with socket.create_connection((hostname, 443), timeout=3) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert = ssock.getpeercert()
                issuer = dict(x[0] for x in cert['issuer'])
                return issuer.get('organizationName', 'Unknown')
    except: return 'None'

# =================================================================
# III. LOGIC TR√çCH XU·∫§T T·ªîNG H·ª¢P
# =================================================================
def extract_full_features(page, url):
    res = {k: 0.0 for k in [
        'V10_HTTP_Extraction_Success', 'V11_WHOIS_Extraction_Success', 'V1_PHash_Distance', 
        'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score', 
        'V8_Total_IFrames', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 
        'V3_Domain_Age_Days', 'V4_DNS_Volatility_Count', 'Is_Top_1M_Domain', 
        'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain'
    ]}
    
    try:
        ext = tldextract.extract(url)
        domain = f"{ext.domain}.{ext.suffix}"
        
        # 1. TR√çCH XU·∫§T Tƒ®NH
        res['V22_IP_Subdomain_Pattern'] = 1 if re.search(r'\d+\.\d+', ext.subdomain) else 0
        res['V23_Entropy_Subdomain'] = get_entropy(ext.subdomain)
        res['Is_Top_1M_Domain'] = 1 if ext.domain in ['google', 'facebook', 'microsoft', 'apple', 'amazon'] else 0
        try:
            res['V4_DNS_Volatility_Count'] = len(socket.gethostbyname_ex(domain)[2])
        except: res['V4_DNS_Volatility_Count'] = 0
        
        issuer = get_tls_issuer(domain)
        res['V5_TLS_Issuer_Reputation'] = 1.0 if issuer != 'None' else 0.0

        # 2. TR√çCH XU·∫§T ƒê·ªòNG (PLAYWRIGHT)
        page.goto(url, timeout=25000, wait_until="domcontentloaded") # TƒÉng t·ªëc b·∫±ng c√°ch ƒë·ª£i DOM xong thay v√¨ load to√†n b·ªô
        res['V10_HTTP_Extraction_Success'] = 1
        
        # V1: PHash Distance (Screenshot)
        img_bytes = page.screenshot()
        img = Image.open(io.BytesIO(img_bytes)).convert('L')
        res['V1_PHash_Distance'] = (imagehash.phash(img) - TARGET_PHASH) / 64.0
        
        # V2: Layout Similarity
        soup = BeautifulSoup(page.content(), 'html.parser')
        depths = [len(list(t.parents)) for t in soup.find_all(True)]
        res['V2_Layout_Similarity'] = np.clip(1.0 - (max(depths or [0])/40.0), 0, 1)
        
        # V6: JS Entropy
        js_code = "".join([s.text for s in soup.find_all('script')])
        res['V6_JS_Entropy'] = get_entropy(js_code)
        
        # V7: Text Readability
        full_text = soup.get_text()
        words = full_text.split()
        sentences = re.split(r'[.!?]+', full_text)
        res['V7_Text_Readability_Score'] = np.clip(len(words)/(len(sentences) or 1) / 20.0, 0, 1)
        
        # V8, V9: IFrames
        iframes = soup.find_all('iframe')
        res['V8_Total_IFrames'] = len(iframes)
        res['V9_Has_Hidden_IFrame'] = 1 if any('none' in str(f.get('style','')).lower() for f in iframes) else 0
        
        res['V11_WHOIS_Extraction_Success'] = 1
        
    except Exception:
        res['V10_HTTP_Extraction_Success'] = 0
        res['V1_PHash_Distance'] = 0.5 
        
    return res

# =================================================================
# IV. QU·∫¢N L√ù ƒêA LU·ªíNG
# =================================================================
def thread_worker(chunk_df):
    results = []
    with sync_playwright() as p:
        # T·ªëi ∆∞u Chrome: T·∫Øt sandbox, t·∫Øt GPU, b·ªè qua r√°c ƒë·ªÉ tƒÉng t·ªëc tr√™n VM
        browser = p.chromium.launch(headless=True, args=[
            "--no-sandbox", 
            "--disable-dev-shm-usage",
            "--disable-gpu",
            "--js-flags='--max-old-space-size=512'"
        ])
        context = browser.new_context(viewport={'width': 1280, 'height': 720})
        page = context.new_page()
        
        for _, row in chunk_df.iterrows():
            data = extract_full_features(page, row['URL'])
            data['URL_KEY'] = row['URL']
            results.append(data)
            
        browser.close()
    return results

def main():
    start_session_time = time.time()
    
    # 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
    if not os.path.exists(RAW_CSV_FILE):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y {RAW_CSV_FILE}"); return
    df_raw = pd.read_csv(RAW_CSV_FILE, usecols=OLD_KEEP_COLS)
    total_all = len(df_raw)

    # 2. X·ª≠ l√Ω Checkpoint (ƒê·ªçc c√°c URL ƒë√£ l√†m xong)
    if os.path.exists(TEMP_LOG_FILE):
        df_checkpoint = pd.read_csv(TEMP_LOG_FILE, usecols=['URL_KEY'])
        processed_urls = set(df_checkpoint['URL_KEY'].astype(str))
    else:
        processed_urls = set()
    
    # L·ªçc danh s√°ch c√≤n l·∫°i
    df_todo = df_raw[~df_raw['URL'].astype(str).isin(processed_urls)]
    num_already_done = len(processed_urls)
    total_todo = len(df_todo)
    
    if total_todo == 0:
        print("‚úÖ ƒê√£ ho√†n th√†nh x·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu."); return

    print(f"üöÄ T·ªïng c·ªông: {total_all} URL")
    print(f"üì¶ ƒê√£ xong: {num_already_done} | C√≤n l·∫°i: {total_todo}")
    print(f"üî• ƒêang kh·ªüi ch·∫°y {MAX_WORKERS} lu·ªìng...")

    # 3. Ch·∫°y ƒëa lu·ªìng
    chunks = [df_todo[i:i + CHUNK_SIZE] for i in range(0, total_todo, CHUNK_SIZE)]
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(thread_worker, c): c for c in chunks}
        
        for i, future in enumerate(as_completed(futures), 1):
            batch_data = future.result()
            
            # L∆∞u ngay xu·ªëng file checkpoint
            pd.DataFrame(batch_data).to_csv(TEMP_LOG_FILE, mode='a', index=False, header=not os.path.exists(TEMP_LOG_FILE))
            
            # T√≠nh to√°n ti·∫øn ƒë·ªô c·ªông d·ªìn
            current_batch_done = i * CHUNK_SIZE
            total_current_done = num_already_done + current_batch_done
            
            # T√≠nh t·ªëc ƒë·ªô tr√™n phi√™n hi·ªán t·∫°i
            elapsed_session = time.time() - start_session_time
            speed = current_batch_done / elapsed_session
            
            # Tr√°nh hi·ªÉn th·ªã v∆∞·ª£t qu√° 100% do sai s·ªë chunk cu·ªëi
            display_done = min(total_current_done, total_all)
            
            print(f"‚ûú Ti·∫øn ƒë·ªô: {display_done}/{total_all} ({display_done/total_all*100:.2f}%) "
                  f"| T·ªëc ƒë·ªô: {speed:.2f} URL/s | ETA: {(total_all - display_done)/speed/60:.1f} ph√∫t")

    # 4. H·ª£p nh·∫•t cu·ªëi c√πng
    print("\nüîÑ ƒêang t·∫°o file k·∫øt qu·∫£ cu·ªëi c√πng...")
    df_new = pd.read_csv(TEMP_LOG_FILE).drop_duplicates('URL_KEY')
    df_final = pd.merge(df_raw, df_new, left_on='URL', right_on='URL_KEY', how='inner')
    
    # T·∫°o bi·∫øn Alarms
    for col in ['V1_PHash_Distance', 'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score']:
        df_final[f'Alarm_{col}_Missing'] = np.where(df_final['V10_HTTP_Extraction_Success'] == 0, 1, 0)

    df_final.drop(columns=['URL_KEY'], inplace=True)
    df_final.to_csv(FINAL_OUTPUT, index=False)
    
    print(f"‚úÖ HO√ÄN T·∫§T! File: {FINAL_OUTPUT}")

if __name__ == "__main__":
    main()
