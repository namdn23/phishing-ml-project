import pandas as pd
import numpy as np
import os, time, math, io, socket, re, tldextract, ssl, threading
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright
import imagehash
from PIL import Image
from bs4 import BeautifulSoup
from collections import Counter

# =================================================================
# I. C·∫§U H√åNH V·∫¨N H√ÄNH T·ªêI ∆ØU (D√ÄNH CHO KALI LINUX)
# =================================================================
RAW_CSV_FILE = 'PhiUSIIL_Phishing_URL_Dataset.csv'
TEMP_LOG_FILE = 'extraction_checkpoint.csv'
FINAL_OUTPUT = 'PhiUSIIL_Extracted_Full.csv'

# Gi·∫£m xu·ªëng 15 lu·ªìng ƒë·ªÉ tr√°nh ngh·∫Ωn CPU/RAM g√¢y ƒë·ª©ng m√°y
MAX_WORKERS = 15    
CHUNK_SIZE = 30     
TIMEOUT_MS = 20000  
TARGET_PHASH = imagehash.hex_to_hash('9880e61f1c7e0c4f')

# Lock ƒë·ªÉ ghi file CSV an to√†n, kh√¥ng b·ªã d√≠nh d√≤ng (corrupted)
csv_lock = threading.Lock()

OLD_KEEP_COLS = [
    'URL', 'NoOfDegitsInURL', 'HasDescription', 'HasSocialNet', 'HasPasswordField', 
    'HasSubmitButton', 'HasExternalFormSubmit', 'DomainTitleMatchScore', 
    'IsHTTPS', 'HasCopyrightInfo', 'label'
]

# =================================================================
# II. C√îNG C·ª§ H·ªñ TR·ª¢ (GI·ªÆ NGUY√äN LOGIC)
# =================================================================
def get_entropy(text):
    if not text or len(text) == 0: return 0.0
    probs = [count/len(text) for count in Counter(text).values()]
    return -sum(p * math.log2(p) for p in probs) / 8.0

def get_tls_issuer(hostname):
    try:
        context = ssl.create_default_context()
        context.check_hostname, context.verify_mode = False, ssl.CERT_NONE
        with socket.create_connection((hostname, 443), timeout=4) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert = ssock.getpeercert()
                issuer = dict(x[0] for x in cert['issuer'])
                return issuer.get('organizationName', 'Unknown')
    except: return 'None'

# =================================================================
# III. LOGIC TR√çCH XU·∫§T (GI·ªÆ NGUY√äN ƒê·∫∂C TR∆ØNG - T·ªêI ∆ØU T·ªêC ƒê·ªò T·∫¢I)
# =================================================================
def extract_full_features(page, url, retry=0):
    res = {k: 0.0 for k in [
        'V10_HTTP_Extraction_Success', 'V11_WHOIS_Extraction_Success', 'V1_PHash_Distance', 
        'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score', 
        'V8_Total_IFrames', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 
        'V3_Domain_Age_Days', 'V4_DNS_Volatility_Count', 'Is_Top_1M_Domain', 
        'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain'
    ]}

    try:
        # Ch·∫∑n t√†i nguy√™n r√°c ƒê·ªÇ TƒÇNG T·ªêC (V·∫´n gi·ªØ Script ƒë·ªÉ t√≠nh Entropy)
        page.route("**/*", lambda route: route.abort() if route.request.resource_type in ["image", "media", "font"] else route.continue_())

        # ƒê·∫∑c tr∆∞ng Domain
        ext = tldextract.extract(url)
        domain = f"{ext.domain}.{ext.suffix}"
        res['V22_IP_Subdomain_Pattern'] = 1 if re.search(r'\d+\.\d+', ext.subdomain) else 0
        res['V23_Entropy_Subdomain'] = get_entropy(ext.subdomain)
        res['Is_Top_1M_Domain'] = 1 if ext.domain in ['google', 'facebook', 'microsoft', 'apple', 'amazon'] else 0
        
        try: res['V4_DNS_Volatility_Count'] = len(socket.gethostbyname_ex(domain)[2])
        except: pass
        res['V5_TLS_Issuer_Reputation'] = 1.0 if get_tls_issuer(domain) != 'None' else 0.0

        # TR·ªåNG T√ÇM: S·ª≠ d·ª•ng domcontentloaded ƒë·ªÉ kh√¥ng b·ªã treo ·ªü c√°c script qu·∫£ng c√°o
        response = page.goto(url, timeout=TIMEOUT_MS, wait_until="domcontentloaded")
        if not response or response.status >= 400: raise Exception("Blocked/Error")

        res['V10_HTTP_Extraction_Success'] = 1
        
        # pHash (V·∫´n ch·ª•p ·∫£nh ƒë·ªÉ l·∫•y Layout)
        try:
            img_bytes = page.screenshot(timeout=5000)
            img = Image.open(io.BytesIO(img_bytes)).convert('L')
            res['V1_PHash_Distance'] = (imagehash.phash(img) - TARGET_PHASH) / 64.0
        except: res['V1_PHash_Distance'] = 0.5

        # Ph√¢n t√≠ch DOM
        content = page.content()
        soup = BeautifulSoup(content, 'lxml') 
        text_content = soup.get_text().strip()
        
        depths = [len(list(t.parents)) for t in soup.find_all(True)]
        res['V2_Layout_Similarity'] = np.clip(1.0 - (max(depths or [0])/50.0), 0, 1)
        res['V6_JS_Entropy'] = get_entropy("".join([s.text for s in soup.find_all('script')]))
        
        words, sents = text_content.split(), re.split(r'[.!?]+', text_content)
        res['V7_Text_Readability_Score'] = np.clip(len(words)/(len(sents) or 1) / 25.0, 0, 1)
        
        iframes = soup.find_all('iframe')
        res['V8_Total_IFrames'] = len(iframes)
        res['V9_Has_Hidden_IFrame'] = 1 if any('none' in str(f.get('style','')).lower() for f in iframes) else 0
        res['V11_WHOIS_Extraction_Success'] = 1

    except Exception:
        if retry < 1:
            time.sleep(2)
            return extract_full_features(page, url, retry=1)
        res['V10_HTTP_Extraction_Success'] = 0
        res['V1_PHash_Distance'] = 0.5 
    return res

def thread_worker(chunk_df):
    results = []
    with sync_playwright() as p:
        # T·ªëi ∆∞u h√≥a tham s·ªë d√≤ng l·ªánh Chromium
        browser = p.chromium.launch(headless=True, args=[
            "--no-sandbox", 
            "--disable-dev-shm-usage", 
            "--single-process",
            "--disable-renderer-backgrounding"
        ])
        context = browser.new_context(user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36")
        
        for _, row in chunk_df.iterrows():
            page = context.new_page()
            data = extract_full_features(page, row['URL'])
            data['URL_KEY'] = str(row['URL'])
            results.append(data)
            page.close()
            
        browser.close()
    return results

# =================================================================
# IV. QU·∫¢N L√ù TI·∫æN TR√åNH
# =================================================================
def main():
    start_time = time.time()
    df_raw = pd.read_csv(RAW_CSV_FILE, usecols=OLD_KEEP_COLS)
    
    processed_urls = set()
    if os.path.exists(TEMP_LOG_FILE):
        try:
            check_df = pd.read_csv(TEMP_LOG_FILE, usecols=['URL_KEY'])
            processed_urls = set(check_df['URL_KEY'].astype(str))
        except: pass

    df_todo = df_raw[~df_raw['URL'].astype(str).isin(processed_urls)]
    total_todo = len(df_todo)
    
    if total_todo == 0:
        print("‚úÖ ƒê√£ ho√†n th√†nh to√†n b·ªô d·ªØ li·ªáu!"); return

    print(f"üöÄ KH·ªûI CH·∫†Y {MAX_WORKERS} LU·ªíNG | C·∫ßn x·ª≠ l√Ω: {total_todo} URL")
    chunks = [df_todo[i:i + CHUNK_SIZE] for i in range(0, total_todo, CHUNK_SIZE)]
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(thread_worker, c): c for c in chunks}
        for i, future in enumerate(as_completed(futures), 1):
            try:
                batch = future.result()
                if batch:
                    with csv_lock:
                        pd.DataFrame(batch).to_csv(TEMP_LOG_FILE, mode='a', header=not os.path.exists(TEMP_LOG_FILE), index=False)
                
                # TƒÉng t·∫ßn su·∫•t d·ªçn d·∫πp h·ªá th·ªëng
                if i % 5 == 0: 
                    os.system('pkill -f chromium')

                done = min(i * CHUNK_SIZE, total_todo)
                elapsed = time.time() - start_time
                speed = done / elapsed if elapsed > 0 else 0
                rem_sec = (total_todo - done) / speed if speed > 0 else 0
                
                print(f"‚ûú [{datetime.now().strftime('%H:%M:%S')}] {len(processed_urls)+done}/{len(df_raw)} "
                      f"| {speed:.2f} URL/s | C√≤n: {str(timedelta(seconds=int(rem_sec)))}")
            except Exception as e:
                print(f"‚ùå L·ªói Batch: {e}")

    # G·ªôp d·ªØ li·ªáu cu·ªëi c√πng
    print("\nüîÑ ƒêang t·∫°o file CSV k·∫øt qu·∫£...")
    df_new = pd.read_csv(TEMP_LOG_FILE).drop_duplicates('URL_KEY')
    df_final = pd.merge(df_raw, df_new, left_on='URL', right_on='URL_KEY', how='inner')
    df_final.drop(columns=['URL_KEY']).to_csv(FINAL_OUTPUT, index=False)
    print(f"‚úÖ XONG! File l∆∞u t·∫°i: {FINAL_OUTPUT}")

if __name__ == "__main__":
    main()
