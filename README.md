import pandas as pd
import numpy as np
import os
import time
import math
import io
import socket
import re
import tldextract
import ssl
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright
import imagehash
from PIL import Image
from bs4 import BeautifulSoup
from collections import Counter

# =================================================================
# I. C·∫§U H√åNH T·ªêI ∆ØU (12 NH√ÇN CPU & TI·∫æT KI·ªÜM DATA 5G)
# =================================================================
RAW_CSV_FILE = 'PhiUSIIL_Phishing_URL_Dataset.csv'
TEMP_LOG_FILE = 'extraction_checkpoint.csv'
FINAL_OUTPUT = 'PhiUSIIL_Extracted_Full.csv'

# TH√îNG S·ªê V·∫¨N H√ÄNH
MAX_WORKERS = 25  # T·ªëi ∆∞u cho CPU 12 nh√¢n
CHUNK_SIZE = 40   # Reset tr√¨nh duy·ªát m·ªói 40 URL ƒë·ªÉ gi·∫£i ph√≥ng RAM v√† s·∫°ch session
TARGET_PHASH = imagehash.hex_to_hash('9880e61f1c7e0c4f')

# C√°c c·ªôt c·∫ßn gi·ªØ t·ª´ file g·ªëc
OLD_KEEP_COLS = [
    'URL', 'NoOfDegitsInURL', 'HasDescription', 'HasSocialNet', 'HasPasswordField', 
    'HasSubmitButton', 'HasExternalFormSubmit', 'DomainTitleMatchScore', 
    'IsHTTPS', 'HasCopyrightInfo', 'label'
]

# =================================================================
# II. H√ÄM H·ªñ TR·ª¢ T√çNH TO√ÅN (Tƒ®NH)
# =================================================================
def get_entropy(text):
    if not text or len(text) == 0: return 0.0
    probs = [count/len(text) for count in Counter(text).values()]
    return -sum(p * math.log2(p) for p in probs) / 8.0

def get_tls_issuer(hostname):
    try:
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        with socket.create_connection((hostname, 443), timeout=3) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert = ssock.getpeercert()
                issuer = dict(x[0] for x in cert['issuer'])
                return issuer.get('organizationName', 'Unknown')
    except: return 'None'

# =================================================================
# III. LOGIC TR√çCH XU·∫§T (T·ªêI ∆ØU CH·∫∂N DATA R√ÅC)
# =================================================================
def extract_full_features(page, url):
    res = {k: 0.0 for k in [
        'V10_HTTP_Extraction_Success', 'V11_WHOIS_Extraction_Success', 'V1_PHash_Distance', 
        'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score', 
        'V8_Total_IFrames', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 
        'V3_Domain_Age_Days', 'V4_DNS_Volatility_Count', 'Is_Top_1M_Domain', 
        'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain'
    ]}
    
    try:
        # CH·∫∂N T·∫¢I T√ÄI NGUY√äN N·∫∂NG (Ti·∫øt ki·ªám ~90% Data 5G)
        page.route("**/*", lambda route: route.abort() 
                   if route.request.resource_type in ["image", "media", "font", "other"] 
                   else route.continue_())

        ext = tldextract.extract(url)
        domain = f"{ext.domain}.{ext.suffix}"
        
        # 1. TR√çCH XU·∫§T Tƒ®NH
        res['V22_IP_Subdomain_Pattern'] = 1 if re.search(r'\d+\.\d+', ext.subdomain) else 0
        res['V23_Entropy_Subdomain'] = get_entropy(ext.subdomain)
        res['Is_Top_1M_Domain'] = 1 if ext.domain in ['google', 'facebook', 'microsoft', 'apple', 'amazon'] else 0
        try:
            res['V4_DNS_Volatility_Count'] = len(socket.gethostbyname_ex(domain)[2])
        except: res['V4_DNS_Volatility_Count'] = 0
        
        res['V5_TLS_Issuer_Reputation'] = 1.0 if get_tls_issuer(domain) != 'None' else 0.0

        # 2. TR√çCH XU·∫§T ƒê·ªòNG (PLAYWRIGHT)
        # wait_until="domcontentloaded" gi√∫p ch·∫°y nhanh v√† √≠t t·ªën data h∆°n "networkidle"
        page.goto(url, timeout=20000, wait_until="domcontentloaded") 
        res['V10_HTTP_Extraction_Success'] = 1
        
        # V1: PHash Distance (V·∫´n screenshot ƒë∆∞·ª£c layout khung x∆∞∆°ng)
        img_bytes = page.screenshot()
        img = Image.open(io.BytesIO(img_bytes)).convert('L')
        res['V1_PHash_Distance'] = (imagehash.phash(img) - TARGET_PHASH) / 64.0
        
        # Parse HTML b·∫±ng BeautifulSoup
        content = page.content()
        soup = BeautifulSoup(content, 'html.parser')
        
        # V2: Layout Similarity (ƒê·ªô s√¢u DOM)
        depths = [len(list(t.parents)) for t in soup.find_all(True)]
        res['V2_Layout_Similarity'] = np.clip(1.0 - (max(depths or [0])/40.0), 0, 1)
        
        # V6: JS Entropy (Kh√¥ng ch·∫∑n script n√™n v·∫´n l·∫•y ƒë∆∞·ª£c code)
        js_code = "".join([s.text for s in soup.find_all('script')])
        res['V6_JS_Entropy'] = get_entropy(js_code)
        
        # V7: Text Readability
        full_text = soup.get_text()
        words = full_text.split()
        sentences = re.split(r'[.!?]+', full_text)
        res['V7_Text_Readability_Score'] = np.clip(len(words)/(len(sentences) or 1) / 20.0, 0, 1)
        
        # V8, V9: IFrames
        iframes = soup.find_all('iframe')
        res['V8_Total_IFrames'] = len(iframes)
        res['V9_Has_Hidden_IFrame'] = 1 if any('none' in str(f.get('style','')).lower() for f in iframes) else 0
        
        res['V11_WHOIS_Extraction_Success'] = 1
        
    except Exception:
        res['V10_HTTP_Extraction_Success'] = 0
        res['V1_PHash_Distance'] = 0.5 
        
    return res

# =================================================================
# IV. QU·∫¢N L√ù ƒêA LU·ªíNG
# =================================================================
def thread_worker(chunk_df):
    results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=[
            "--no-sandbox", 
            "--disable-setuid-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu",
            "--blink-settings=imagesEnabled=false", # Ch·∫∑n ·∫£nh t·ª´ l√µi tr√¨nh duy·ªát
        ])
        context = browser.new_context(
            viewport={'width': 1280, 'height': 720},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        for _, row in chunk_df.iterrows():
            data = extract_full_features(page, row['URL'])
            data['URL_KEY'] = row['URL']
            results.append(data)
            
        browser.close()
    return results

def main():
    start_session_time = time.time()
    
    if not os.path.exists(RAW_CSV_FILE):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y {RAW_CSV_FILE}"); return
        
    df_raw = pd.read_csv(RAW_CSV_FILE, usecols=OLD_KEEP_COLS)
    total_all = len(df_raw)

    # ƒê·ªçc Checkpoint ƒë·ªÉ ch·∫°y ti·∫øp
    if os.path.exists(TEMP_LOG_FILE):
        df_checkpoint = pd.read_csv(TEMP_LOG_FILE, usecols=['URL_KEY'])
        processed_urls = set(df_checkpoint['URL_KEY'].astype(str))
    else:
        processed_urls = set()
    
    df_todo = df_raw[~df_raw['URL'].astype(str).isin(processed_urls)]
    num_already_done = len(processed_urls)
    total_todo = len(df_todo)
    
    if total_todo == 0:
        print("‚úÖ ƒê√£ ho√†n th√†nh x·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu."); return

    print(f"üöÄ T·ªïng c·ªông: {total_all} | ƒê√£ xong: {num_already_done} | C√≤n l·∫°i: {total_todo}")
    print(f"üî• ƒêang ch·∫°y {MAX_WORKERS} lu·ªìng tr√™n CPU 12 nh√¢n...")

    chunks = [df_todo[i:i + CHUNK_SIZE] for i in range(0, total_todo, CHUNK_SIZE)]
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(thread_worker, c): c for c in chunks}
        
        for i, future in enumerate(as_completed(futures), 1):
            batch_data = future.result()
            pd.DataFrame(batch_data).to_csv(TEMP_LOG_FILE, mode='a', index=False, header=not os.path.exists(TEMP_LOG_FILE))
            
            # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
            current_batch_done = i * CHUNK_SIZE
            total_current_done = min(num_already_done + current_batch_done, total_all)
            elapsed = time.time() - start_session_time
            speed = current_batch_done / elapsed
            
            print(f"‚ûú [{datetime.now().strftime('%H:%M:%S')}] {total_current_done}/{total_all} ({total_current_done/total_all*100:.2f}%) "
                  f"| T·ªëc ƒë·ªô: {speed:.2f} URL/s | ETA: {(total_all - total_current_done)/speed/60:.1f} ph√∫t")

    # H·ª£p nh·∫•t cu·ªëi c√πng
    print("\nüîÑ ƒêang t·∫°o file k·∫øt qu·∫£ cu·ªëi c√πng...")
    df_new = pd.read_csv(TEMP_LOG_FILE).drop_duplicates('URL_KEY')
    df_final = pd.merge(df_raw, df_new, left_on='URL', right_on='URL_KEY', how='inner')
    
    for col in ['V1_PHash_Distance', 'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score']:
        df_final[f'Alarm_{col}_Missing'] = np.where(df_final['V10_HTTP_Extraction_Success'] == 0, 1, 0)

    df_final.drop(columns=['URL_KEY'], inplace=True)
    df_final.to_csv(FINAL_OUTPUT, index=False)
    print(f"‚úÖ HO√ÄN T·∫§T! File: {FINAL_OUTPUT}")

if __name__ == "__main__":
    main()
