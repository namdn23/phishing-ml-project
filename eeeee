import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. Cáº¤U HÃŒNH (Tá»I Æ¯U CHO 27GB RAM)
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"

NUM_PROCESSES = 8          # 8 Tiáº¿n trÃ¬nh song song
PAGES_PER_PROCESS = 5      # 5 Tab má»—i tiáº¿n trÃ¬nh (Tá»•ng 40 tabs)
TIMEOUT = 60000           

# Äáº£m báº£o file log tá»“n táº¡i ngay tá»« Ä‘áº§u
if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        f.write(f"--- START LOG: {datetime.now()} ---\n")

# --- HÃ m tÃ ng hÃ¬nh & trÃ­ch xuáº¥t tÄ©nh ---
async def apply_stealth(page):
    await page.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', { get: () => false });
        window.chrome = { runtime: {} };
        Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
    """)

def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        return {
            'domainEntropy': calculate_entropy(domain),
            'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
            'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
            'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
            'domainLength': len(domain),
            'Subdomain_Level': domain.count('.'),
            'IsHTTPS': 1 if url.startswith('https') else 0,
            'URL_Depth': url.count('/') - 2
        }
    except: return {f: 0 for f in range(8)}

# =========================================================
# 2. HÃ€M Xá»¬ LÃ URL (Ã‰P GHI LOG NGAY Láº¬P Tá»¨C)
# =========================================================
async def process_url(url, label, context, semaphore, log_file):
    static_data = extract_static(url)
    dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                    'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 
                                    'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}
    status, reason = "FAILED", "CRASH"

    async with semaphore:
        page = await context.new_page()
        await apply_stealth(page)
        try:
            await page.set_extra_http_headers({"Referer": "https://www.google.com/"})
            await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
            await asyncio.sleep(random.uniform(3, 6))
            
            content = (await page.content()).lower()
            title = (await page.title()) or "NoTitle"

            if "just a moment" in title.lower() or "cloudflare" in content:
                reason = "Cloudflare_Blocked"
            else:
                domain = urlparse(url).netloc.lower()
                v9_val = 0
                for f in page.frames[1:]:
                    try:
                        el = await f.frame_element()
                        if el and not await el.is_visible(): v9_val = 1; break
                    except: continue

                dynamic_data.update({
                    'V9_Has_Hidden_IFrame': v9_val,
                    'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                    'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
                    'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0
                })
                status, reason = "SUCCESS", f"OK_{title[:15]}"
        except Exception as e:
            reason = f"ERR_{str(e).splitlines()[0][:30]}"
        finally:
            await page.close()
            
    # --- LOGGING CÆ¯á» NG Bá»¨C (CÆ¡ cháº¿ Flush) ---
    log_line = f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {reason}\n"
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(log_line)
        f.flush() # Äáº©y dá»¯ liá»‡u tá»« RAM xuá»‘ng á»• cá»©ng ngay
        os.fsync(f.fileno()) # Ã‰p há»‡ Ä‘iá»u hÃ nh ghi file
    
    return {**static_data, **dynamic_data, 'URL': url, 'label': label}

# =========================================================
# 3. ÄA LUá»’NG & LÆ¯U CHECKPOINT CSV
# =========================================================
async def run_batch(df_batch):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False, args=['--no-sandbox'])
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0")
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        tasks = [process_url(row['URL'], row['label'], context, semaphore, LOG_FILE) for _, row in df_batch.iterrows()]
        results = await asyncio.gather(*tasks)
        await browser.close()
        return results

def process_entry(df_batch):
    try: return asyncio.run(run_batch(df_batch))
    except: return []

if __name__ == "__main__":
    start_time = time.time()
    df_all = pd.read_csv(INPUT_FILE)
    
    # Äá»c Checkpoint tá»« file Log Ä‘á»ƒ bá» qua URL cÅ©
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total = len(df_todo)
    print(f"ğŸš€ Tá»•ng URL cáº§n cÃ o: {total} | Äang dÃ¹ng 27GB RAM cho {NUM_PROCESSES} luá»“ng.")

    # Chia batch nhá» (20 URL má»—i Ä‘á»£t) Ä‘á»ƒ lÆ°u file liÃªn tá»¥c
    batch_size = NUM_PROCESSES * 5 
    for i in range(0, total, batch_size):
        current_batch = df_todo.iloc[i : i + batch_size]
        sub_batches = [b for b in np.array_split(current_batch, NUM_PROCESSES) if not b.empty]
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            futures = [executor.submit(process_entry, sub_batches[p]) for p in range(len(sub_batches))]
            batch_results = []
            for fut in futures: batch_results.extend(fut.result())
        
        # --- LÆ¯U CSV NGAY SAU Má»–I BATCH (CHECKPOINT) ---
        if batch_results:
            df_res = pd.DataFrame(batch_results)
            # Náº¿u file chÆ°a cÃ³ thÃ¬ ghi header, cÃ³ rá»“i thÃ¬ append khÃ´ng header
            file_exists = os.path.isfile(OUTPUT_FILE)
            df_res.to_csv(OUTPUT_FILE, mode='a', index=False, header=not file_exists, encoding="utf-8")
        
        # Tiáº¿n Ä‘á»™ vÃ  ETA
        done = i + len(current_batch)
        speed = done / (time.time() - start_time)
        eta = str(timedelta(seconds=int((total - done) / speed))) if speed > 0 else "N/A"
        print(f"âœ… ÄÃ£ lÆ°u checkpoint {done}/{total} | Speed: {speed:.2f} URL/s | ETA: {eta}")
