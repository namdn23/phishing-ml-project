#!/usr/bin/env python3
"""
PRODUCTION-READY Feature Extractor - 27 Features
✅ Checkpoint & Resume
✅ Time Estimation & ETA
✅ Error Handling với Fallback
✅ Multi-threading (56 workers)
✅ RAM Management (27GB Intel Ultra 5H)
✅ Anti-blocking strategies

Author: [Your Name]
Date: 2024
"""

import pandas as pd
import numpy as np
import requests
import whois
import ssl
import socket
import time
import json
import hashlib
import re
import gc
import psutil
from datetime import datetime
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path
from collections import defaultdict, Counter
from threading import Lock
import warnings
warnings.filterwarnings('ignore')

# Disable SSL warnings
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ===================== CONFIG =====================
CONFIG = {
    'max_workers': 56,
    'timeout': 5,
    'max_retries': 3,
    'checkpoint_interval': 500,  # Save every 500 URLs
    'batch_size': 2000,  # Process in batches to avoid RAM overflow
    'output_dir': 'checkpoints',
    'rate_limit': {
        'whois': 3,      # 3 req/sec (conservative)
        'http': 15,      # 15 req/sec
        'ssl': 30        # 30 req/sec
    },
    'memory_threshold': 0.85,  # Trigger GC at 85% RAM usage
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0'
    }
}

Path(CONFIG['output_dir']).mkdir(exist_ok=True)

# ===================== CONSTANTS =====================
BRANDS = [
    'google', 'facebook', 'paypal', 'amazon', 'microsoft',
    'apple', 'netflix', 'instagram', 'twitter', 'linkedin',
    'github', 'dropbox', 'yahoo', 'ebay', 'adobe', 'chase',
    'wellsfargo', 'bankofamerica', 'citibank', 'americanexpress',
    'visa', 'mastercard', 'discover', 'fedex', 'ups', 'usps'
]

SUSPICIOUS_TLDS = [
    '.tk', '.ml', '.ga', '.cf', '.gq', '.xyz', '.top', 
    '.work', '.date', '.racing', '.loan', '.click', 
    '.download', '.stream', '.science', '.club', '.win',
    '.bid', '.faith', '.cricket', '.accountant', '.review'
]

PHISHING_KEYWORDS = [
    'login', 'signin', 'account', 'verify', 'secure', 'update',
    'bank', 'paypal', 'ebay', 'amazon', 'apple', 'microsoft',
    'password', 'confirm', 'suspended', 'locked', 'alert',
    'security', 'credential', 'validate', 'auth', 'urgent',
    'important', 'action', 'required', 'expire', 'notification'
]

# ===================== MEMORY MONITOR =====================
class MemoryMonitor:
    """Monitor RAM usage and trigger GC if needed"""
    
    @staticmethod
    def get_usage():
        return psutil.virtual_memory().percent / 100
    
    @staticmethod
    def check_and_cleanup():
        usage = MemoryMonitor.get_usage()
        if usage > CONFIG['memory_threshold']:
            print(f"\n⚠️  RAM usage: {usage*100:.1f}% - Running garbage collection...")
            gc.collect()
            time.sleep(1)
            new_usage = MemoryMonitor.get_usage()
            print(f"✅ RAM after cleanup: {new_usage*100:.1f}%")
            return True
        return False

# ===================== RATE LIMITER =====================
class RateLimiter:
    """Thread-safe rate limiter"""
    
    def __init__(self):
        self.locks = defaultdict(Lock)
        self.last_call = defaultdict(float)
    
    def wait(self, key, min_interval):
        with self.locks[key]:
            elapsed = time.time() - self.last_call[key]
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)
            self.last_call[key] = time.time()

rate_limiter = RateLimiter()

# ===================== UTILITIES =====================
def safe_request(url, timeout=5, follow_redirects=True):
    """HTTP request with retry and anti-blocking"""
    session = requests.Session()
    session.headers.update(CONFIG['headers'])
    
    for attempt in range(CONFIG['max_retries']):
        try:
            rate_limiter.wait('http', 1.0 / CONFIG['rate_limit']['http'])
            
            response = session.get(
                url,
                timeout=timeout,
                allow_redirects=follow_redirects,
                verify=False
            )
            return response
            
        except requests.exceptions.Timeout:
            if attempt == CONFIG['max_retries'] - 1:
                return None
            time.sleep(1 * (attempt + 1))
            
        except requests.exceptions.ConnectionError:
            if attempt == CONFIG['max_retries'] - 1:
                return None
            time.sleep(2 * (attempt + 1))
            
        except requests.exceptions.TooManyRedirects:
            return None
            
        except Exception as e:
            if attempt == CONFIG['max_retries'] - 1:
                return None
            time.sleep(0.5 * (attempt + 1))
    
    return None

def extract_domain_parts(url):
    """Parse URL into components with error handling"""
    try:
        if not url or not isinstance(url, str):
            return None
            
        # Add scheme if missing
        if not url.startswith(('http://', 'https://')):
            url = 'http://' + url
            
        parsed = urlparse(url)
        domain = parsed.netloc.split(':')[0].lower()  # Remove port
        
        if not domain:
            return None
            
        parts = domain.split('.')
        
        return {
            'full_domain': domain,
            'subdomain': '.'.join(parts[:-2]) if len(parts) > 2 else '',
            'main_domain': parts[-2] if len(parts) >= 2 else domain,
            'tld': parts[-1] if parts else '',
            'path': parsed.path,
            'scheme': parsed.scheme or 'http',
            'netloc': parsed.netloc
        }
    except Exception as e:
        return None

def calculate_entropy(text):
    """Calculate Shannon entropy"""
    if not text or len(text) == 0:
        return 0.0
    counter = Counter(text)
    length = len(text)
    entropy = -sum((count/length) * np.log2(count/length) 
                   for count in counter.values())
    return round(entropy, 4)

# ===================== STATIC FEATURES (15) =====================

def extract_static_features(url):
    """Extract ALL 15 static features (< 10ms)"""
    
    # Default values for error cases
    default_features = {
        'Is_Top_1M_Domain': 0,
        'Entropy_Subdomain': 0.0,
        'Is_HTTPS': 0,
        'Subdomain_Count': 0,
        'Has_Phishing_Keyword': 0,
        'Path_Depth': 0,
        'URL_Length': 0,
        'Levenshtein_Brand': 10,
        'Digit_Ratio': 0.0,
        'Has_IP_Address': 0,
        'Suspicious_TLD': 0,
        'Brand_In_Subdomain': 0,
        'Has_At_Symbol': 0,
        'Special_Char_Ratio': 0.0,
        'Prefix_Suffix_Domain': 0
    }
    
    try:
        parts = extract_domain_parts(url)
        if not parts:
            return default_features
        
        features = {}
        
        # 1. Is_Top_1M_Domain (placeholder - need to load list)
        features['Is_Top_1M_Domain'] = 0
        
        # 2. Entropy_Subdomain
        subdomain = parts['subdomain']
        features['Entropy_Subdomain'] = calculate_entropy(subdomain) if subdomain else 0.0
        
        # 3. Is_HTTPS
        features['Is_HTTPS'] = 1 if parts['scheme'] == 'https' else 0
        
        # 4. Subdomain_Count
        features['Subdomain_Count'] = subdomain.count('.') + 1 if subdomain else 0
        
        # 5. Has_Phishing_Keyword
        url_lower = url.lower()
        features['Has_Phishing_Keyword'] = 1 if any(kw in url_lower for kw in PHISHING_KEYWORDS) else 0
        
        # 6. Path_Depth
        path = parts['path'].strip('/')
        features['Path_Depth'] = path.count('/') if path else 0
        
        # 7. URL_Length
        features['URL_Length'] = len(url)
        
        # 8. Levenshtein_Brand
        try:
            from Levenshtein import distance
            main_domain = parts['main_domain'].lower()
            distances = [distance(main_domain, brand) for brand in BRANDS]
            features['Levenshtein_Brand'] = min(distances) if distances else 10
        except ImportError:
            # Fallback: simple similarity check
            main_domain = parts['main_domain'].lower()
            min_dist = 10
            for brand in BRANDS:
                if brand in main_domain or main_domain in brand:
                    min_dist = min(min_dist, abs(len(brand) - len(main_domain)))
            features['Levenshtein_Brand'] = min_dist
        except:
            features['Levenshtein_Brand'] = 10
        
        # 9. Digit_Ratio
        digit_count = sum(c.isdigit() for c in url)
        features['Digit_Ratio'] = round(digit_count / len(url), 4) if len(url) > 0 else 0.0
        
        # 10. Has_IP_Address
        netloc = parts['netloc']
        ipv4_pattern = r'^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(:[0-9]+)?$'
        features['Has_IP_Address'] = 1 if re.match(ipv4_pattern, netloc) else 0
        
        # 11. Suspicious_TLD
        full_domain = parts['full_domain'].lower()
        features['Suspicious_TLD'] = 1 if any(full_domain.endswith(tld) for tld in SUSPICIOUS_TLDS) else 0
        
        # 12. Brand_In_Subdomain
        if subdomain:
            subdomain_lower = subdomain.lower()
            main_domain_lower = parts['main_domain'].lower()
            brand_found = any(brand in subdomain_lower and brand != main_domain_lower 
                            for brand in BRANDS)
            features['Brand_In_Subdomain'] = 1 if brand_found else 0
        else:
            features['Brand_In_Subdomain'] = 0
        
        # 13. Has_At_Symbol
        features['Has_At_Symbol'] = 1 if '@' in parts['netloc'] else 0
        
        # 14. Special_Char_Ratio
        special_chars = '@-_=?&/#%'
        special_count = sum(url.count(c) for c in special_chars)
        features['Special_Char_Ratio'] = round(special_count / len(url), 4) if len(url) > 0 else 0.0
        
        # 15. Prefix_Suffix_Domain
        main_domain = parts['main_domain']
        features['Prefix_Suffix_Domain'] = 1 if '-' in main_domain else 0
        
        return features
        
    except Exception as e:
        return default_features

# ===================== DYNAMIC FEATURES (12) =====================

def extract_form_features(soup, original_domain):
    """Extract form-related features with fallback"""
    features = {
        'Has_External_Form_Submit': 0,
        'Has_Submit_Button': 0,
        'Has_Password_Field': 0
    }
    
    try:
        forms = soup.find_all('form')
        
        if forms:
            features['Has_Submit_Button'] = 1
            
            for form in forms:
                # Check password fields
                password_inputs = form.find_all('input', {'type': 'password'})
                if password_inputs:
                    features['Has_Password_Field'] = 1
                
                # Check external form submission
                action = form.get('action', '')
                if action and action.startswith('http'):
                    form_domain = extract_domain_parts(action)
                    if form_domain and form_domain['full_domain'] != original_domain:
                        features['Has_External_Form_Submit'] = 1
                        
        # Also check for submit buttons outside forms
        submit_buttons = soup.find_all(['button', 'input'], {'type': ['submit', 'button']})
        if submit_buttons and features['Has_Submit_Button'] == 0:
            features['Has_Submit_Button'] = 1
            
    except:
        pass
    
    return features

def extract_iframe_features(soup):
    """Extract iframe features with detailed checks"""
    features = {
        'Total_IFrames': 0,
        'Has_Hidden_IFrame': 0
    }
    
    try:
        iframes = soup.find_all('iframe')
        features['Total_IFrames'] = len(iframes)
        
        for iframe in iframes:
            # Multiple ways to hide iframe
            style = iframe.get('style', '').lower()
            width = str(iframe.get('width', '')).lower()
            height = str(iframe.get('height', '')).lower()
            hidden_attr = iframe.get('hidden')
            
            is_hidden = (
                'display:none' in style or
                'display: none' in style or
                'visibility:hidden' in style or
                'visibility: hidden' in style or
                width in ['0', '0px', '1', '1px'] or
                height in ['0', '0px', '1', '1px'] or
                hidden_attr is not None
            )
            
            if is_hidden:
                features['Has_Hidden_IFrame'] = 1
                break
                
    except:
        pass
    
    return features

def extract_html_features(url):
    """Extract HTML-based features (8 features from 1 request)"""
    
    default_features = {
        'Has_External_Form_Submit': -1,
        'Has_Submit_Button': -1,
        'Has_Password_Field': -1,
        'Total_IFrames': -1,
        'Has_Hidden_IFrame': -1,
        'Favicon_Match': -1,
        'External_Links_Ratio': -1,
        'Has_Popup': -1,
    }
    
    try:
        response = safe_request(url, timeout=CONFIG['timeout'], follow_redirects=False)
        
        if not response or response.status_code not in [200, 201]:
            return default_features
        
        # Check content type
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' not in content_type:
            return default_features
        
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')
        parts = extract_domain_parts(url)
        
        if not parts:
            return default_features
        
        original_domain = parts['full_domain']
        features = {}
        
        # Form features (3 features)
        form_features = extract_form_features(soup, original_domain)
        features.update(form_features)
        
        # IFrame features (2 features)
        iframe_features = extract_iframe_features(soup)
        features.update(iframe_features)
        
        # Favicon Match
        try:
            favicon = soup.find('link', rel=lambda x: x and 'icon' in x.lower()) if soup else None
            if favicon and favicon.get('href'):
                favicon_url = favicon['href']
                if not favicon_url.startswith('http'):
                    favicon_url = urljoin(url, favicon_url)
                
                favicon_parts = extract_domain_parts(favicon_url)
                if favicon_parts:
                    favicon_domain = favicon_parts['full_domain']
                    features['Favicon_Match'] = 1 if (favicon_domain == original_domain or not favicon_domain) else 0
                else:
                    features['Favicon_Match'] = 0
            else:
                features['Favicon_Match'] = 0
        except:
            features['Favicon_Match'] = 0
        
        # External Links Ratio
        try:
            all_links = soup.find_all('a', href=True)
            if all_links:
                external_count = 0
                for link in all_links:
                    href = link.get('href', '')
                    if href.startswith('http') and original_domain not in href:
                        external_count += 1
                features['External_Links_Ratio'] = round(external_count / len(all_links), 3)
            else:
                features['External_Links_Ratio'] = 0
        except:
            features['External_Links_Ratio'] = 0
        
        # Has Popup
        try:
            html_lower = html.lower()
            popup_keywords = [
                'window.open(', 'popup', 'modal', 'overlay',
                'alert(', 'confirm(', 'prompt(', 'showmodal',
                'window.showmodaldialog', 'window.showmodelessdialog'
            ]
            features['Has_Popup'] = 1 if any(kw in html_lower for kw in popup_keywords) else 0
        except:
            features['Has_Popup'] = 0
        
        return features
        
    except Exception as e:
        return default_features

def extract_tls_issuer(url):
    """Extract TLS issuer reputation with timeout"""
    try:
        rate_limiter.wait('ssl', 1.0 / CONFIG['rate_limit']['ssl'])
        
        parts = extract_domain_parts(url)
        if not parts or parts['scheme'] != 'https':
            return -1
        
        domain = parts['full_domain']
        
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        
        with socket.create_connection((domain, 443), timeout=CONFIG['timeout']) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                
                if not cert:
                    return -1
                    
                issuer = dict(x[0] for x in cert.get('issuer', []))
                issuer_name = issuer.get('organizationName', '').lower()
                
                # Reputation scoring
                trusted_cas = ['digicert', 'sectigo', 'godaddy', 'globalsign', 'entrust', 'comodo']
                if any(ca in issuer_name for ca in trusted_cas):
                    return 2  # High reputation
                elif "let's encrypt" in issuer_name or 'letsencrypt' in issuer_name:
                    return 1  # Medium (free but valid)
                else:
                    return 0  # Unknown/Low
                    
    except socket.timeout:
        return -1
    except:
        return -1

def extract_domain_age(url):
    """Domain age via WHOIS with conservative rate limiting"""
    try:
        rate_limiter.wait('whois', 1.0 / CONFIG['rate_limit']['whois'])
        
        parts = extract_domain_parts(url)
        if not parts:
            return -1
        
        domain = parts['full_domain']
        
        # Remove 'www.' prefix for WHOIS
        if domain.startswith('www.'):
            domain = domain[4:]
        
        w = whois.whois(domain)
        
        created = w.creation_date
        if isinstance(created, list):
            created = created[0]
        
        if created and isinstance(created, datetime):
            age_days = (datetime.now() - created).days
            return max(0, age_days)
            
        return -1
        
    except Exception as e:
        # Common WHOIS errors - don't spam logs
        return -1

def extract_certificate_age(url):
    """SSL certificate age with error handling"""
    try:
        rate_limiter.wait('ssl', 1.0 / CONFIG['rate_limit']['ssl'])
        
        parts = extract_domain_parts(url)
        if not parts or parts['scheme'] != 'https':
            return -1
        
        domain = parts['full_domain']
        
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        
        with socket.create_connection((domain, 443), timeout=CONFIG['timeout']) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                
                if not cert:
                    return -1
                
                not_before_str = cert.get('notBefore')
                if not not_before_str:
                    return -1
                    
                not_before = datetime.strptime(not_before_str, '%b %d %H:%M:%S %Y %Z')
                age_days = (datetime.now() - not_before).days
                return max(0, age_days)
                
    except socket.timeout:
        return -1
    except:
        return -1

def extract_redirect_count(url):
    """Count HTTP redirects"""
    try:
        response = safe_request(url, timeout=CONFIG['timeout'], follow_redirects=True)
        if response:
            return len(response.history)
        return -1
    except:
        return -1

def extract_dynamic_features(url):
    """Extract ALL 12 dynamic features with individual fallbacks"""
    
    features = {}
    
    try:
        # HTML features (8 features from 1 request)
        html_features = extract_html_features(url)
        features.update(html_features)
        
        # TLS Issuer (separate - can fail independently)
        try:
            features['TLS_Issuer_Reputation'] = extract_tls_issuer(url)
        except:
            features['TLS_Issuer_Reputation'] = -1
        
        # Domain Age (slowest - WHOIS)
        try:
            features['Domain_Age'] = extract_domain_age(url)
        except:
            features['Domain_Age'] = -1
        
        # Certificate Age
        try:
            features['Certificate_Age'] = extract_certificate_age(url)
        except:
            features['Certificate_Age'] = -1
        
        # Redirect Count
        try:
            features['Redirect_Count'] = extract_redirect_count(url)
        except:
            features['Redirect_Count'] = -1
        
        return features
        
    except Exception as e:
        # Return all -1 if complete failure
        return {
            'Has_External_Form_Submit': -1,
            'Has_Submit_Button': -1,
            'Has_Password_Field': -1,
            'Total_IFrames': -1,
            'Has_Hidden_IFrame': -1,
            'TLS_Issuer_Reputation': -1,
            'Domain_Age': -1,
            'Certificate_Age': -1,
            'Redirect_Count': -1,
            'Favicon_Match': -1,
            'External_Links_Ratio': -1,
            'Has_Popup': -1,
        }

# ===================== MAIN EXTRACTOR =====================

def extract_all_features(row, include_dynamic=True):
    """Extract ALL 27 features for one URL"""
    
    url = row.get('url', '')
    
    result = {
        'url': url,
        'timestamp': datetime.now().isoformat(),
        'status': 'pending'
    }
    
    try:
        # Always extract static features (fast)
        static = extract_static_features(url)
        result.update(static)
        
        # Conditionally extract dynamic features (slow)
        if include_dynamic:
            dynamic = extract_dynamic_features(url)
            result.update(dynamic)
        
        result['status'] = 'success'
        
    except Exception as e:
        result['status'] = 'error'
        result['error_message'] = str(e)
        
        # Fill all missing features with -1
        all_feature_keys = [
            'Is_Top_1M_Domain', 'Entropy_Subdomain', 'Is_HTTPS',
            'Subdomain_Count', 'Has_Phishing_Keyword', 'Path_Depth',
            'URL_Length', 'Levenshtein_Brand', 'Digit_Ratio',
            'Has_IP_Address', 'Suspicious_TLD', 'Brand_In_Subdomain',
            'Has_At_Symbol', 'Special_Char_Ratio', 'Prefix_Suffix_Domain'
        ]
        
        if include_dynamic:
            all_feature_keys.extend([
                'Has_External_Form_Submit', 'Has_Submit_Button',
                'Has_Password_Field', 'Total_IFrames', 'Has_Hidden_IFrame',
                'TLS_Issuer_Reputation', 'Domain_Age', 'Certificate_Age',
                'Redirect_Count', 'Favicon_Match', 'External_Links_Ratio',
                'Has_Popup'
            ])
        
        for key in all_feature_keys:
            if key not in result:
                result[key] = -1
    
    return result

# ===================== CHECKPOINT MANAGER =====================

class CheckpointManager:
    """Manage checkpoints for resume capability"""
    
    def __init__(self, output_dir, input_file):
        self.output_dir = Path(output_dir)
        self.input_file = input_file
        self.file_hash = self._get_file_hash()
        self.checkpoint_file = self.output_dir / f"checkpoint_{self.file_hash}.json"
        self.results_file = self.output_dir / f"results_{self.file_hash}.csv"
    
    def _get_file_hash(self):
        """Generate hash from filename"""
        return hashlib.md5(self.input_file.encode()).hexdigest()[:8]
    
    def save_checkpoint(self, processed_indices, results):
        """Save checkpoint to disk"""
        checkpoint_data = {
            'timestamp': datetime.now().isoformat(),
            'processed_indices': list(processed_indices),
            'total_processed': len(processed_indices),
            'input_file': self.input_file
        }
        
        with open(self.checkpoint_file, 'w') as f:
            json.dump(checkpoint_data, f, indent=2)
        
        # Save results incrementally
        if results:
            df_results = pd.DataFrame(results)
            df_results.to_csv(self.results_file, index=False)
        
        return checkpoint_data
    
    def load_checkpoint(self):
        """Load checkpoint if exists"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r') as f:
                    data = json.load(f)
                
                # Load results
                if self.results_file.exists():
                    df_results = pd.read_csv(self.results_file)
                    results = df_results.to_dict('records')
                else:
                    results = []
                
                return set(data['processed_indices']), results
            except Exception as e:
                print(f"⚠️  Error loading checkpoint: {e}")
                return set(), []
        
        return set(), []
    
    def clear_checkpoint(self):
        
