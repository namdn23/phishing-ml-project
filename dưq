#!/usr/bin/env python3
"""
Advanced Feature Extractor v·ªõi Resume t·ª´ Checkpoint
T·ªëi ∆∞u cho Intel Ultra 5H (27GB RAM, 56 workers)
"""

import pandas as pd
import numpy as np
import requests
import whois
import ssl
import socket
import time
import json
import hashlib
from datetime import datetime
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ===================== CONFIG =====================
CONFIG = {
    'max_workers': 56,
    'timeout': 5,
    'max_retries': 2,
    'checkpoint_interval': 500,  # Checkpoint m·ªói 500 URLs
    'output_dir': 'checkpoints',
    'rate_limit': {
        'whois': 5,
        'http': 20,
        'ssl': 50
    },
    # Anti-detection headers
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
}

# Create checkpoint directory
Path(CONFIG['output_dir']).mkdir(exist_ok=True)

# ===================== RATE LIMITER =====================
from collections import defaultdict
from threading import Lock

class RateLimiter:
    def __init__(self):
        self.locks = defaultdict(Lock)
        self.last_call = defaultdict(float)
    
    def wait(self, key, min_interval):
        with self.locks[key]:
            elapsed = time.time() - self.last_call[key]
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)
            self.last_call[key] = time.time()

rate_limiter = RateLimiter()

# ===================== UTILITY FUNCTIONS =====================

def safe_request(url, timeout=5, follow_redirects=True):
    """HTTP request v·ªõi retry + anti-detection"""
    session = requests.Session()
    
    for attempt in range(CONFIG['max_retries']):
        try:
            rate_limiter.wait('http', 1.0 / CONFIG['rate_limit']['http'])
            
            response = session.get(
                url,
                headers=CONFIG['headers'],
                timeout=timeout,
                allow_redirects=follow_redirects,
                verify=False
            )
            return response
        except Exception as e:
            if attempt == CONFIG['max_retries'] - 1:
                return None
            time.sleep(0.5 * (attempt + 1))  # Exponential backoff
    return None

def extract_domain_parts(url):
    """Parse URL th√†nh c√°c ph·∫ßn"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc
        path = parsed.path
        
        # Split domain
        parts = domain.split('.')
        
        return {
            'full_domain': domain,
            'subdomain': '.'.join(parts[:-2]) if len(parts) > 2 else '',
            'main_domain': parts[-2] if len(parts) >= 2 else '',
            'tld': parts[-1] if parts else '',
            'path': path,
            'scheme': parsed.scheme
        }
    except:
        return None

# ===================== STATIC FEATURE EXTRACTORS =====================

def extract_static_features(url):
    """Tr√≠ch xu·∫•t T·∫§T C·∫¢ static features (instant)"""
    features = {}
    
    try:
        parts = extract_domain_parts(url)
        if not parts:
            return {k: -1 for k in [
                'Subdomain_Count', 'maxSub30', 'hasKeyword', 
                'Path_Depth', 'URL_Length', 'Levenshtein_Brand'
            ]}
        
        # 1. Subdomain_Count
        subdomain = parts['subdomain']
        features['Subdomain_Count'] = subdomain.count('.') + 1 if subdomain else 0
        
        # 2. maxSub30 (max length of subdomain parts)
        if subdomain:
            sub_parts = subdomain.split('.')
            features['maxSub30'] = max(len(part) for part in sub_parts)
        else:
            features['maxSub30'] = 0
        
        # 3. hasKeyword (suspicious keywords)
        KEYWORDS = [
            'login', 'signin', 'account', 'verify', 'secure', 'update',
            'bank', 'paypal', 'ebay', 'amazon', 'apple', 'microsoft',
            'password', 'confirm', 'suspended', 'locked', 'alert',
            'security', 'credential', 'validate', 'auth'
        ]
        url_lower = url.lower()
        features['hasKeyword'] = 1 if any(kw in url_lower for kw in KEYWORDS) else 0
        
        # 4. Path_Depth
        path = parts['path'].strip('/')
        features['Path_Depth'] = path.count('/') if path else 0
        
        # 5. URL_Length
        features['URL_Length'] = len(url)
        
        # 6. Levenshtein_Brand (distance to known brands)
        try:
            from Levenshtein import distance
            
            BRANDS = [
                'google', 'facebook', 'paypal', 'amazon', 'microsoft',
                'apple', 'netflix', 'instagram', 'twitter', 'linkedin',
                'github', 'dropbox', 'yahoo', 'ebay', 'adobe', 'chase',
                'wellsfargo', 'bankofamerica', 'citibank', 'americanexpress'
            ]
            
            main_domain = parts['main_domain'].lower()
            distances = [distance(main_domain, brand) for brand in BRANDS]
            features['Levenshtein_Brand'] = min(distances)
        except:
            # Fallback: simple check
            features['Levenshtein_Brand'] = 10  # Default high value
        
        return features
        
    except Exception as e:
        return {k: -1 for k in [
            'Subdomain_Count', 'maxSub30', 'hasKeyword', 
            'Path_Depth', 'URL_Length', 'Levenshtein_Brand'
        ]}

# ===================== DYNAMIC FEATURE EXTRACTORS =====================

def extract_domain_age(url):
    """Domain age via WHOIS"""
    try:
        rate_limiter.wait('whois', 1.0 / CONFIG['rate_limit']['whois'])
        
        parts = extract_domain_parts(url)
        if not parts:
            return -1
        
        domain = parts['full_domain']
        w = whois.whois(domain)
        
        created = w.creation_date
        if isinstance(created, list):
            created = created[0]
        
        if created:
            age_days = (datetime.now() - created).days
            return max(0, age_days)  # Kh√¥ng √¢m
        return -1
    except:
        return -1

def extract_certificate_age(url):
    """SSL certificate age"""
    try:
        rate_limiter.wait('ssl', 1.0 / CONFIG['rate_limit']['ssl'])
        
        parts = extract_domain_parts(url)
        if not parts or parts['scheme'] != 'https':
            return -1
        
        domain = parts['full_domain']
        
        context = ssl.create_default_context()
        with socket.create_connection((domain, 443), timeout=CONFIG['timeout']) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                not_before = datetime.strptime(cert['notBefore'], '%b %d %H:%M:%S %Y %Z')
                age_days = (datetime.now() - not_before).days
                return max(0, age_days)
    except:
        return -1

def extract_redirect_count(url):
    """Count HTTP redirects"""
    try:
        response = safe_request(url, follow_redirects=True)
        if response:
            return len(response.history)
        return -1
    except:
        return -1

def extract_html_features(url):
    """
    Tr√≠ch xu·∫•t: Favicon_Match, External_Links_Ratio, Has_Popup
    Ch·ªâ c·∫ßn 1 request ƒë·ªÉ l·∫•y c·∫£ 3 features
    """
    features = {
        'Favicon_Match': -1,
        'External_Links_Ratio': -1,
        'Has_Popup': -1
    }
    
    try:
        response = safe_request(url, follow_redirects=False)
        if not response or response.status_code != 200:
            return features
        
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')
        parts = extract_domain_parts(url)
        if not parts:
            return features
        
        original_domain = parts['full_domain']
        
        # 1. Favicon_Match
        favicon = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')
        if favicon and favicon.get('href'):
            favicon_url = favicon['href']
            
            # Handle relative URLs
            if not favicon_url.startswith('http'):
                from urllib.parse import urljoin
                favicon_url = urljoin(url, favicon_url)
            
            favicon_parts = extract_domain_parts(favicon_url)
            if favicon_parts:
                favicon_domain = favicon_parts['full_domain']
                features['Favicon_Match'] = 1 if (favicon_domain == original_domain or not favicon_domain) else 0
            else:
                features['Favicon_Match'] = 0
        else:
            features['Favicon_Match'] = 0
        
        # 2. External_Links_Ratio
        all_links = soup.find_all('a', href=True)
        if all_links:
            external = sum(1 for link in all_links if original_domain not in link.get('href', ''))
            features['External_Links_Ratio'] = round(external / len(all_links), 3)
        else:
            features['External_Links_Ratio'] = 0
        
        # 3. Has_Popup
        html_lower = html.lower()
        popup_keywords = [
            'window.open(', 'popup', 'modal', 'overlay',
            'alert(', 'confirm(', 'prompt(', 'showmodal'
        ]
        features['Has_Popup'] = 1 if any(kw in html_lower for kw in popup_keywords) else 0
        
        return features
        
    except Exception as e:
        return features

def extract_dynamic_features(url):
    """Tr√≠ch xu·∫•t T·∫§T C·∫¢ dynamic features"""
    features = {}
    
    try:
        # HTML features (1 request cho 3 features)
        html_features = extract_html_features(url)
        features.update(html_features)
        
        # Redirect count (ri√™ng bi·ªát)
        features['Redirect_Count'] = extract_redirect_count(url)
        
        # Domain age (WHOIS - slow)
        features['Domain_Age'] = extract_domain_age(url)
        
        # Certificate age (SSL)
        features['Certificate_Age'] = extract_certificate_age(url)
        
        return features
        
    except Exception as e:
        return {
            'Favicon_Match': -1,
            'External_Links_Ratio': -1,
            'Has_Popup': -1,
            'Redirect_Count': -1,
            'Domain_Age': -1,
            'Certificate_Age': -1
        }

# ===================== MAIN EXTRACTOR =====================

def extract_all_new_features(row):
    """
    Tr√≠ch xu·∫•t t·∫•t c·∫£ features m·ªõi cho 1 row
    Input: pandas Series (1 row t·ª´ DataFrame)
    Output: dict v·ªõi t·∫•t c·∫£ features m·ªõi
    """
    url = row.get('url', '')
    
    result = {
        'url': url,
        'timestamp': datetime.now().isoformat(),
        'status': 'pending'
    }
    
    try:
        # 1. Static features (instant - <1ms)
        static = extract_static_features(url)
        result.update(static)
        
        # 2. Dynamic features (slow - 5-10s)
        dynamic = extract_dynamic_features(url)
        result.update(dynamic)
        
        result['status'] = 'success'
        
    except Exception as e:
        result['status'] = 'error'
        result['error_message'] = str(e)
        
        # Fill missing v·ªõi -1
        missing_keys = [
            'Subdomain_Count', 'maxSub30', 'hasKeyword', 'Path_Depth',
            'URL_Length', 'Levenshtein_Brand', 'Favicon_Match',
            'External_Links_Ratio', 'Has_Popup', 'Redirect_Count',
            'Domain_Age', 'Certificate_Age'
        ]
        for key in missing_keys:
            if key not in result:
                result[key] = -1
    
    return result

# ===================== CHECKPOINT MANAGER =====================

class CheckpointManager:
    def __init__(self, output_dir, input_file):
        self.output_dir = Path(output_dir)
        self.input_file = input_file
        self.checkpoint_file = self.output_dir / f"checkpoint_{self._get_file_hash()}.json"
        self.results_file = self.output_dir / f"results_{self._get_file_hash()}.csv"
    
    def _get_file_hash(self):
        """T·∫°o hash t·ª´ filename ƒë·ªÉ tr√°nh conflict"""
        return hashlib.md5(self.input_file.encode()).hexdigest()[:8]
    
    def save_checkpoint(self, processed_indices, results):
        """L∆∞u checkpoint"""
        checkpoint_data = {
            'timestamp': datetime.now().isoformat(),
            'processed_indices': list(processed_indices),
            'total_processed': len(processed_indices),
        }
        
        with open(self.checkpoint_file, 'w') as f:
            json.dump(checkpoint_data, f)
        
        # Save results
        if results:
            df_results = pd.DataFrame(results)
            df_results.to_csv(self.results_file, index=False)
        
        return checkpoint_data
    
    def load_checkpoint(self):
        """Load checkpoint n·∫øu c√≥"""
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r') as f:
                data = json.load(f)
            
            # Load results
            if self.results_file.exists():
                df_results = pd.read_csv(self.results_file)
                results = df_results.to_dict('records')
            else:
                results = []
            
            return set(data['processed_indices']), results
        
        return set(), []
    
    def clear_checkpoint(self):
        """X√≥a checkpoint sau khi ho√†n th√†nh"""
        if self.checkpoint_file.exists():
            self.checkpoint_file.unlink()

# ===================== BATCH PROCESSOR =====================

def process_batch(df, checkpoint_manager, skip_dynamic=False):
    """
    X·ª≠ l√Ω batch v·ªõi checkpoint & resume
    
    Args:
        df: DataFrame g·ªëc
        checkpoint_manager: CheckpointManager instance
        skip_dynamic: N·∫øu True, ch·ªâ tr√≠ch xu·∫•t static features (nhanh)
    """
    
    # Load checkpoint n·∫øu c√≥
    processed_indices, existing_results = checkpoint_manager.load_checkpoint()
    
    if processed_indices:
        print(f"üìÇ Found checkpoint: {len(processed_indices)} URLs already processed")
        remaining_df = df[~df.index.isin(processed_indices)]
    else:
        remaining_df = df
        existing_results = []
    
    if len(remaining_df) == 0:
        print("‚úÖ All URLs already processed!")
        return pd.DataFrame(existing_results)
    
    print(f"üöÄ Processing {len(remaining_df)} URLs with {CONFIG['max_workers']} workers...")
    if skip_dynamic:
        print("‚ö° FAST MODE: Only extracting static features")
    
    results = existing_results.copy()
    start_time = time.time()
    processed_count = len(processed_indices)
    
    with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:
        # Submit tasks
        futures = {}
        for idx, row in remaining_df.iterrows():
            if skip_dynamic:
                # Ch·ªâ extract static
                future = executor.submit(extract_static_features, row.get('url', ''))
            else:
                # Extract full
                future = executor.submit(extract_all_new_features, row)
            futures[future] = (idx, row)
        
        # Progress bar
        with tqdm(total=len(remaining_df), desc="Extracting", initial=0) as pbar:
            for future in as_completed(futures):
                idx, row = futures[future]
                
                try:
                    result = future.result(timeout=60)
                    
                    # Merge v·ªõi data g·ªëc n·∫øu c·∫ßn
                    if skip_dynamic:
                        result = {'url': row.get('url', ''), **result}
                    
                    results.append(result)
                    processed_indices.add(idx)
                    processed_count += 1
                    
                    # Checkpoint
                    if processed_count % CONFIG['checkpoint_interval'] == 0:
                        checkpoint_manager.save_checkpoint(processed_indices, results)
                        elapsed = time.time() - start_time
                        rate = processed_count / elapsed
                        eta = (len(df) - processed_count) / rate / 60
                        print(f"\nüíæ Checkpoint saved: {processed_count}/{len(df)} URLs")
                        print(f"‚è±Ô∏è  Rate: {rate:.1f} URL/s | ETA: {eta:.1f} min")
                
                except Exception as e:
                    print(f"\n‚ùå Error processing {row.get('url', 'unknown')}: {e}")
                    results.append({
                        'url': row.get('url', ''),
                        'status': 'timeout',
                        'error_message': str(e)
                    })
                
                pbar.update(1)
                
                # Update postfix m·ªói 100 URLs
                if processed_count % 100 == 0:
                    elapsed = time.time() - start_time
                    rate = processed_count / elapsed
                    eta = (len(df) - processed_count) / rate / 60
                    pbar.set_postfix({
                        'rate': f'{rate:.1f} URL/s',
                        'ETA': f'{eta:.1f}m'
                    })
    
    # Final save
    checkpoint_manager.save_checkpoint(processed_indices, results)
    
    elapsed = time.time() - start_time
    print(f"\n‚úÖ Extraction completed!")
    print(f"‚è±Ô∏è  Total time: {elapsed/60:.1f} minutes")
    print(f"üìà Average rate: {len(df)/elapsed:.1f} URLs/second")
    
    return pd.DataFrame(results)

# ===================== MAIN FUNCTION =====================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Feature Extraction v·ªõi Checkpoint')
    parser.add_argument('--input', default='Dataset_v32_Final.csv', help='Input CSV')
    parser.add_argument('--output', default='Dataset_v33_WithNewFeatures.csv', help='Output CSV')
    parser.add_argument('--sample', type=int, help='Test v·ªõi N URLs')
    parser.add_argument('--skip-dynamic', action='store_true', help='Ch·ªâ extract static (nhanh)')
    parser.add_argument('--resume', action='store_true', help='Resume t·ª´ checkpoint')
    parser.add_argument('--clear-checkpoint', action='store_true', help='X√≥a checkpoint c≈©')
    
    args = parser.parse_args()
    
    # Initialize checkpoint manager
    checkpoint_manager = CheckpointManager(CONFIG['output_dir'], args.input)
    
    if args.clear_checkpoint:
        checkpoint_manager.clear_checkpoint()
        print("üóëÔ∏è  Checkpoint cleared")
        return
    
    # Load data
    print(f"üìÇ Loading {args.input}...")
    df = pd.read_csv(args.input)
    print(f"‚úÖ Loaded {len(df)} rows")
    
    if args.sample:
        df = df.head(args.sample)
        print(f"‚ö†Ô∏è  Sample mode: {args.sample} URLs")
    
    # Estimate time
    if not args.skip_dynamic:
        avg_time_per_url = 8  # seconds (with dynamic features)
        estimated_hours = (len(df) * avg_time_per_url) / CONFIG['max_workers'] / 3600
        print(f"\n‚è±Ô∏è  ESTIMATED TIME: {estimated_hours:.1f} hours")
        print(f"   (Assuming {avg_time_per_url}s/URL v·ªõi {CONFIG['max_workers']} workers)")
    else:
        print(f"\n‚ö° FAST MODE: ~5-10 minutes for {len(df)} URLs")
    
    print(f"\nüéØ Features to extract:")
    print(f"   Static (instant): 6 features")
    if not args.skip_dynamic:
        print(f"   Dynamic (slow): 6 features")
    print(f"   TOTAL: {'12' if not args.skip_dynamic else '6'} new features")
    
    input("\nPress ENTER to start (Ctrl+C to cancel)...")
    
    # Process
    df_new_features = process_batch(df, checkpoint_manager, skip_dynamic=args.skip_dynamic)
    
    # Merge v·ªõi DataFrame g·ªëc
    print("\nüîó Merging with original data...")
    
    # Keep features to retain
    keep_cols = [
        'NoOfDegitsInURL', 'IsHTTPS', 'HasExternalFormSubmit',
        'HasSubmitButton', 'HasPasswordField', 'V8_Total_IFrames',
        'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation',
        'Is_Top_1M_Domain', 'V23_Entropy_Subdomain',
        'DomainTitleMatchScore', 'HasSocialNet', 'HasCopyrightInfo',
        'V2_Layout_Similarity', 'V4_DNS_Volatility_Count',
        'label'
    ]
    
    # Ensure 'url' column exists in original df
    if 'url' not in df.columns:
        print("‚ùå Error: 'url' column not found in input CSV")
        return
    
    df_original = df[['url'] + [col for col in keep_cols if col in df.columns]]
    
    # Merge
    df_final = df_original.merge(df_new_features, on='url', how='left')
    
    # Save
    df_final.to_csv(args.output, index=False)
    print(f"\nüíæ Results saved to {args.output}")
    
    # Summary
    success_count = (df_new_features['status'] == 'success').sum()
    error_count = len(df_new_features) - success_count
    
    print(f"\nüìä Summary:")
    print(f"   ‚úÖ Success: {success_count} ({success_count/len(df)*100:.1f}%)")
    print(f"   ‚ùå Errors: {error_count} ({error_count/len(df)*100:.1f}%)")
    
    # Feature stats
    if not args.skip_dynamic:
        print(f"\nüìà Feature Statistics (new features):")
        for col in ['URL_Length', 'Domain_Age', 'Redirect_Count', 'Certificate_Age']:
            if col in df_new_features.columns:
                valid = df_new_features[df_new_features[col] != -1][col]
                if len(valid) > 0:
                    print(f"   {col}: mean={valid.mean():.1f}, median={valid.median():.1f}")
    
    # Clear checkpoint
    checkpoint_manager.clear_checkpoint()
    print("\nüóëÔ∏è  Checkpoint cleared (extraction complete)")

if __name__ == '__main__':
    main()
