import pandas as pd
import requests
import concurrent.futures
import time
import os
import warnings

# --- C·∫§U H√åNH ---
INPUT_FILE = 'PhiUSIIL_Final_Dataset_Clean.csv'       # <--- ƒê√É C·∫¨P NH·∫¨T T√äN FILE C·ª¶A B·∫†N
OUTPUT_GOOD = 'urldata_clean.csv'    # File k·∫øt qu·∫£ (URL s·ªëng)import pandas as pd
import requests
import concurrent.futures
import time
import sys
import os

# --- C·∫§U H√åNH ---
INPUT_FILE = 'urldata_old_121k.csv'  # ƒê·ªïi t√™n th√†nh file data c≈© c·ªßa b·∫°n
OUTPUT_FILE = 'urldata_live_only.csv' # File k·∫øt qu·∫£ ch·ª©a link c√≤n s·ªëng
MAX_WORKERS = 100                     # Qu√©t nhanh n√™n b·∫≠t 100 lu·ªìng lu√¥n
TIMEOUT = 3                           # 3 gi√¢y kh√¥ng ph·∫£n h·ªìi l√† coi nh∆∞ ch·∫øt

def check_alive(row):
    """
    H√†m ki·ªÉm tra nhanh xem URL c√≤n s·ªëng kh√¥ng
    """
    url = str(row.get('url', '')).strip()
    label = row.get('label', '')
    
    if not url: return None
    if not url.startswith(('http://', 'https://')):
        url = 'http://' + url
        
    try:
        # Ch·ªâ g·ª≠i request HEAD (nh·∫π h∆°n GET) ƒë·ªÉ check status
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'}
        response = requests.head(url, headers=headers, timeout=TIMEOUT, allow_redirects=True, verify=False)
        
        # N·∫øu server tr·∫£ v·ªÅ code < 400 (200, 301, 302...) l√† S·ªêNG
        if response.status_code < 400:
            return {'url': url, 'label': label}
            
        # N·∫øu HEAD l·ªói (m·ªôt s·ªë server ch·∫∑n HEAD), th·ª≠ GET nh·∫π c√°i
        if response.status_code >= 400:
             response = requests.get(url, headers=headers, timeout=TIMEOUT, stream=True, verify=False)
             if response.status_code < 400:
                 return {'url': url, 'label': label}
                 
    except:
        pass # L·ªói k·∫øt n·ªëi -> Coi nh∆∞ ch·∫øt
    
    return None

def main():
    print(f"üöÄ B·∫ÆT ƒê·∫¶U L·ªåC URL S·ªêNG/CH·∫æT")
    print(f"   üìÇ Input: {INPUT_FILE}")
    
    if not os.path.exists(INPUT_FILE):
        print("‚ùå Kh√¥ng t√¨m th·∫•y file input.")
        return

    try:
        df = pd.read_csv(INPUT_FILE)
        print(f"üìä T·ªïng s·ªë URL ban ƒë·∫ßu: {len(df):,}")
    except:
        print("‚ùå L·ªói ƒë·ªçc file CSV.")
        return

    data = df.to_dict('records')
    live_urls = []
    
    start_time = time.time()
    processed = 0
    total = len(data)

    print("‚è≥ ƒêang qu√©t (T·ªëc ƒë·ªô cao)...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_row = {executor.submit(check_alive, row): row for row in data}
        
        for future in concurrent.futures.as_completed(future_to_row):
            result = future.result()
            processed += 1
            
            if result:
                live_urls.append(result)
            
            # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô
            if processed % 100 == 0 or processed == total:
                percent = (processed / total) * 100
                sys.stdout.write(f"\r   ‚ñ∂ ƒê√£ check: {processed}/{total} ({percent:.1f}%) | üå± C√≤n s·ªëng: {len(live_urls)}")
                sys.stdout.flush()

    print("\n" + "="*60)
    print(f"‚úÖ HO√ÄN T·∫§T!")
    print(f"   üíÄ ƒê√£ lo·∫°i b·ªè: {total - len(live_urls)} URL ch·∫øt.")
    print(f"   üå± Gi·ªØ l·∫°i:    {len(live_urls)} URL s·ªëng.")
    
    if live_urls:
        final_df = pd.DataFrame(live_urls)
        final_df.to_csv(OUTPUT_FILE, index=False)
        print(f"üíæ ƒê√£ l∆∞u v√†o: {OUTPUT_FILE}")
        
        # Th·ªëng k√™ nhanh
        print("\nüìä Th·ªëng k√™ d·ªØ li·ªáu m·ªõi:")
        print(final_df['label'].value_counts())
    else:
        print("‚ùå Kh√¥ng c√≤n URL n√†o s·ªëng s√≥t!")

if __name__ == "__main__":
    import warnings
    warnings.filterwarnings('ignore')
    main()
OUTPUT_BAD = 'urldata_dead.csv'      # File k·∫øt qu·∫£ (URL ch·∫øt)
MAX_WORKERS = 100                    # S·ªë lu·ªìng ch·∫°y song song

# Headers gi·∫£ l·∫≠p tr√¨nh duy·ªát
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'
}

def check_url_status(row):
    # L·∫•y URL, x·ª≠ l√Ω tr∆∞·ªùng h·ª£p data b·ªã r·ªóng
    url = str(row.get('url', '')).strip()
    if not url or url.lower() == 'nan': return None
    
    # Th√™m http n·∫øu thi·∫øu
    if not url.startswith(('http://', 'https://')):
        target_url = 'http://' + url
    else:
        target_url = url

    try:
        # stream=True: Ch·ªâ t·∫£i Headers -> C·ª±c nhanh
        # verify=False: B·ªè qua l·ªói SSL (nhi·ªÅu web phishing SSL l·ªüm nh∆∞ng v·∫´n s·ªëng)
        response = requests.get(target_url, headers=HEADERS, timeout=5, stream=True, verify=False)
        
        # L·∫•y c√°c trang tr·∫£ v·ªÅ code 200 (OK)
        if 200 <= response.status_code < 300:
            return {**row, 'status': 'alive'}
        else:
            return {**row, 'status': 'dead', 'code': response.status_code}
            
    except:
        # M·ªçi l·ªói (Timeout, DNS...) ƒë·ªÅu coi l√† ch·∫øt
        return {**row, 'status': 'dead', 'code': 'error'}

def main():
    # T·∫Øt c·∫£nh b√°o SSL
    requests.packages.urllib3.disable_warnings()
    
    print(f"üöÄ B·∫Øt ƒë·∫ßu l·ªçc URL t·ª´ file: {INPUT_FILE}")
    
    try:
        # ƒê·ªçc file CSV
        df = pd.read_csv(INPUT_FILE)
        print(f"üìä T·ªïng s·ªë l∆∞·ª£ng URL g·ªëc: {len(df)}")
        
        # Ki·ªÉm tra xem c√≥ c·ªôt 'url' kh√¥ng
        if 'url' not in df.columns:
            print("‚ùå L·ªói: File CSV kh√¥ng c√≥ c·ªôt t√™n l√† 'url'. H√£y ki·ªÉm tra l·∫°i header!")
            return
            
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file '{INPUT_FILE}' trong th∆∞ m·ª•c n√†y!")
        return
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {e}")
        return

    data = df.to_dict('records')
    good_urls = []
    bad_urls = []
    
    start_time = time.time()
    processed_count = 0
    total = len(data)

    print(f"üî• ƒêang ch·∫°y {MAX_WORKERS} lu·ªìng song song...")

    # CH·∫†Y ƒêA LU·ªíNG
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {executor.submit(check_url_status, item): item for item in data}
        
        for future in concurrent.futures.as_completed(future_to_url):
            result = future.result()
            processed_count += 1
            
            if result:
                if result['status'] == 'alive':
                    # X√≥a key 'status' t·∫°m tr∆∞·ªõc khi l∆∞u
                    del result['status']
                    good_urls.append(result)
                else:
                    bad_urls.append(result)
            
            # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô
            if processed_count % 50 == 0 or processed_count == total:
                percent = (processed_count / total) * 100
                print(f"   ‚è≥ X·ª≠ l√Ω: {processed_count}/{total} ({percent:.1f}%) | ‚úÖ S·ªëng: {len(good_urls)} | ‚ùå Ch·∫øt: {len(bad_urls)}", end='\r')

    print("\n" + "="*50)
    print(f"‚úÖ HO√ÄN T·∫§T! Th·ªùi gian: {time.time() - start_time:.2f}s")
    
    # L∆∞u file k·∫øt qu·∫£
    if good_urls:
        pd.DataFrame(good_urls).to_csv(OUTPUT_GOOD, index=False)
        print(f"üíæ ƒê√£ l∆∞u {len(good_urls)} URL s·∫°ch v√†o: {OUTPUT_GOOD} (D√πng file n√†y ƒë·ªÉ ch·∫°y Feature Extractor)")
    
    if bad_urls:
        pd.DataFrame(bad_urls).to_csv(OUTPUT_BAD, index=False)
        print(f"üóëÔ∏è  ƒê√£ lo·∫°i b·ªè {len(bad_urls)} URL ch·∫øt.")

if __name__ == "__main__":
    main()
