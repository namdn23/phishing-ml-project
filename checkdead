import pandas as pd
import requests
import concurrent.futures
import time
import os
import warnings

# --- C·∫§U H√åNH ---
INPUT_FILE = 'urldata.csv'       # <--- ƒê√É C·∫¨P NH·∫¨T T√äN FILE C·ª¶A B·∫†N
OUTPUT_GOOD = 'urldata_clean.csv'    # File k·∫øt qu·∫£ (URL s·ªëng)
OUTPUT_BAD = 'urldata_dead.csv'      # File k·∫øt qu·∫£ (URL ch·∫øt)
MAX_WORKERS = 100                    # S·ªë lu·ªìng ch·∫°y song song

# Headers gi·∫£ l·∫≠p tr√¨nh duy·ªát
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'
}

def check_url_status(row):
    # L·∫•y URL, x·ª≠ l√Ω tr∆∞·ªùng h·ª£p data b·ªã r·ªóng
    url = str(row.get('url', '')).strip()
    if not url or url.lower() == 'nan': return None
    
    # Th√™m http n·∫øu thi·∫øu
    if not url.startswith(('http://', 'https://')):
        target_url = 'http://' + url
    else:
        target_url = url

    try:
        # stream=True: Ch·ªâ t·∫£i Headers -> C·ª±c nhanh
        # verify=False: B·ªè qua l·ªói SSL (nhi·ªÅu web phishing SSL l·ªüm nh∆∞ng v·∫´n s·ªëng)
        response = requests.get(target_url, headers=HEADERS, timeout=5, stream=True, verify=False)
        
        # L·∫•y c√°c trang tr·∫£ v·ªÅ code 200 (OK)
        if 200 <= response.status_code < 300:
            return {**row, 'status': 'alive'}
        else:
            return {**row, 'status': 'dead', 'code': response.status_code}
            
    except:
        # M·ªçi l·ªói (Timeout, DNS...) ƒë·ªÅu coi l√† ch·∫øt
        return {**row, 'status': 'dead', 'code': 'error'}

def main():
    # T·∫Øt c·∫£nh b√°o SSL
    requests.packages.urllib3.disable_warnings()
    
    print(f"üöÄ B·∫Øt ƒë·∫ßu l·ªçc URL t·ª´ file: {INPUT_FILE}")
    
    try:
        # ƒê·ªçc file CSV
        df = pd.read_csv(INPUT_FILE)
        print(f"üìä T·ªïng s·ªë l∆∞·ª£ng URL g·ªëc: {len(df)}")
        
        # Ki·ªÉm tra xem c√≥ c·ªôt 'url' kh√¥ng
        if 'url' not in df.columns:
            print("‚ùå L·ªói: File CSV kh√¥ng c√≥ c·ªôt t√™n l√† 'url'. H√£y ki·ªÉm tra l·∫°i header!")
            return
            
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file '{INPUT_FILE}' trong th∆∞ m·ª•c n√†y!")
        return
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {e}")
        return

    data = df.to_dict('records')
    good_urls = []
    bad_urls = []
    
    start_time = time.time()
    processed_count = 0
    total = len(data)

    print(f"üî• ƒêang ch·∫°y {MAX_WORKERS} lu·ªìng song song...")

    # CH·∫†Y ƒêA LU·ªíNG
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {executor.submit(check_url_status, item): item for item in data}
        
        for future in concurrent.futures.as_completed(future_to_url):
            result = future.result()
            processed_count += 1
            
            if result:
                if result['status'] == 'alive':
                    # X√≥a key 'status' t·∫°m tr∆∞·ªõc khi l∆∞u
                    del result['status']
                    good_urls.append(result)
                else:
                    bad_urls.append(result)
            
            # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô
            if processed_count % 50 == 0 or processed_count == total:
                percent = (processed_count / total) * 100
                print(f"   ‚è≥ X·ª≠ l√Ω: {processed_count}/{total} ({percent:.1f}%) | ‚úÖ S·ªëng: {len(good_urls)} | ‚ùå Ch·∫øt: {len(bad_urls)}", end='\r')

    print("\n" + "="*50)
    print(f"‚úÖ HO√ÄN T·∫§T! Th·ªùi gian: {time.time() - start_time:.2f}s")
    
    # L∆∞u file k·∫øt qu·∫£
    if good_urls:
        pd.DataFrame(good_urls).to_csv(OUTPUT_GOOD, index=False)
        print(f"üíæ ƒê√£ l∆∞u {len(good_urls)} URL s·∫°ch v√†o: {OUTPUT_GOOD} (D√πng file n√†y ƒë·ªÉ ch·∫°y Feature Extractor)")
    
    if bad_urls:
        pd.DataFrame(bad_urls).to_csv(OUTPUT_BAD, index=False)
        print(f"üóëÔ∏è  ƒê√£ lo·∫°i b·ªè {len(bad_urls)} URL ch·∫øt.")

if __name__ == "__main__":
    main()
