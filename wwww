import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright

# =========================================================
# 1. Cáº¤U HÃŒNH ÄA LUá»’NG & Há»† THá»NG
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "processed_urls.log"
TOP_1M_FILE = "top-1m.csv"

# Sá»‘ lÆ°á»£ng luá»“ng cháº¡y song song (TÃ¹y vÃ o RAM, 30-40 lÃ  Ä‘áº¹p trÃªn Kali)
CONCURRENT_PAGES = 35 
TIMEOUT = 45000 # 45 giÃ¢y cho má»—i trang

# =========================================================
# 2. HÃ€M Bá»” TRá»¢ & Äáº¶C TRÆ¯NG TÄ¨NH (8 Features)
# =========================================================
def load_processed_urls():
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, 'r') as f:
            return set(line.split('|')[0].strip() for line in f if line.strip())
    return set()

try:
    top_1m_set = set(pd.read_csv(TOP_1M_FILE, header=None)[1].astype(str).str.lower().values)
except:
    top_1m_set = set()

def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    return {
        'domainEntropy': calculate_entropy(domain),
        'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
        'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
        'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
        'domainLength': len(domain),
        'Subdomain_Level': domain.count('.'),
        'IsHTTPS': 1 if url.startswith('https') else 0,
        'Is_Top_1M_Domain': 1 if any(d in top_1m_set for d in [domain, '.'.join(domain.split('.')[-2:])]) else 0
    }

async def intercept_route(route):
    # Cháº·n tÃ i nguyÃªn náº·ng Ä‘á»ƒ Ä‘a luá»“ng cháº¡y mÆ°á»£t hÆ¡n
    if route.request.resource_type in ["image", "media", "font"]:
        await route.abort()
    else:
        try: await route.continue_()
        except: pass

# =========================================================
# 3. LOGIC TRÃCH XUáº¤T Äá»˜NG (10 Features) - Xá»¬ LÃ ÄA LUá»’NG
# =========================================================
async def extract_task(url, label, context, semaphore, log_handle):
    static_data = extract_static(url)
    status = "SKIP"
    
    if static_data['Is_Top_1M_Domain'] == 1:
        dynamic_data = {
            'Outlink_Ratio': 0.0, 'HasExternalFormSubmit': 0, 'HasPasswordField': 0,
            'DomainTitleMatchScore': 1, 'HasSocialNet': 1, 'HasCopyrightInfo': 1, 'HasDescription': 1,
            'V9_Has_Hidden_IFrame': 0, 'V5_TLS_Issuer_Reputation': 1, 'V4_DNS_Volatility_Count': 0
        }
    else:
        # Sá»­ dá»¥ng semaphore Ä‘á»ƒ giá»›i háº¡n sá»‘ tab má»Ÿ cÃ¹ng lÃºc
        async with semaphore:
            page = await context.new_page()
            # Giáº£ dáº¡ng ngÆ°á»i dÃ¹ng (Anti-bot)
            await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            await page.route("**/*", intercept_route)

            try:
                await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
                await asyncio.sleep(1.5) # Chá» trang á»•n Ä‘á»‹nh

                domain = urlparse(url).netloc.lower()
                content = (await page.content()).lower()
                title = (await page.title()).lower()

                # TrÃ­ch xuáº¥t liÃªn káº¿t
                all_links = await page.query_selector_all('a')
                ext_l = 0
                for l in all_links:
                    try:
                        h = await l.get_attribute('href')
                        if h and 'http' in h and domain not in h: ext_l += 1
                    except: continue

                dynamic_data = {
                    'Outlink_Ratio': ext_l / len(all_links) if len(all_links) > 0 else 0,
                    'HasExternalFormSubmit': 1 if await page.query_selector('form[action^="http"]') else 0,
                    'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                    'DomainTitleMatchScore': 1 if domain.split('.')[0] in title else 0,
                    'HasSocialNet': 1 if any(s in content for s in ['facebook', 'twitter', 'linkedin', 'instagram']) else 0,
                    'HasCopyrightInfo': 1 if ('Â©' in content or 'copyright' in content) else 0,
                    'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
                    'V9_Has_Hidden_IFrame': 1 if any(not await f.is_visible() for f in await page.query_selector_all('iframe')) else 0,
                    'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0,
                    'V4_DNS_Volatility_Count': 0
                }
                status = "SUCCESS"
            except Exception:
                # Náº¿u trang lá»—i gÃ¡n giÃ¡ trá»‹ 0.5 (Neutral)
                dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                                'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 
                                                'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}
                status = "FAILED"
            finally:
                if not page.is_closed(): await page.close()

    # LÆ°u Log chi tiáº¿t
    log_handle.write(f"{url} | {status} | Out:{dynamic_data.get('Outlink_Ratio',0):.2f} | {datetime.now().strftime('%H:%M:%S')}\n")
    log_handle.flush()
    
    return {**static_data, **dynamic_data, 'label': label}

# =========================================================
# 4. QUáº¢N LÃ TIáº¾N TRÃŒNH CHÃNH
# =========================================================
async def main():
    start_time = time.time()
    processed_urls = load_processed_urls()

    if not os.path.exists(INPUT_FILE):
        print(f"âŒ KhÃ´ng tháº¥y file {INPUT_FILE}")
        return

    df_all = pd.read_csv(INPUT_FILE)
    url_col = [c for c in df_all.columns if 'url' in c.lower()][0]
    label_col = [c for c in df_all.columns if 'label' in c.lower()][0]
    
    df_to_do = df_all[~df_all[url_col].isin(processed_urls)]
    total_todo = len(df_to_do)
    
    print(f"ğŸš€ Báº¯t Ä‘áº§u Ä‘a luá»“ng: {CONCURRENT_PAGES} luá»“ng | CÃ²n láº¡i: {total_todo} URL")

    semaphore = asyncio.Semaphore(CONCURRENT_PAGES)
    
    async with async_playwright() as p:
        # Cháº¡y trÃ¬nh duyá»‡t Chromium
        browser = await p.chromium.launch(headless=True, args=['--disable-blink-features=AutomationControlled'])
        
        # Táº¡o Context giáº£ láº­p trÃ¬nh duyá»‡t tháº­t
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            extra_http_headers={'Accept-Language': 'en-US,en;q=0.9'}
        )

        with open(LOG_FILE, 'a') as log_h:
            # Chia nhá» dá»¯ liá»‡u thÃ nh tá»«ng Batch Ä‘á»ƒ lÆ°u CSV liÃªn tá»¥c
            for i in range(0, total_todo, CONCURRENT_PAGES):
                batch = df_to_do.iloc[i : i + CONCURRENT_PAGES]
                
                # Táº¡o danh sÃ¡ch cÃ¡c task cháº¡y song song
                tasks = [
                    extract_task(row[url_col], row[label_col], context, semaphore, log_h) 
                    for _, row in batch.iterrows()
                ]
                
                # Chá» cáº£ batch hoÃ n thÃ nh
                results = await asyncio.gather(*tasks)
                
                # LÆ°u vÃ o file CSV ngay láº­p tá»©c
                output_df = pd.DataFrame(results)
                output_df.to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
                
                # TÃ­nh tá»‘c Ä‘á»™ & Dá»± bÃ¡o
                done = i + len(batch)
                elapsed = time.time() - start_time
                speed = done / elapsed if elapsed > 0 else 0
                eta = str(timedelta(seconds=int((total_todo - done) / speed))) if speed > 0 else "N/A"
                
                print(f"âœ… ÄÃ£ xong {done}/{total_todo} | Tá»‘c Ä‘á»™: {speed:.2f} URL/s | Æ¯á»›c tÃ­nh xong sau: {eta}")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
