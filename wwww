import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import requests
import io
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# ================= CONFIGURATION =================
URLHAUS_URL = "https://urlhaus.abuse.ch/downloads/csv_online/"
OUTPUT_FILE = "Phishing_URLHaus_24F.csv"
NUM_PROCESSES = 6       # Sá»‘ nhÃ¢n CPU trÃªn Linux cá»§a Ã´ng
LIMIT_URLS = 15000      # Sá»‘ lÆ°á»£ng URL muá»‘n trÃ­ch xuáº¥t
TIMEOUT_MS = 35000      # Thá»i gian chá» má»—i trang

# Danh sÃ¡ch 24 cá»™t chuáº©n Ä‘á»ƒ gá»™p data sau nÃ y
COLUMNS = [
    'URL', 'NoOfDegitsInURL', 'IsHTTPS', 'DomainTitleMatchScore', 'HasDescription', 
    'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasPasswordField', 
    'HasCopyrightInfo', 'label', 'V1_PHash_Distance', 'V2_Layout_Similarity', 
    'V6_JS_Entropy', 'V7_Text_Readability_Score', 'V8_Total_IFrames', 
    'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count', 
    'Is_Top_1M_Domain', 'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain', 
    'Trust_Score', 'Digit_Ratio'
]

# ================= LOGIC TRÃCH XUáº¤T =================
def calculate_entropy(text):
    if not text or len(text) == 0: return 0
    prob = [float(text.count(c)) / len(text) for c in dict.fromkeys(list(text))]
    return -sum(p * math.log(p, 2) for p in prob)

async def get_features(page, url):
    try:
        await page.goto(url, timeout=TIMEOUT_MS, wait_until="domcontentloaded")
        await asyncio.sleep(1.5)
        
        content = (await page.content()).lower()
        title = (await page.title()) or ""
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        visible_text = await page.inner_text("body")

        # Features
        digits = sum(c.isdigit() for c in url)
        is_https = 1 if url.startswith('https') else 0
        domain_match = 1 if domain.split('.')[0] in title.lower() else 0
        
        # Form Submit External logic
        ext_form = 0
        forms = await page.query_selector_all('form')
        for form in forms:
            action = await form.get_attribute('action')
            if action and action.startswith('http') and domain not in action:
                ext_form = 1; break

        return {
            'URL': url, 'NoOfDegitsInURL': digits, 'IsHTTPS': is_https,
            'DomainTitleMatchScore': domain_match,
            'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
            'HasExternalFormSubmit': ext_form,
            'HasSocialNet': 1 if any(s in content for s in ['facebook.com', 'twitter.com', 'instagram.com']) else 0,
            'HasSubmitButton': 1 if await page.query_selector('input[type="submit"], button[type="submit"]') else 0,
            'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
            'HasCopyrightInfo': 1 if 'copyright' in content or 'Â©' in content else 0,
            'label': 1, 'V1_PHash_Distance': 0, 'V2_Layout_Similarity': 0,
            'V6_JS_Entropy': calculate_entropy(content),
            'V7_Text_Readability_Score': len(visible_text.split()) / 400,
            'V8_Total_IFrames': len(await page.query_selector_all('iframe')),
            'V9_Has_Hidden_IFrame': 1 if await page.query_selector('iframe[style*="display:none"]') else 0,
            'V5_TLS_Issuer_Reputation': is_https, 'V4_DNS_Volatility_Count': 0,
            'Is_Top_1M_Domain': 0, 'V22_IP_Subdomain_Pattern': 1 if re.match(r'\d+\.\d+\.\d+', domain) else 0,
            'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
            'Trust_Score': domain_match * is_https, 'Digit_Ratio': digits / len(url) if len(url) > 0 else 0
        }
    except: return None

async def worker(urls, proc_id):
    results = []
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox'])
        context = await browser.new_context(user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36")
        for i, url in enumerate(urls):
            page = await context.new_page()
            data = await get_features(page, url)
            if data:
                results.append(data)
                print(f"[P{proc_id}] âœ… ({i+1}/{len(urls)})")
            else:
                print(f"[P{proc_id}] âŒ ({i+1}/{len(urls)})")
            await page.close()
        await browser.close()
    return results

def start_process(urls, proc_id):
    return asyncio.run(worker(urls, proc_id))

if __name__ == "__main__":
    # 1. Táº£i vÃ  xá»­ lÃ½ URLHaus CSV
    print("ğŸ“¥ Äang káº¿t ná»‘i URLHaus...")
    res = requests.get(URLHAUS_URL, timeout=30)
    # URLHaus dÃ¹ng dáº¥u # cho comment, ta skip 8 dÃ²ng Ä‘áº§u hoáº·c dÃ¹ng comment='#'
    df_raw = pd.read_csv(io.StringIO(res.text), comment='#', 
                         names=['id', 'dateadded', 'url', 'url_status', 'last_online', 'threat', 'tags', 'urlhaus_link', 'reporter'],
                         quoting=3)
    
    # Lá»c cÃ¡c URL Ä‘ang Online Ä‘á»ƒ Ä‘áº£m báº£o láº¥y Ä‘Æ°á»£c Ä‘áº·c trÆ°ng Ä‘á»™ng
    phish_urls = df_raw[df_raw['url_status'] == 'online']['url'].head(LIMIT_URLS).tolist()
    print(f"ğŸ“¦ ÄÃ£ tÃ¬m tháº¥y {len(phish_urls)} URL Ä‘ang hoáº¡t Ä‘á»™ng.")

    # 2. Äa luá»“ng trÃ­ch xuáº¥t
    chunks = np.array_split(phish_urls, NUM_PROCESSES)
    final_data = []
    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
        futures = [executor.submit(start_process, chunks[i].tolist(), i) for i in range(NUM_PROCESSES)]
        for f in futures:
            final_data.extend(f.result())

    # 3. LÆ°u káº¿t quáº£ chuáº©n hÃ³a
    df_final = pd.DataFrame(final_data)
    if not df_final.empty:
        df_final = df_final[COLUMNS]
        df_final.to_csv(OUTPUT_FILE, index=False)
        print(f"ğŸ HoÃ n thÃ nh! File lÆ°u táº¡i: {OUTPUT_FILE}")
