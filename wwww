import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from playwright_stealth import stealth # D√πng stealth theo y√™u c·∫ßu c·ªßa √¥ng
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"
SCREENSHOT_DIR = "debug_screenshots"

NUM_PROCESSES = 2         
PAGES_PER_PROCESS = 2     
TIMEOUT = 80000           

if not os.path.exists(SCREENSHOT_DIR):
    os.makedirs(SCREENSHOT_DIR)

# --- C√°c h√†m static gi·ªØ nguy√™n nh∆∞ c≈© ---
def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        return {
            'domainEntropy': calculate_entropy(domain),
            'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
            'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
            'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
            'domainLength': len(domain),
            'Subdomain_Level': domain.count('.'),
            'IsHTTPS': 1 if url.startswith('https') else 0,
            'URL_Depth': url.count('/') - 2
        }
    except: return {f: 0 for f in range(8)}

# =========================================================
# 3. H√ÄM X·ª¨ L√ù CH√çNH (C√ì LOG FLUSH & STEALTH)
# =========================================================
async def process_url(url, label, context, semaphore, log_file):
    static_data = extract_static(url)
    dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                    'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 
                                    'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}
    status = "FAILED"
    reason = "INIT"

    async with semaphore:
        page = await context.new_page()
        # √Åp d·ª•ng stealth cho b·∫£n Kali
        await stealth(page) 
        
        try:
            await page.set_extra_http_headers({
                "Referer": "https://www.google.com/",
                "Accept-Language": "en-US,en;q=0.9"
            })

            # Truy c·∫≠p
            response = await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
            
            # ƒê·ª£i CF gi·∫£i m√£
            await asyncio.sleep(random.uniform(5, 10))
            
            content = (await page.content()).lower()
            title = (await page.title()) or "NO_TITLE"

            if "just a moment" in title.lower() or "checking your browser" in content:
                reason = "K·∫πt ·ªü Cloudflare Challenge"
            elif "access denied" in content:
                reason = "Cloudflare Blocked IP"
            else:
                # TR√çCH XU·∫§T TH√ÄNH C√îNG
                domain = urlparse(url).netloc.lower()
                
                # S·ª≠a l·ªói frames
                v9_val = 0
                frames = page.frames
                for f in frames[1:]:
                    try:
                        el = await f.frame_element()
                        if el and not await el.is_visible():
                            v9_val = 1; break
                    except: continue

                dynamic_data.update({
                    'V9_Has_Hidden_IFrame': v9_val,
                    'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                    'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
                    # ... th√™m c√°c feature kh√°c t∆∞∆°ng t·ª± ...
                })
                status = "SUCCESS"
                reason = f"V√†o ƒë∆∞·ª£c trang - Title: {title[:20]}"

        except Exception as e:
            reason = f"L·ªói: {str(e).splitlines()[0][:50]}"
            # Ch·ª•p ·∫£nh l·ªói
            try:
                clean_name = re.sub(r'\W+', '_', url)[:20]
                await page.screenshot(path=f"{SCREENSHOT_DIR}/{clean_name}.png")
            except: pass
        finally:
            await page.close()
            
    # GHI LOG NGAY L·∫¨P T·ª®C (Flush)
    log_line = f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {reason}\n"
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(log_line)
        f.flush() # √âp ghi xu·ªëng ƒëƒ©a ngay
    
    return {**static_data, **dynamic_data, 'URL': url, 'label': label}

# =========================================================
# 4. CH·∫†Y ƒêA LU·ªíNG & L∆ØU BATCH
# =========================================================
async def run_batch(df_batch, process_id):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False, args=[
            '--no-sandbox', '--disable-setuid-sandbox', '--disable-blink-features=AutomationControlled'
        ])
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0")
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        
        tasks = [process_url(row['URL'], row['label'], context, semaphore, LOG_FILE) for _, row in df_batch.iterrows()]
        results = await asyncio.gather(*tasks)
        await browser.close()
        return results

def process_entry(df_batch, process_id):
    try: return asyncio.run(run_batch(df_batch, process_id))
    except: return []

if __name__ == "__main__":
    if not os.path.exists(INPUT_FILE): exit()

    df_all = pd.read_csv(INPUT_FILE)
    
    # Checkpoint: Ch·ªâ l·∫•y nh·ªØng URL ch∆∞a c√≥ trong Log
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total = len(df_todo)
    print(f"üöÄ C·∫ßn x·ª≠ l√Ω: {total} URL. Theo d√µi log b·∫±ng l·ªánh: tail -f {LOG_FILE}")

    # Chia batch nh·ªè ƒë·ªÉ l∆∞u file li√™n t·ª•c
    batch_size = NUM_PROCESSES * 5 
    for i in range(0, total, batch_size):
        current_batch = df_todo.iloc[i : i + batch_size]
        sub_batches = [b for b in np.array_split(current_batch, NUM_PROCESSES) if not b.empty]
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            futures = [executor.submit(process_entry, sub_batches[p], p) for p in range(len(sub_batches))]
            batch_results = []
            for fut in futures:
                batch_results.extend(fut.result())
        
        # L∆∞u v√†o CSV ngay sau m·ªói batch
        if batch_results:
            df_save = pd.DataFrame(batch_results)
            df_save.to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
            print(f"‚úÖ ƒê√£ l∆∞u checkpoint {i + len(current_batch)}/{total}")
