import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"
SCREENSHOT_DIR = "debug_screenshots"

NUM_PROCESSES = 2         # Gi·∫£m xu·ªëng 2 ƒë·ªÉ Kali ch·∫°y m∆∞·ª£t, tr√°nh l·ªói memory
PAGES_PER_PROCESS = 2     
TIMEOUT = 70000           

if not os.path.exists(SCREENSHOT_DIR):
    os.makedirs(SCREENSHOT_DIR)

# =========================================================
# 2. H√ÄM TR√çCH XU·∫§T & PH√ÇN T√çCH L·ªñI
# =========================================================

def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    return {
        'domainEntropy': calculate_entropy(domain),
        'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
        'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
        'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
        'domainLength': len(domain),
        'Subdomain_Level': domain.count('.'),
        'IsHTTPS': 1 if url.startswith('https') else 0
    }

async def process_url(url, label, context, semaphore, log_file):
    static_data = extract_static(url)
    dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                    'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame']}
    status = "FAILED"
    reason = "UNKNOWN"

    async with semaphore:
        page = await context.new_page()
        # Fake WebDriver (C∆° b·∫£n)
        await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # 1. Th·ª≠ truy c·∫≠p
            response = await page.goto(url, timeout=TIMEOUT, wait_until="load") # ƒê·ªïi sang 'load' cho b·ªÅn
            
            # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ c√°c script ch·ªëng bot ch·∫°y xong
            await asyncio.sleep(random.uniform(3, 5))
            
            content = (await page.content()).lower()
            title = (await page.title()) or "No Title"

            # PH√ÇN T√çCH CH·∫∂N
            if response.status in [403, 429]:
                reason = f"Blocked: HTTP {response.status}"
                if "cloudflare" in content: reason = "Cloudflare WAF"
            elif "just a moment" in title.lower():
                reason = "Cloudflare Challenge"
            else:
                # TR√çCH XU·∫§T KHI TH√ÄNH C√îNG
                domain = urlparse(url).netloc.lower()
                v9 = 1 if any(not await f.frame_element().is_visible() for f in page.frames[1:] if f.frame_element()) else 0
                links = await page.query_selector_all('a')
                ext_l = sum(1 for l in links if 'http' in (await l.get_attribute('href') or '') and domain not in (await l.get_attribute('href') or ''))
                
                dynamic_data.update({
                    'Outlink_Ratio': ext_l / len(links) if links else 0,
                    'V9_Has_Hidden_IFrame': v9,
                    'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
                    'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                })
                status = "SUCCESS"
                reason = f"Title: {title[:15]}"

        except Exception as e:
            reason = str(e).split('\n')[0][:40]
            # CH·ª§P ·∫¢NH KHI L·ªñI (Quan tr·ªçng nh·∫•t ƒë·ªÉ debug)
            clean_url = re.sub(r'\W+', '_', url)[:30]
            await page.screenshot(path=f"{SCREENSHOT_DIR}/{clean_url}.png")
        finally:
            await page.close()
            
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {reason}\n")
    
    return {**static_data, **dynamic_data, 'URL': url, 'label': label}

# =========================================================
# 3. RUNNER (C·∫•u h√¨nh "B·∫•t t·ª≠" cho Linux)
# =========================================================

async def run_batch(df_batch, process_id):
    async with async_playwright() as p:
        # √âP CHROME CH·∫†Y TR√äN KALI (S·ª¨A L·ªñI FAILED H·∫æT)
        browser = await p.chromium.launch(headless=False, args=[
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-blink-features=AutomationControlled',
            '--disable-gpu'
        ])
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0")
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        
        tasks = [process_url(row['URL'], row['label'], context, semaphore, LOG_FILE) for _, row in df_batch.iterrows()]
        results = await asyncio.gather(*tasks)
        
        await browser.close()
        return results

def process_entry(df_batch, process_id):
    return asyncio.run(run_batch(df_batch, process_id))

# =========================================================
# 4. EXECUTION
# =========================================================

if __name__ == "__main__":
    start_time = time.time()
    if not os.path.exists(INPUT_FILE): exit()

    df_all = pd.read_csv(INPUT_FILE)
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total = len(df_todo)
    print(f"üöÄ Ch·∫°y {total} URL. Ki·ªÉm tra screenshot trong folder '{SCREENSHOT_DIR}' n·∫øu l·ªói.")

    batch_size = NUM_PROCESSES * 5
    for i in range(0, total, batch_size):
        current_batch = df_todo.iloc[i : i + batch_size]
        sub_batches = np.array_split(current_batch, NUM_PROCESSES)
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            futures = [executor.submit(process_entry, sub_batches[p], p) for p in range(len(sub_batches))]
            batch_results = []
            for fut in futures:
                batch_results.extend(fut.result())
        
        pd.DataFrame(batch_results).to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
        
        # ETA
        done = i + len(current_batch)
        elapsed = time.time() - start_time
        speed = done / elapsed
        print(f"üèÅ {done}/{total} | T·ªëc ƒë·ªô: {speed:.2f} URL/s | ETA: {timedelta(seconds=int((total-done)/speed))}")
