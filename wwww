import pandas as pd
import numpy as np
import os, time, math, io, socket, re, tldextract, ssl, threading
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright
import imagehash
from PIL import Image
from bs4 import BeautifulSoup
from collections import Counter

# =================================================================
# I. C·∫§U H√åNH SI√äU T·ªêC ƒê·ªò & AN TO√ÄN RAM (D√†nh cho m√°y 32GB)
# =================================================================
RAW_CSV_FILE = 'PhiUSIIL_Phishing_URL_Dataset.csv'
TEMP_LOG_FILE = 'extraction_checkpoint.csv'
FINAL_OUTPUT = 'PhiUSIIL_Extracted_Full.csv'

# C·∫•u h√¨nh "V√πng Xanh" cho RAM 93%
MAX_WORKERS = 25   # S·ªë lu·ªìng song song (t·∫≠n d·ª•ng 12 CPU)
CHUNK_SIZE = 5     # C·ª© xong 5 URL ƒë√≥ng tr√¨nh duy·ªát 1 l·∫ßn ƒë·ªÉ x·∫£ RAM s·∫°ch s·∫Ω
TIMEOUT_MS = 8000  # 8 gi√¢y (ƒë·ªß ƒë·ªÉ l·∫•y DOM/Script m√† kh√¥ng t·ªën th·ªùi gian ch·ªù web ch·∫øt)
TARGET_PHASH = imagehash.hex_to_hash('9880e61f1c7e0c4f')

csv_lock = threading.Lock()

# C√°c c·ªôt c·∫ßn gi·ªØ l·∫°i t·ª´ file g·ªëc
OLD_KEEP_COLS = [
    'URL', 'NoOfDegitsInURL', 'HasDescription', 'HasSocialNet', 'HasPasswordField', 
    'HasSubmitButton', 'HasExternalFormSubmit', 'DomainTitleMatchScore', 
    'IsHTTPS', 'HasCopyrightInfo', 'label'
]

# =================================================================
# II. LOGIC TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG (GI·ªÆ NGUY√äN 100%)
# =================================================================
def get_entropy(text):
    if not text or len(text) == 0: return 0.0
    probs = [count/len(text) for count in Counter(text).values()]
    return -sum(p * math.log2(p) for p in probs) / 8.0

def get_tls_issuer(hostname):
    try:
        context = ssl.create_default_context()
        context.check_hostname, context.verify_mode = False, ssl.CERT_NONE
        with socket.create_connection((hostname, 443), timeout=3) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert = ssock.getpeercert()
                issuer = dict(x[0] for x in cert['issuer'])
                return issuer.get('organizationName', 'Unknown')
    except: return 'None'

def extract_full_features(page, url):
    res = {k: 0.0 for k in [
        'V10_HTTP_Extraction_Success', 'V11_WHOIS_Extraction_Success', 'V1_PHash_Distance', 
        'V2_Layout_Similarity', 'V6_JS_Entropy', 'V7_Text_Readability_Score', 
        'V8_Total_IFrames', 'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 
        'V3_Domain_Age_Days', 'V4_DNS_Volatility_Count', 'Is_Top_1M_Domain', 
        'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain'
    ]}

    try:
        # Ch·∫∑n Request r√°c ƒë·ªÉ nh·∫π m√°y nh∆∞ng gi·ªØ Script/HTML
        page.route("**/*", lambda route: route.abort() if route.request.resource_type in ["image", "media", "font", "websocket"] else route.continue_())

        # 1. ƒê·∫∑c tr∆∞ng URL & DNS
        ext = tldextract.extract(url)
        domain = f"{ext.domain}.{ext.suffix}"
        res['V22_IP_Subdomain_Pattern'] = 1 if re.search(r'\d+\.\d+', ext.subdomain) else 0
        res['V23_Entropy_Subdomain'] = get_entropy(ext.subdomain)
        res['Is_Top_1M_Domain'] = 1 if ext.domain in ['google', 'facebook', 'microsoft', 'apple', 'amazon'] else 0
        
        try: res['V4_DNS_Volatility_Count'] = len(socket.gethostbyname_ex(domain)[2])
        except: pass
        res['V5_TLS_Issuer_Reputation'] = 1.0 if get_tls_issuer(domain) != 'None' else 0.0

        # 2. T·∫£i trang v√† tr√≠ch xu·∫•t DOM
        response = page.goto(url, timeout=TIMEOUT_MS, wait_until="domcontentloaded")
        if response and response.status < 400:
            res['V10_HTTP_Extraction_Success'] = 1
            
            # Feature: pHash (B·ªë c·ª•c)
            try:
                img_bytes = page.screenshot(timeout=2500)
                img = Image.open(io.BytesIO(img_bytes)).convert('L')
                res['V1_PHash_Distance'] = (imagehash.phash(img) - TARGET_PHASH) / 64.0
            except: res['V1_PHash_Distance'] = 0.5

            # Feature: HTML Analysis
            content = page.content()
            soup = BeautifulSoup(content, 'lxml') 
            text_content = soup.get_text().strip()
            
            depths = [len(list(t.parents)) for t in soup.find_all(True)]
            res['V2_Layout_Similarity'] = np.clip(1.0 - (max(depths or [0])/50.0), 0, 1)
            res['V6_JS_Entropy'] = get_entropy("".join([s.text for s in soup.find_all('script')]))
            
            words, sents = text_content.split(), re.split(r'[.!?]+', text_content)
            res['V7_Text_Readability_Score'] = np.clip(len(words)/(len(sents) or 1) / 25.0, 0, 1)
            
            iframes = soup.find_all('iframe')
            res['V8_Total_IFrames'] = len(iframes)
            res['V9_Has_Hidden_IFrame'] = 1 if any('none' in str(f.get('style','')).lower() for f in iframes) else 0
            res['V11_WHOIS_Extraction_Success'] = 1

    except:
        res['V1_PHash_Distance'] = 0.5 
    return res

# =================================================================
# III. V·∫¨N H√ÄNH ƒêA LU·ªíNG (THREAD WORKER)
# =================================================================
def thread_worker(chunk_df):
    results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=[
            "--no-sandbox", "--disable-gpu", "--disable-dev-shm-usage",
            "--single-process", "--disable-extensions", "--no-zygote",
            "--js-flags='--max-old-space-size=128'" # √âp RAM c·ª±c th·∫•p cho m·ªói tab
        ])
        context = browser.new_context(
            user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            viewport={'width': 800, 'height': 600}
        )
        
        for _, row in chunk_df.iterrows():
            page = None
            try:
                page = context.new_page()
                data = extract_full_features(page, row['URL'])
                data['URL_KEY'] = str(row['URL'])
                results.append(data)
            except: pass
            finally:
                if page: page.close()
            
        browser.close()
    return results

# =================================================================
# IV. CH∆Ø∆†NG TR√åNH CH√çNH
# =================================================================
def main():
    start_time = time.time()
    
    # 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
    df_raw = pd.read_csv(RAW_CSV_FILE, usecols=OLD_KEEP_COLS)
    
    # 2. Ki·ªÉm tra ti·∫øn ƒë·ªô t·ª´ checkpoint
    processed_urls = set()
    if os.path.exists(TEMP_LOG_FILE):
        try:
            check_df = pd.read_csv(TEMP_LOG_FILE, usecols=['URL_KEY'])
            processed_urls = set(check_df['URL_KEY'].astype(str))
            print(f"üîÑ ƒê√£ t√¨m th·∫•y {len(processed_urls)} URL c≈©. ƒêang ch·∫°y ti·∫øp...")
        except: pass

    df_todo = df_raw[~df_raw['URL'].astype(str).isin(processed_urls)]
    total_todo = len(df_todo)
    
    if total_todo == 0:
        print("‚úÖ M·ªçi d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω xong!"); return

    print(f"üöÄ KH·ªûI CH·∫†Y {MAX_WORKERS} LU·ªíNG | C√≤n l·∫°i: {total_todo} URL")
    
    # 3. Chia nh·ªè c√¥ng vi·ªác
    chunks = [df_todo[i:i + CHUNK_SIZE] for i in range(0, total_todo, CHUNK_SIZE)]
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(thread_worker, c): c for c in chunks}
        for i, future in enumerate(as_completed(futures), 1):
            try:
                batch = future.result()
                if batch:
                    with csv_lock:
                        pd.DataFrame(batch).to_csv(TEMP_LOG_FILE, mode='a', header=not os.path.exists(TEMP_LOG_FILE), index=False)
                
                # C·ª© 20 l∆∞·ª£t d·ªçn d·∫πp b·ªô nh·ªõ ƒë·ªám Linux 1 l·∫ßn
                if i % 20 == 0: 
                    os.system('sync; echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null')

                done = min(i * CHUNK_SIZE, total_todo)
                elapsed = time.time() - start_time
                speed = done / elapsed if elapsed > 0 else 0
                rem_sec = (total_todo - done) / speed if speed > 0 else 0
                
                print(f"‚ûú [{datetime.now().strftime('%H:%M:%S')}] {len(processed_urls)+done}/{len(df_raw)} "
                      f"| {speed:.2f} URL/s | C√≤n: {str(timedelta(seconds=int(rem_sec)))}")
            except Exception as e:
                print(f"‚ùå L·ªói batch: {e}")

    # 4. G·ªôp d·ªØ li·ªáu cu·ªëi c√πng
    print("\nüîÑ ƒêang t·ªïng h·ª£p file k·∫øt qu·∫£ cu·ªëi c√πng...")
    df_new = pd.read_csv(TEMP_LOG_FILE).drop_duplicates('URL_KEY')
    df_final = pd.merge(df_raw, df_new, left_on='URL', right_on='URL_KEY', how='inner')
    df_final.drop(columns=['URL_KEY']).to_csv(FINAL_OUTPUT, index=False)
    print(f"‚úÖ XONG! D·ªØ li·ªáu l∆∞u t·∫°i: {FINAL_OUTPUT}")

if __name__ == "__main__":
    main()
