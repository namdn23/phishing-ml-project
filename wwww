import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. Cáº¤U HÃŒNH Há»† THá»NG
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"
SCREENSHOT_DIR = "debug_screenshots"

NUM_PROCESSES = 2         # Kali 8GB nÃªn Ä‘á»ƒ 2 Ä‘á»ƒ trÃ¡nh crash Chrome
PAGES_PER_PROCESS = 2     # Má»—i cá»­a sá»• má»Ÿ 2 tab
TIMEOUT = 80000           # 80s Ä‘á»ƒ Ä‘á»£i Cloudflare giáº£i mÃ£ JS

if not os.path.exists(SCREENSHOT_DIR):
    os.makedirs(SCREENSHOT_DIR)

# =========================================================
# 2. HÃ€M TRÃCH XUáº¤T STATIC (8 Features)
# =========================================================
def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        subdomain = domain.split('.')[0]
        return {
            'domainEntropy': calculate_entropy(domain),
            'V23_Entropy_Subdomain': calculate_entropy(subdomain),
            'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
            'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
            'domainLength': len(domain),
            'Subdomain_Level': domain.count('.'),
            'IsHTTPS': 1 if url.startswith('https') else 0,
            'URL_Depth': url.count('/') - 2
        }
    except:
        return {f: 0 for f in ['domainEntropy', 'V23_Entropy_Subdomain', 'hasIp', 'numHypRatio', 'domainLength', 'Subdomain_Level', 'IsHTTPS', 'URL_Depth']}

# =========================================================
# 3. HÃ€M TRÃCH XUáº¤T Äá»˜NG & VÆ¯á»¢T CHáº¶N (10 Features)
# =========================================================
async def process_url(url, label, context, semaphore, log_file):
    static_data = extract_static(url)
    # Khá»Ÿi táº¡o giÃ¡ trá»‹ máº·c Ä‘á»‹nh (0.5 Ä‘á»ƒ trung láº­p náº¿u failed)
    dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                    'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 
                                    'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}
    status = "FAILED"
    reason = "UNKNOWN"

    async with semaphore:
        page = await context.new_page()
        # KÃ­ch hoáº¡t tÃ ng hÃ¬nh (Bypass navigator.webdriver, WebGL, etc.)
        await stealth_async(page)
        
        try:
            # Bypass CF báº±ng cÃ¡ch giáº£ láº­p traffic tá»« máº¡ng xÃ£ há»™i
            await page.set_extra_http_headers({
                "Referer": "https://www.google.com/",
                "Accept-Language": "en-US,en;q=0.9"
            })

            # Truy cáº­p trang
            response = await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
            
            # --- CÆ  CHáº¾ Tá»° Äá»˜NG VÆ¯á»¢T CLOUDFLARE ---
            await asyncio.sleep(random.uniform(5, 8)) # Äá»£i giáº£i mÃ£ JS
            await page.mouse.move(random.randint(100, 500), random.randint(100, 500)) # Di chuyá»ƒn chuá»™t
            await page.mouse.wheel(0, 500) # Cuá»™n trang

            content = (await page.content()).lower()
            title = (await page.title()) or ""
            
            # Kiá»ƒm tra náº¿u váº«n káº¹t á»Ÿ trang chá»
            if "just a moment" in title.lower() or "checking your browser" in content:
                await asyncio.sleep(10) # KiÃªn nháº«n Ä‘á»£i thÃªm 10s
                title = await page.title()

            if "access denied" in content:
                reason = "Cloudflare IP Blocked"
            else:
                # --- TRÃCH XUáº¤T Dá»® LIá»†U ---
                domain = urlparse(url).netloc.lower()
                
                # Sá»­a lá»—i 'async_generator' báº±ng cÃ¡ch duyá»‡t list trá»±c tiáº¿p
                v9_val = 0
                all_frames = page.frames
                for f in all_frames[1:]:
                    try:
                        f_el = await f.frame_element()
                        if f_el and not await f_el.is_visible():
                            v9_val = 1; break
                    except: continue

                links = await page.query_selector_all('a')
                ext_l = 0
                for l in links[:50]: # Giá»›i háº¡n check 50 link Ä‘á»ƒ trÃ¡nh treo
                    try:
                        h = await l.get_attribute('href')
                        if h and 'http' in h and domain not in h: ext_l += 1
                    except: continue

                dynamic_data.update({
                    'Outlink_Ratio': ext_l / len(links) if links else 0,
                    'HasExternalFormSubmit': 1 if await page.query_selector('form[action^="http"]') else 0,
                    'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                    'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
                    'HasSocialNet': 1 if any(s in content for s in ['facebook', 'twitter', 'instagram', 'linkedin']) else 0,
                    'HasCopyrightInfo': 1 if 'Â©' in content or 'copyright' in content else 0,
                    'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
                    'V9_Has_Hidden_IFrame': v9_val,
                    'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0,
                    'V4_DNS_Volatility_Count': 0 # Äáº·c trÆ°ng nÃ y cáº§n tÃ¡ch thread DNS riÃªng
                })
                status = "SUCCESS"
                reason = f"T: {title[:15]}"

        except Exception as e:
            reason = str(e).split('\n')[0][:40]
            # Chá»¥p áº£nh lá»—i Ä‘á»ƒ check Cloudflare/Captcha
            clean_name = re.sub(r'\W+', '_', url)[:25]
            await page.screenshot(path=f"{SCREENSHOT_DIR}/{clean_name}.png")
        finally:
            await page.close()
            
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {reason}\n")
    
    return {**static_data, **dynamic_data, 'URL': url, 'label': label}

# =========================================================
# 4. QUáº¢N LÃ ÄA LUá»’NG & TRÃŒNH DUYá»†T
# =========================================================
async def run_batch(df_batch, process_id):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False, args=[
            '--no-sandbox', 
            '--disable-setuid-sandbox',
            '--disable-blink-features=AutomationControlled'
        ])
        context = await browser.new_context(
            viewport={'width': 1280, 'height': 800},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        tasks = [process_url(row['URL'], row['label'], context, semaphore, LOG_FILE) for _, row in df_batch.iterrows()]
        results = await asyncio.gather(*tasks)
        await browser.close()
        return results

def process_entry(df_batch, process_id):
    try:
        return asyncio.run(run_batch(df_batch, process_id))
    except: return []

if __name__ == "__main__":
    start_time = time.time()
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ KhÃ´ng tháº¥y file {INPUT_FILE}"); exit()

    df_all = pd.read_csv(INPUT_FILE)
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total = len(df_todo)
    print(f"ðŸš€ Báº¯t Ä‘áº§u cÃ o {total} URL vá»›i {NUM_PROCESSES} luá»“ng tÃ ng hÃ¬nh...")

    batch_size = NUM_PROCESSES * 10
    for i in range(0, total, batch_size):
        current_batch = df_todo.iloc[i : i + batch_size]
        sub_batches = [b for b in np.array_split(current_batch, NUM_PROCESSES) if not b.empty]
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            futures = [executor.submit(process_entry, sub_batches[p], p) for p in range(len(sub_batches))]
            batch_results = []
            for fut in futures: batch_results.extend(fut.result())
        
        if batch_results:
            df_res = pd.DataFrame(batch_results)
            df_res.to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
            
            done = i + len(current_batch)
            speed = done / (time.time() - start_time)
            eta = str(timedelta(seconds=int((total - done) / speed))) if speed > 0 else "N/A"
            print(f"âœ… Xong {done}/{total} | Tá»‘c Ä‘á»™: {speed:.2f} URL/s | ETA: {eta}")
