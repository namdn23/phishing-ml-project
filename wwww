import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import requests
import io
import csv
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# ================= CONFIGURATION =================
URLHAUS_URL = "https://urlhaus.abuse.ch/downloads/csv_online/"
OUTPUT_FILE = "Phishing_URLHaus_24F.csv"
NUM_PROCESSES = 6       
LIMIT_URLS = 15000      
TIMEOUT_MS = 35000      
SAVE_INTERVAL = 50  # C·ª© 50 m·∫´u th√¨ l∆∞u checkpoint m·ªôt l·∫ßn

COLUMNS = [
    'URL', 'NoOfDegitsInURL', 'IsHTTPS', 'DomainTitleMatchScore', 'HasDescription', 
    'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasPasswordField', 
    'HasCopyrightInfo', 'label', 'V1_PHash_Distance', 'V2_Layout_Similarity', 
    'V6_JS_Entropy', 'V7_Text_Readability_Score', 'V8_Total_IFrames', 
    'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count', 
    'Is_Top_1M_Domain', 'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain', 
    'Trust_Score', 'Digit_Ratio'
]

# ================= LOGIC TR√çCH XU·∫§T =================
def calculate_entropy(text):
    if not text or len(text) == 0: return 0
    prob = [float(text.count(c)) / len(text) for c in dict.fromkeys(list(text))]
    return -sum(p * math.log(p, 2) for p in prob)

async def get_features(page, url):
    try:
        await page.goto(url, timeout=TIMEOUT_MS, wait_until="domcontentloaded")
        await asyncio.sleep(1.5)
        content = (await page.content()).lower()
        title = (await page.title()) or ""
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        visible_text = await page.inner_text("body")

        digits = sum(c.isdigit() for c in url)
        is_https = 1 if url.startswith('https') else 0
        
        ext_form = 0
        forms = await page.query_selector_all('form')
        for form in forms:
            action = await form.get_attribute('action')
            if action and action.startswith('http') and domain not in action:
                ext_form = 1; break

        return {
            'URL': url, 'NoOfDegitsInURL': digits, 'IsHTTPS': is_https,
            'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
            'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
            'HasExternalFormSubmit': ext_form,
            'HasSocialNet': 1 if any(s in content for s in ['facebook.com', 'twitter.com', 'instagram.com']) else 0,
            'HasSubmitButton': 1 if await page.query_selector('input[type="submit"], button[type="submit"]') else 0,
            'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
            'HasCopyrightInfo': 1 if 'copyright' in content or '¬©' in content else 0,
            'label': 1, 'V1_PHash_Distance': 0, 'V2_Layout_Similarity': 0,
            'V6_JS_Entropy': calculate_entropy(content),
            'V7_Text_Readability_Score': len(visible_text.split()) / 400,
            'V8_Total_IFrames': len(await page.query_selector_all('iframe')),
            'V9_Has_Hidden_IFrame': 1 if await page.query_selector('iframe[style*="display:none"]') else 0,
            'V5_TLS_Issuer_Reputation': is_https, 'V4_DNS_Volatility_Count': 0,
            'Is_Top_1M_Domain': 0, 'V22_IP_Subdomain_Pattern': 1 if re.match(r'\d+\.\d+\.\d+', domain) else 0,
            'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
            'Trust_Score': is_https, 'Digit_Ratio': digits / len(url) if len(url) > 0 else 0
        }
    except: return None

async def worker(urls, proc_id):
    results = []
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox'])
        context = await browser.new_context(user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36")
        
        for i, url in enumerate(urls):
            page = await context.new_page()
            data = await get_features(page, url)
            if data:
                results.append(data)
                # Checkpoint n·ªôi b·ªô m·ªói worker
                if len(results) >= SAVE_INTERVAL:
                    pd.DataFrame(results).to_csv(OUTPUT_FILE, mode='a', header=not os.path.exists(OUTPUT_FILE), index=False)
                    results = []
                print(f"‚úÖ [P{proc_id}] Progress: {i+1}/{len(urls)} | Found: {url[:30]}")
            else:
                print(f"‚ùå [P{proc_id}] Fail: {url[:30]}")
            await page.close()
        
        await browser.close()
    return results

def start_process(urls, proc_id):
    return asyncio.run(worker(urls, proc_id))

if __name__ == "__main__":
    # 1. Ki·ªÉm tra Checkpoint c≈©
    processed_urls = set()
    if os.path.exists(OUTPUT_FILE):
        try:
            old_df = pd.read_csv(OUTPUT_FILE)
            processed_urls = set(old_df['URL'].tolist())
            print(f"üîÑ ƒê√£ t√¨m th·∫•y file c≈©. B·ªè qua {len(processed_urls)} URL ƒë√£ tr√≠ch xu·∫•t.")
        except: pass

    # 2. T·∫£i URL m·ªõi
    print("üì• ƒêang t·∫£i URL t·ª´ URLHaus...")
    res = requests.get(URLHAUS_URL, timeout=30)
    f = io.StringIO(res.text)
    reader = csv.reader(f)
    
    phish_urls = []
    for row in reader:
        if not row or row[0].startswith('#'): continue
        try:
            url_val = row[2]
            if row[3] == 'online' and url_val not in processed_urls:
                phish_urls.append(url_val)
        except: continue
        if len(phish_urls) >= LIMIT_URLS: break

    print(f"üì¶ S·ªë l∆∞·ª£ng URL c·∫ßn ch·∫°y m·ªõi: {len(phish_urls)}")

    # 3. Ch·∫°y ƒëa lu·ªìng
    if phish_urls:
        chunks = np.array_split(phish_urls, NUM_PROCESSES)
        final_temp = []
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            futures = [executor.submit(start_process, chunks[i].tolist(), i) for i in range(NUM_PROCESSES)]
            for f in futures:
                final_temp.extend(f.result())
        
        # L∆∞u n·ªët ph·∫ßn c√≤n l·∫°i
        if final_temp:
            pd.DataFrame(final_temp).to_csv(OUTPUT_FILE, mode='a', header=not os.path.exists(OUTPUT_FILE), index=False)

    # 4. D·ªçn d·∫πp cu·ªëi c√πng (S·∫Øp x·∫øp l·∫°i c·ªôt v√† x√≥a tr√πng)
    if os.path.exists(OUTPUT_FILE):
        df_final = pd.read_csv(OUTPUT_FILE)
        df_final.drop_duplicates(subset=['URL'], inplace=True)
        # L·ªçc ƒë√∫ng c√°c c·ªôt y√™u c·∫ßu (n·∫øu c√≥ c·ªôt r√°c ph√°t sinh)
        df_final = df_final[[c for c in COLUMNS if c in df_final.columns]]
        df_final.to_csv(OUTPUT_FILE, index=False)
        print(f"üèÅ T·ªîNG K·∫æT: ƒê√£ thu ho·∫°ch ƒë∆∞·ª£c {len(df_final)} m·∫´u Phishing.")
