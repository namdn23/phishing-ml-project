import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from urllib.parse import urlparse
from playwright.async_api import async_playwright

# =========================================================
# C·∫§U H√åNH H·ªÜ TH·ªêNG
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "processed_urls.log"
TOP_1M_FILE = "top-1m.csv"

CONCURRENT_PAGES = 30 # Gi·∫£m xu·ªëng 30 ƒë·ªÉ tr√°nh tr√†n RAM tr√™n Kali
TIMEOUT = 25000

# Danh s√°ch User-Agent x·ªãn ƒë·ªÉ gi·∫£ d·∫°ng tr√¨nh duy·ªát th·∫≠t
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

# =========================================================
# H√ÄM B·ªî TR·ª¢
# =========================================================
def load_processed_urls():
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, 'r') as f:
            return set(line.strip() for line in f)
    return set()

# Load Top 1M ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
try:
    top_1m_df = pd.read_csv(TOP_1M_FILE, header=None)
    top_1m_set = set(top_1m_df[1].astype(str).str.lower().values)
except:
    top_1m_set = set()
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y top-1m.csv")

def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    return {
        'domainEntropy': calculate_entropy(domain),
        'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
        'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
        'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
        'domainLength': len(domain),
        'Subdomain_Level': domain.count('.'),
        'IsHTTPS': 1 if url.startswith('https') else 0,
        'Is_Top_1M_Domain': 1 if any(d in top_1m_set for d in [domain, '.'.join(domain.split('.')[-2:])]) else 0
    }

async def intercept_route(route):
    # Ch·∫∑n ·∫£nh/font/media ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô c√†o g·∫•p 3 l·∫ßn
    if route.request.resource_type in ["image", "media", "font"]:
        await route.abort()
    else:
        try:
            await route.continue_()
        except:
            pass

# =========================================================
# TR√çCH XU·∫§T DYNAMIC FEATURES
# =========================================================
async def extract_dynamic(url, label, context, semaphore, log_handle):
    static_data = extract_static(url)

    if static_data['Is_Top_1M_Domain'] == 1:
        log_handle.write(f"{url}\n")
        log_handle.flush()
        return {**static_data, 'Outlink_Ratio': 0.0, 'HasExternalFormSubmit': 0, 'HasPasswordField': 0,
                'DomainTitleMatchScore': 1, 'HasSocialNet': 1, 'HasCopyrightInfo': 1, 'HasDescription': 1,
                'V9_Has_Hidden_IFrame': 0, 'V5_TLS_Issuer_Reputation': 1, 'V4_DNS_Volatility_Count': 0, 'label': label}

    async with semaphore:
        page = await context.new_page()
        # "Stealth th·ªß c√¥ng": V√¥ hi·ªáu h√≥a webdriver flag
        await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        await page.route("**/*", intercept_route)

        try:
            await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
            await asyncio.sleep(random.uniform(0.5, 1.5))

            domain = urlparse(url).netloc.lower()
            content = (await page.content()).lower()
            title = (await page.title()).lower()

            all_links = await page.query_selector_all('a')
            ext_links = 0
            for l in all_links:
                try:
                    h = await l.get_attribute('href')
                    if h and 'http' in h and domain not in h: ext_links += 1
                except: continue

            forms = await page.query_selector_all('form')
            ext_form = 0
            for f in forms:
                try:
                    act = await f.get_attribute('action')
                    if act and 'http' in act and domain not in act:
                        ext_form = 1
                        break
                except: continue

            dynamic_data = {
                'Outlink_Ratio': ext_links / len(all_links) if len(all_links) > 0 else 0,
                'HasExternalFormSubmit': ext_form,
                'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                'DomainTitleMatchScore': 1 if domain.split('.')[0] in title else 0,
                'HasSocialNet': 1 if any(s in content for s in ['facebook', 'twitter', 'linkedin']) else 0,
                'HasCopyrightInfo': 1 if ('¬©' in content or 'copyright' in content) else 0,
                'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
                'V9_Has_Hidden_IFrame': 1 if any(not await f.is_visible() for f in await page.query_selector_all('iframe')) else 0,
                'V5_TLS_Issuer_Reputation': 1, 'V4_DNS_Volatility_Count': 0
            }
        except Exception:
            # N·∫øu l·ªói (dead link), ƒëi·ªÅn gi√° tr·ªã trung gian 0.5
            dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                            'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 
                                            'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}
        finally:
            if not page.is_closed():
                await page.close()
            log_handle.write(f"{url}\n")
            log_handle.flush()

        return {**static_data, **dynamic_data, 'label': label}

# =========================================================
# MAIN
# =========================================================
async def main():
    start_time = time.time()
    processed_urls = load_processed_urls()

    if not os.path.exists(INPUT_FILE):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y {INPUT_FILE}")
        return

    df_all = pd.read_csv(INPUT_FILE)
    
    # T·ª± ƒë·ªông nh·∫≠n di·ªán c·ªôt URL (tr√°nh l·ªói vi·∫øt hoa/th∆∞·ªùng)
    url_col = [c for c in df_all.columns if 'url' in c.lower()][0]
    label_col = [c for c in df_all.columns if 'label' in c.lower()][0]
    
    df_to_do = df_all[~df_all[url_col].isin(processed_urls)]
    print(f"üöÄ C√≤n l·∫°i {len(df_to_do)} URL. ƒêang kh·ªüi ch·∫°y tr√¨nh duy·ªát...")

    semaphore = asyncio.Semaphore(CONCURRENT_PAGES)

    async with async_playwright() as p:
        # Kh·ªüi t·∫°o browser v·ªõi c√°c flag ·∫©n danh m·∫°nh m·∫Ω
        browser = await p.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--no-sandbox',
                '--disable-setuid-sandbox'
            ]
        )
        
        context = await browser.new_context(
            user_agent=random.choice(USER_AGENTS),
            ignore_https_errors=True,
            viewport={'width': 1920, 'height': 1080}
        )

        with open(LOG_FILE, 'a') as log_handle:
            for i in range(0, len(df_to_do), CONCURRENT_PAGES):
                batch = df_to_do.iloc[i:i + CONCURRENT_PAGES]
                tasks = [extract_dynamic(row[url_col], row[label_col], context, semaphore, log_handle) for _, row in batch.iterrows()]
                
                results = await asyncio.gather(*tasks)
                
                # L∆∞u ngay sau m·ªói batch ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu
                output_df = pd.DataFrame(results)
                output_df.to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
                
                elapsed = time.time() - start_time
                print(f"‚úÖ ƒê√£ xong {i + len(batch)} URL | T·ªëc ƒë·ªô: {( (i+len(batch)) / elapsed):.2f} URL/s")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
