import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright

# =========================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG - T·ªêI ∆ØU ƒê·ªò ·ªîN ƒê·ªäNH
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "processed_urls.log"
TOP_1M_FILE = "top-1m.csv"

# Gi·∫£m xu·ªëng 15 lu·ªìng ƒë·ªÉ tr√°nh b·ªã server ch·∫∑n IP (Rate Limit)
CONCURRENT_PAGES = 15 
TIMEOUT = 60000 # 60 gi√¢y ƒë·ªÉ ch·ªù c√°c server ·ªü xa ph·∫£n h·ªìi
MAX_RETRIES = 2 # Th·ª≠ l·∫°i t·ªëi ƒëa 2 l·∫ßn n·∫øu l·ªói

# =========================================================
# 2. H√ÄM B·ªî TR·ª¢ & ƒê·∫∂C TR∆ØNG Tƒ®NH
# =========================================================
def load_processed_urls():
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, 'r') as f:
            return set(line.split('|')[0].strip() for line in f if line.strip())
    return set()

try:
    top_1m_set = set(pd.read_csv(TOP_1M_FILE, header=None)[1].astype(str).str.lower().values)
except:
    top_1m_set = set()

def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    return {
        'domainEntropy': calculate_entropy(domain),
        'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
        'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
        'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
        'domainLength': len(domain),
        'Subdomain_Level': domain.count('.'),
        'IsHTTPS': 1 if url.startswith('https') else 0,
        'Is_Top_1M_Domain': 1 if any(d in top_1m_set for d in [domain, '.'.join(domain.split('.')[-2:])]) else 0
    }

async def intercept_route(route):
    if route.request.resource_type in ["image", "media", "font", "stylesheet"]:
        await route.abort()
    else:
        try: await route.continue_()
        except: pass

# =========================================================
# 3. LOGIC TR√çCH XU·∫§T ƒê·ªòNG - C√ì RETRY & GI·∫¢ L·∫¨P NG∆Ø·ªúI TH·∫¨T
# =========================================================
async def extract_task(url, label, context, semaphore, log_handle):
    static_data = extract_static(url)
    status = "SKIP"
    dynamic_data = {}

    if static_data['Is_Top_1M_Domain'] == 1:
        dynamic_data = {
            'Outlink_Ratio': 0.0, 'HasExternalFormSubmit': 0, 'HasPasswordField': 0,
            'DomainTitleMatchScore': 1, 'HasSocialNet': 1, 'HasCopyrightInfo': 1, 'HasDescription': 1,
            'V9_Has_Hidden_IFrame': 0, 'V5_TLS_Issuer_Reputation': 1, 'V4_DNS_Volatility_Count': 0
        }
    else:
        async with semaphore:
            # C∆° ch·∫ø Retry
            for attempt in range(MAX_RETRIES):
                page = await context.new_page()
                await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                await page.route("**/*", intercept_route)

                try:
                    # Truy c·∫≠p trang v·ªõi timeout l·ªõn
                    response = await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
                    
                    if response and response.status < 400:
                        # Gi·∫£ l·∫≠p thao t√°c ng∆∞·ªùi d√πng: Cu·ªôn chu·ªôt nh·∫π
                        await page.mouse.wheel(0, 500)
                        await asyncio.sleep(2)

                        domain = urlparse(url).netloc.lower()
                        content = (await page.content()).lower()
                        title = (await page.title()).lower()

                        all_links = await page.query_selector_all('a')
                        ext_l = 0
                        for l in all_links:
                            try:
                                h = await l.get_attribute('href')
                                if h and 'http' in h and domain not in h: ext_l += 1
                            except: continue

                        dynamic_data = {
                            'Outlink_Ratio': ext_l / len(all_links) if len(all_links) > 0 else 0,
                            'HasExternalFormSubmit': 1 if await page.query_selector('form[action^="http"]') else 0,
                            'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                            'DomainTitleMatchScore': 1 if domain.split('.')[0] in title else 0,
                            'HasSocialNet': 1 if any(s in content for s in ['facebook', 'twitter', 'linkedin', 'instagram']) else 0,
                            'HasCopyrightInfo': 1 if ('¬©' in content or 'copyright' in content) else 0,
                            'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
                            'V9_Has_Hidden_IFrame': 1 if any(not await f.is_visible() for f in await page.query_selector_all('iframe')) else 0,
                            'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0,
                            'V4_DNS_Volatility_Count': 0
                        }
                        status = "SUCCESS"
                        await page.close()
                        break # Tho√°t v√≤ng l·∫∑p retry n·∫øu th√†nh c√¥ng
                    else:
                        raise Exception("Trang ph·∫£n h·ªìi l·ªói status code")

                except Exception:
                    status = "FAILED"
                    if not page.is_closed(): await page.close()
                    if attempt < MAX_RETRIES - 1:
                        await asyncio.sleep(3) # ƒê·ª£i 3 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i

            # N·∫øu sau c√°c l·∫ßn th·ª≠ v·∫´n h·ªèng th√¨ g√°n 0.5
            if status == "FAILED":
                dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                                'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 
                                                'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}

    log_handle.write(f"{url} | {status} | Out:{dynamic_data.get('Outlink_Ratio',0):.2f} | {datetime.now().strftime('%H:%M:%S')}\n")
    log_handle.flush()
    return {**static_data, **dynamic_data, 'label': label}

# =========================================================
# 4. MAIN
# =========================================================
async def main():
    start_time = time.time()
    processed_urls = load_processed_urls()
    if not os.path.exists(INPUT_FILE): return

    df_all = pd.read_csv(INPUT_FILE)
    url_col = [c for c in df_all.columns if 'url' in c.lower()][0]
    label_col = [c for c in df_all.columns if 'label' in c.lower()][0]
    df_to_do = df_all[~df_all[url_col].isin(processed_urls)]
    total_todo = len(df_to_do)
    
    print(f"üöÄ Ch·∫°y b·∫£n ·ªïn ƒë·ªãnh: {CONCURRENT_PAGES} lu·ªìng | Retry: {MAX_RETRIES} | C√≤n: {total_todo}")
    semaphore = asyncio.Semaphore(CONCURRENT_PAGES)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=['--disable-blink-features=AutomationControlled'])
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            extra_http_headers={'Accept-Language': 'en-US,en;q=0.9', 'Referer': 'https://www.google.com/'}
        )

        with open(LOG_FILE, 'a') as log_h:
            for i in range(0, total_todo, CONCURRENT_PAGES):
                batch = df_to_do.iloc[i : i + CONCURRENT_PAGES]
                tasks = [extract_task(row[url_col], row[label_col], context, semaphore, log_h) for _, row in batch.iterrows()]
                results = await asyncio.gather(*tasks)
                
                pd.DataFrame(results).to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
                
                done = i + len(batch)
                elapsed = time.time() - start_time
                speed = done / elapsed if elapsed > 0 else 0
                eta = str(timedelta(seconds=int((total_todo - done) / speed))) if speed > 0 else "N/A"
                print(f"‚úÖ Xong {done}/{total_todo} | Speed: {speed:.2f} u/s | ETA: {eta}")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
