import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# =========================================================
# 1. C·∫§U H√åNH SI√äU T·ªêC
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "detailed_process.log"
TOP_1M_FILE = "top-1m.csv"

NUM_PROCESSES = 3         # S·ªë c·ª≠a s·ªï tr√¨nh duy·ªát (Process)
PAGES_PER_PROCESS = 3     # S·ªë tab trong m·ªói c·ª≠a s·ªï (Semaphore)
TIMEOUT = 70000           # 70 gi√¢y

# =========================================================
# 2. H√ÄM TR√çCH XU·∫§T STATIC & DYNAMIC
# =========================================================

def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    return {
        'domainEntropy': calculate_entropy(domain),
        'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
        'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
        'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
        'domainLength': len(domain),
        'Subdomain_Level': domain.count('.'),
        'IsHTTPS': 1 if url.startswith('https') else 0
    }

async def apply_stealth(page):
    await page.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        window.chrome = { runtime: {} };
    """)

async def process_url(url, label, context, semaphore, log_file):
    static_data = extract_static(url)
    dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                    'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame']}
    status = "FAILED"
    check_info = ""

    async with semaphore:
        page = await context.new_page()
        await apply_stealth(page)
        try:
            # 1. Truy c·∫≠p
            await page.goto(url, timeout=TIMEOUT, wait_until="networkidle")
            
            # 2. Gi·∫£ l·∫≠p h√†nh vi (V∆∞·ª£t ch·∫∑n)
            await page.mouse.wheel(0, 500)
            await asyncio.sleep(random.uniform(4, 7))

            # 3. Tr√≠ch xu·∫•t
            title = (await page.title()) or "No Title"
            domain = urlparse(url).netloc.lower()
            content = (await page.content()).lower()

            # Feature V9 (IFrame ·∫©n)
            v9 = 1 if any(not await f.frame_element().is_visible() for f in page.frames[1:] if f.frame_element()) else 0
            
            # Outlink Ratio
            links = await page.query_selector_all('a')
            ext_l = 0
            for l in links:
                try:
                    h = await l.get_attribute('href')
                    if h and 'http' in h and domain not in h: ext_l += 1
                except: continue
            
            out_ratio = ext_l / len(links) if links else 0

            dynamic_data.update({
                'Outlink_Ratio': out_ratio,
                'V9_Has_Hidden_IFrame': v9,
                'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
                'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                'HasSocialNet': 1 if any(s in content for s in ['facebook', 'twitter', 'instagram']) else 0
            })
            status = "SUCCESS"
            check_info = f"T: {title[:20]} | Out: {out_ratio:.2f} | V9: {v9}"
        except Exception as e:
            check_info = str(e)[:30]
        finally:
            await page.close()
            
    # Ghi log k√®m d·ªØ li·ªáu ƒë·ªÉ check
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {status} | {url} | {check_info}\n")
    
    return {**static_data, **dynamic_data, 'URL': url, 'label': label}

# =========================================================
# 3. RUNNER & BATCH MANAGEMENT
# =========================================================

async def run_batch(df_batch, process_id):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False, args=['--disable-blink-features=AutomationControlled'])
        context = await browser.new_context(viewport={'width': 1280, 'height': 800})
        semaphore = asyncio.Semaphore(PAGES_PER_PROCESS)
        
        tasks = [process_url(row['URL'], row['label'], context, semaphore, LOG_FILE) for _, row in df_batch.iterrows()]
        results = await asyncio.gather(*tasks)
        
        await browser.close()
        return results

def process_entry(df_batch, process_id):
    return asyncio.run(run_batch(df_batch, process_id))

# =========================================================
# 4. EXECUTION
# =========================================================

if __name__ == "__main__":
    start_time = time.time()
    if not os.path.exists(INPUT_FILE):
        print("‚ùå Kh√¥ng t√¨m th·∫•y file ƒë·∫ßu v√†o!"); exit()

    df_all = pd.read_csv(INPUT_FILE)
    
    # Resume logic: Check log ƒë·ªÉ b·ªè qua URL ƒë√£ l√†m
    processed_urls = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.split(" | ")
                if len(parts) > 1: processed_urls.add(parts[1].strip())

    # L·ªçc l·∫•y nh·ªØng URL ch∆∞a x·ª≠ l√Ω
    df_todo = df_all[~df_all['URL'].isin(processed_urls)]
    total = len(df_todo)
    
    print(f"üöÄ T·ªïng c·ªông: {total} URL c·∫ßn x·ª≠ l√Ω. ƒêang d√πng {NUM_PROCESSES} Process.")

    batch_size = NUM_PROCESSES * 10  # M·ªói l·∫ßn l∆∞u file sau khi xong 30 URL
    for i in range(0, total, batch_size):
        current_batch = df_todo.iloc[i : i + batch_size]
        sub_batches = np.array_split(current_batch, NUM_PROCESSES)
        
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            futures = [executor.submit(process_entry, sub_batches[p], p) for p in range(NUM_PROCESSES)]
            batch_results = []
            for fut in futures:
                batch_results.extend(fut.result())
        
        # L∆∞u v√†o CSV ngay l·∫≠p t·ª©c (Append mode)
        df_res = pd.DataFrame(batch_results)
        df_res.to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
        
        # C·∫≠p nh·∫≠t ETA
        done = i + len(current_batch)
        elapsed = time.time() - start_time
        avg_speed = done / elapsed
        eta_seconds = int((total - done) / avg_speed)
        print(f"üèÅ Xong {done}/{total} | T·ªëc ƒë·ªô: {avg_speed:.2f} URL/s | ETA: {timedelta(seconds=eta_seconds)}")
