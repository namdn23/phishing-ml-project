import pandas as pd
import numpy as np
import requests
import whois
import ssl
import socket
import time
import json
import hashlib
import re
from datetime import datetime
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# ===================== CONFIG =====================
CONFIG = {
    'max_workers': 56,
    'timeout': 5,
    'max_retries': 2,
    'checkpoint_interval': 500,
    'output_dir': 'checkpoints',
    'rate_limit': {
        'whois': 5,
        'http': 20,
        'ssl': 50
    },
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
}

Path(CONFIG['output_dir']).mkdir(exist_ok=True)

# ===================== RATE LIMITER =====================
from collections import defaultdict
from threading import Lock

class RateLimiter:
    def __init__(self):
        self.locks = defaultdict(Lock)
        self.last_call = defaultdict(float)
    
    def wait(self, key, min_interval):
        with self.locks[key]:
            elapsed = time.time() - self.last_call[key]
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)
            self.last_call[key] = time.time()

rate_limiter = RateLimiter()

# ===================== UTILITY FUNCTIONS =====================

def safe_request(url, timeout=5, follow_redirects=True):
    """HTTP request v·ªõi retry + anti-detection"""
    session = requests.Session()
    
    for attempt in range(CONFIG['max_retries']):
        try:
            rate_limiter.wait('http', 1.0 / CONFIG['rate_limit']['http'])
            
            response = session.get(
                url,
                headers=CONFIG['headers'],
                timeout=timeout,
                allow_redirects=follow_redirects,
                verify=False
            )
            return response
        except Exception as e:
            if attempt == CONFIG['max_retries'] - 1:
                return None
            time.sleep(0.5 * (attempt + 1))
    return None

def extract_domain_parts(url):
    """Parse URL th√†nh c√°c ph·∫ßn"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc
        path = parsed.path
        
        parts = domain.split('.')
        
        return {
            'full_domain': domain,
            'subdomain': '.'.join(parts[:-2]) if len(parts) > 2 else '',
            'main_domain': parts[-2] if len(parts) >= 2 else '',
            'tld': parts[-1] if parts else '',
            'path': path,
            'scheme': parsed.scheme,
            'query': parsed.query
        }
    except:
        return None

# ===================== STATIC FEATURE EXTRACTORS (14) =====================

def extract_static_features(url):
    """
    Tr√≠ch xu·∫•t T·∫§T C·∫¢ 14 static features
    
    STATIC (14):
    1. Is_Top_1M_Domain ‚Üê T·ª´ v32 (gi·ªØ nguy√™n)
    2. Entropy_Subdomain ‚Üê T·ª´ v32 (V23_Entropy_Subdomain)
    3. Is_HTTPS ‚Üê T·ª´ v32 (IsHTTPS)
    4. Subdomain_Count ‚Üê Code m·ªõi
    5. Has_Phishing_Keyword ‚Üê Code m·ªõi (hasKeyword)
    6. Path_Depth ‚Üê Code m·ªõi
    7. URL_Length ‚Üê Code m·ªõi
    8. Levenshtein_Brand ‚Üê Code m·ªõi
    9. Digit_Ratio ‚Üê Code m·ªõi (thay NoOfDegitsInURL)
    10. Has_IP_Address ‚Üê TH√äM M·ªöI ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
    11. Suspicious_TLD ‚Üê TH√äM M·ªöI ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
    12. Brand_In_Subdomain ‚Üê TH√äM M·ªöI ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
    13. Has_At_Symbol ‚Üê TH√äM M·ªöI ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
    14. Special_Char_Ratio ‚Üê TH√äM M·ªöI ‚≠ê‚≠ê‚≠ê‚≠ê
    15. Prefix_Suffix_Domain ‚Üê TH√äM M·ªöI ‚≠ê‚≠ê‚≠ê‚≠ê
    
    NOTE: T·∫•t c·∫£ return values ph·∫£i l√† int ho·∫∑c float, KH√îNG PH·∫¢I string!
    """
    features = {}
    
    try:
        parts = extract_domain_parts(url)
        if not parts:
            # Return int -1, not string "-1"
            return {k: int(-1) for k in [
                'Subdomain_Count', 'Has_Phishing_Keyword', 'Path_Depth',
                'URL_Length', 'Levenshtein_Brand', 'Digit_Ratio',
                'Entropy_Subdomain', 'Is_HTTPS', 'Has_IP_Address',
                'Suspicious_TLD', 'Brand_In_Subdomain', 'Has_At_Symbol',
                'Special_Char_Ratio', 'Prefix_Suffix_Domain'
            ]}
        
        # 3. Is_HTTPS (t·ª´ v32: IsHTTPS) - int 0 or 1
        features['Is_HTTPS'] = int(1 if parts['scheme'] == 'https' else 0)
        
        # 4. Subdomain_Count - int
        subdomain = parts['subdomain']
        features['Subdomain_Count'] = int(subdomain.count('.') + 1 if subdomain else 0)
        
        # 5. Has_Phishing_Keyword (t·ª´ hasKeyword) - int 0 or 1
        PHISHING_KEYWORDS = [
            'login', 'signin', 'account', 'verify', 'secure', 'update',
            'bank', 'paypal', 'ebay', 'amazon', 'apple', 'microsoft',
            'password', 'confirm', 'suspended', 'locked', 'alert',
            'security', 'credential', 'validate', 'auth', 'verification',
            'billing', 'support', 'helpdesk', 'webscr', 'customer'
        ]
        url_lower = url.lower()
        features['Has_Phishing_Keyword'] = int(1 if any(kw in url_lower for kw in PHISHING_KEYWORDS) else 0)
        
        # 6. Path_Depth - int
        path = parts['path'].strip('/')
        features['Path_Depth'] = int(path.count('/') if path else 0)
        
        # 7. URL_Length - int
        features['URL_Length'] = int(len(url))
        
        # 8. Levenshtein_Brand - int
        try:
            from Levenshtein import distance
            
            BRANDS = [
                'google', 'facebook', 'paypal', 'amazon', 'microsoft',
                'apple', 'netflix', 'instagram', 'twitter', 'linkedin',
                'github', 'dropbox', 'yahoo', 'ebay', 'adobe', 'chase',
                'wellsfargo', 'bankofamerica', 'citibank', 'americanexpress',
                'visa', 'mastercard', 'discover', 'payoneer', 'stripe'
            ]
            
            main_domain = parts['main_domain'].lower()
            distances = [distance(main_domain, brand) for brand in BRANDS]
            features['Levenshtein_Brand'] = int(min(distances))
        except:
            # Fallback: simple character difference
            features['Levenshtein_Brand'] = int(10)
        
        # 9. Digit_Ratio (thay NoOfDegitsInURL) - float
        digits = sum(c.isdigit() for c in url)
        features['Digit_Ratio'] = float(round(digits / len(url), 4) if len(url) > 0 else 0.0)
        
        # 2. Entropy_Subdomain (t·ª´ v32: V23_Entropy_Subdomain) - float
        if subdomain:
            char_counts = Counter(subdomain)
            total_chars = len(subdomain)
            entropy = -sum((count/total_chars) * np.log2(count/total_chars) 
                          for count in char_counts.values())
            features['Entropy_Subdomain'] = float(round(entropy, 4))
        else:
            features['Entropy_Subdomain'] = float(0.0)
        
        # 10. Has_IP_Address ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - int 0 or 1
        # Check n·∫øu domain l√† IP address
        ip_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
        has_ip = 1 if re.search(ip_pattern, parts['full_domain']) else 0
        features['Has_IP_Address'] = int(has_ip)
        
        # 11. Suspicious_TLD ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - int 0 or 1
        SUSPICIOUS_TLDS = [
            'tk', 'ml', 'ga', 'cf', 'gq',  # Free TLDs
            'pw', 'cc', 'top', 'xyz', 'club',
            'work', 'date', 'racing', 'stream', 'download',
            'bid', 'win', 'faith', 'science', 'party'
        ]
        tld = parts['tld'].lower()
        features['Suspicious_TLD'] = int(1 if tld in SUSPICIOUS_TLDS else 0)
        
        # 12. Brand_In_Subdomain ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - int 0 or 1
        # Check n·∫øu subdomain ch·ª©a t√™n brand n·ªïi ti·∫øng (d·∫•u hi·ªáu phishing)
        MAJOR_BRANDS = [
            'paypal', 'amazon', 'microsoft', 'apple', 'google',
            'facebook', 'netflix', 'instagram', 'bank', 'secure',
            'login', 'account', 'verify'
        ]
        subdomain_lower = subdomain.lower()
        features['Brand_In_Subdomain'] = int(1 if any(brand in subdomain_lower for brand in MAJOR_BRANDS) else 0)
        
        # 13. Has_At_Symbol ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - int 0 or 1
        # '@' trong URL c√≥ th·ªÉ d√πng ƒë·ªÉ phishing (http://trusted.com@evil.com)
        features['Has_At_Symbol'] = int(1 if '@' in url else 0)
        
        # 14. Special_Char_Ratio ‚≠ê‚≠ê‚≠ê‚≠ê - float
        # T·ª∑ l·ªá k√Ω t·ª± ƒë·∫∑c bi·ªát (-, _, ~, etc)
        special_chars = sum(c in '-_~!@#$%^&*()+=[]{}|;:,.<>?/' for c in url)
        features['Special_Char_Ratio'] = float(round(special_chars / len(url), 4) if len(url) > 0 else 0.0)
        
        # 15. Prefix_Suffix_Domain ‚≠ê‚≠ê‚≠ê‚≠ê - int 0 or 1
        # Check n·∫øu domain c√≥ d·∫•u '-' (e.g., paypal-secure.com)
        main_domain = parts['main_domain']
        features['Prefix_Suffix_Domain'] = int(1 if '-' in main_domain else 0)
        
        return features
        
    except Exception as e:
        # Return int -1 for all on error
        return {k: int(-1) for k in [
            'Subdomain_Count', 'Has_Phishing_Keyword', 'Path_Depth',
            'URL_Length', 'Levenshtein_Brand', 'Digit_Ratio',
            'Entropy_Subdomain', 'Is_HTTPS', 'Has_IP_Address',
            'Suspicious_TLD', 'Brand_In_Subdomain', 'Has_At_Symbol',
            'Special_Char_Ratio', 'Prefix_Suffix_Domain'
        ]}

# ===================== DYNAMIC FEATURE EXTRACTORS (12) =====================

def extract_domain_age(url):
    """Domain_Age via WHOIS (t·ª´ code m·ªõi) - Returns int"""
    try:
        rate_limiter.wait('whois', 1.0 / CONFIG['rate_limit']['whois'])
        
        parts = extract_domain_parts(url)
        if not parts:
            return int(-1)
        
        domain = parts['full_domain']
        w = whois.whois(domain)
        
        created = w.creation_date
        if isinstance(created, list):
            created = created[0]
        
        if created:
            age_days = (datetime.now() - created).days
            return int(max(0, age_days))
        return int(-1)
    except:
        return int(-1)

def extract_certificate_age(url):
    """Certificate_Age via SSL (t·ª´ code m·ªõi) - Returns int"""
    try:
        rate_limiter.wait('ssl', 1.0 / CONFIG['rate_limit']['ssl'])
        
        parts = extract_domain_parts(url)
        if not parts or parts['scheme'] != 'https':
            return int(-1)
        
        domain = parts['full_domain']
        
        context = ssl.create_default_context()
        with socket.create_connection((domain, 443), timeout=CONFIG['timeout']) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                not_before = datetime.strptime(cert['notBefore'], '%b %d %H:%M:%S %Y %Z')
                age_days = (datetime.now() - not_before).days
                return int(max(0, age_days))
    except:
        return int(-1)

def extract_redirect_count(url):
    """Redirect_Count (t·ª´ code m·ªõi) - Returns int"""
    try:
        response = safe_request(url, follow_redirects=True)
        if response:
            return int(len(response.history))
        return int(-1)
    except:
        return int(-1)

def extract_html_features(url):
    """
    Tr√≠ch xu·∫•t: Favicon_Match, External_Links_Ratio, Has_Popup,
                Has_External_Form_Submit, Has_Submit_Button, Has_Password_Field,
                Total_IFrames, Has_Hidden_IFrame
    
    T·∫•t c·∫£ trong 1 request duy nh·∫•t!
    """
    features = {
        'Favicon_Match': -1,
        'External_Links_Ratio': -1,
        'Has_Popup': -1,
        'Has_External_Form_Submit': -1,
        'Has_Submit_Button': -1,
        'Has_Password_Field': -1,
        'Total_IFrames': -1,
        'Has_Hidden_IFrame': -1
    }
    
    try:
        response = safe_request(url, follow_redirects=False)
        if not response or response.status_code != 200:
            return features
        
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')
        parts = extract_domain_parts(url)
        if not parts:
            return features
        
        original_domain = parts['full_domain']
        
        # 1. Favicon_Match (t·ª´ code m·ªõi)
        favicon = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')
        if favicon and favicon.get('href'):
            favicon_url = favicon['href']
            
            if not favicon_url.startswith('http'):
                from urllib.parse import urljoin
                favicon_url = urljoin(url, favicon_url)
            
            favicon_parts = extract_domain_parts(favicon_url)
            if favicon_parts:
                favicon_domain = favicon_parts['full_domain']
                features['Favicon_Match'] = 1 if (favicon_domain == original_domain or not favicon_domain) else 0
            else:
                features['Favicon_Match'] = 0
        else:
            features['Favicon_Match'] = 0
        
        # 2. External_Links_Ratio (t·ª´ code m·ªõi)
        all_links = soup.find_all('a', href=True)
        if all_links:
            external = sum(1 for link in all_links if original_domain not in link.get('href', ''))
            features['External_Links_Ratio'] = round(external / len(all_links), 4)
        else:
            features['External_Links_Ratio'] = 0
        
        # 3. Has_Popup (t·ª´ code m·ªõi)
        html_lower = html.lower()
        popup_keywords = [
            'window.open(', 'popup', 'modal', 'overlay',
            'alert(', 'confirm(', 'prompt(', 'showmodal'
        ]
        features['Has_Popup'] = 1 if any(kw in html_lower for kw in popup_keywords) else 0
        
        # 4. Has_External_Form_Submit (t·ª´ v32: HasExternalFormSubmit)
        forms = soup.find_all('form')
        has_external_form = 0
        for form in forms:
            action = form.get('action', '')
            if action and action.startswith('http') and original_domain not in action:
                has_external_form = 1
                break
        features['Has_External_Form_Submit'] = has_external_form
        
        # 5. Has_Submit_Button (t·ª´ v32: HasSubmitButton)
        submit_buttons = soup.find_all(['input', 'button'], type='submit')
        features['Has_Submit_Button'] = 1 if submit_buttons else 0
        
        # 6. Has_Password_Field (t·ª´ v32: HasPasswordField)
        password_fields = soup.find_all('input', type='password')
        features['Has_Password_Field'] = 1 if password_fields else 0
        
        # 7. Total_IFrames (t·ª´ v32: V8_Total_IFrames)
        iframes = soup.find_all('iframe')
        features['Total_IFrames'] = len(iframes)
        
        # 8. Has_Hidden_IFrame (t·ª´ v32: V9_Has_Hidden_IFrame)
        has_hidden = 0
        for iframe in iframes:
            style = iframe.get('style', '').lower()
            width = iframe.get('width', '')
            height = iframe.get('height', '')
            
            if ('display:none' in style or 'visibility:hidden' in style or
                width == '0' or height == '0'):
                has_hidden = 1
                break
        features['Has_Hidden_IFrame'] = has_hidden
        
        return features
        
    except Exception as e:
        return features

def extract_tls_issuer_reputation(url):
    """
    TLS_Issuer_Reputation (t·ª´ v32: V5_TLS_Issuer_Reputation)
    1 = Trusted CA, 0 = Unknown/Self-signed, -1 = Error/No HTTPS
    """
    try:
        rate_limiter.wait('ssl', 1.0 / CONFIG['rate_limit']['ssl'])
        
        parts = extract_domain_parts(url)
        if not parts or parts['scheme'] != 'https':
            return -1
        
        domain = parts['full_domain']
        
        context = ssl.create_default_context()
        with socket.create_connection((domain, 443), timeout=CONFIG['timeout']) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                
                # Check issuer
                issuer = dict(x[0] for x in cert['issuer'])
                issuer_org = issuer.get('organizationName', '').lower()
                
                # Trusted CAs
                TRUSTED_CAS = [
                    'let\'s encrypt', 'digicert', 'comodo', 'godaddy',
                    'globalsign', 'thawte', 'geotrust', 'sectigo',
                    'entrust', 'certum', 'amazon', 'google', 'microsoft'
                ]
                
                if any(ca in issuer_org for ca in TRUSTED_CAS):
                    return 1
                else:
                    return 0
    except:
        return -1

def extract_dynamic_features(url):
    """
    Tr√≠ch xu·∫•t T·∫§T C·∫¢ 12 dynamic features
    
    DYNAMIC (12):
    1. Has_External_Form_Submit ‚Üê v32
    2. Has_Submit_Button ‚Üê v32
    3. Has_Password_Field ‚Üê v32
    4. Total_IFrames ‚Üê v32
    5. Has_Hidden_IFrame ‚Üê v32
    6. TLS_Issuer_Reputation ‚Üê v32
    7. Domain_Age ‚Üê Code m·ªõi
    8. Certificate_Age ‚Üê Code m·ªõi
    9. Redirect_Count ‚Üê Code m·ªõi
    10. Favicon_Match ‚Üê Code m·ªõi
    11. External_Links_Ratio ‚Üê Code m·ªõi
    12. Has_Popup ‚Üê Code m·ªõi
    """
    features = {}
    
    try:
        # HTML features (1 request cho 8 features!)
        html_features = extract_html_features(url)
        features.update(html_features)
        
        # Redirect count
        features['Redirect_Count'] = extract_redirect_count(url)
        
        # TLS Issuer Reputation
        features['TLS_Issuer_Reputation'] = extract_tls_issuer_reputation(url)
        
        # Domain age (WHOIS - slow)
        features['Domain_Age'] = extract_domain_age(url)
        
        # Certificate age (SSL)
        features['Certificate_Age'] = extract_certificate_age(url)
        
        return features
        
    except Exception as e:
        return {
            'Favicon_Match': -1,
            'External_Links_Ratio': -1,
            'Has_Popup': -1,
            'Redirect_Count': -1,
            'Domain_Age': -1,
            'Certificate_Age': -1,
            'Has_External_Form_Submit': -1,
            'Has_Submit_Button': -1,
            'Has_Password_Field': -1,
            'Total_IFrames': -1,
            'Has_Hidden_IFrame': -1,
            'TLS_Issuer_Reputation': -1
        }

# ===================== MAIN EXTRACTOR =====================

def extract_all_features(row):
    """
    Tr√≠ch xu·∫•t T·∫§T C·∫¢ 26 features cho 1 row
    Input: pandas Series (1 row)
    Output: dict v·ªõi 26 features + metadata
    """
    # CRITICAL: Ensure URL is properly extracted
    if isinstance(row, pd.Series):
        url = str(row.get('url', ''))
    elif isinstance(row, dict):
        url = str(row.get('url', ''))
    else:
        url = str(row) if row else ''
    
    # Validate URL
    if not url or url == '' or url == 'nan':
        print(f"‚ö†Ô∏è  WARNING: Empty URL detected in row")
        url = 'INVALID_URL'
    
    result = {
        'url': url,
        'timestamp': datetime.now().isoformat(),
        'status': 'pending'
    }
    
    try:
        # 1. STATIC features (14 features - instant)
        static = extract_static_features(url)
        result.update(static)
        
        # 2. DYNAMIC features (12 features - slow)
        dynamic = extract_dynamic_features(url)
        result.update(dynamic)
        
        # 3. Is_Top_1M_Domain (c·∫ßn load t·ª´ v32 ho·∫∑c t√≠nh to√°n ri√™ng)
        # Gi·∫£ s·ª≠ gi·ªØ t·ª´ dataset g·ªëc
        result['Is_Top_1M_Domain'] = row.get('Is_Top_1M_Domain', -1)
        
        result['status'] = 'success'
        
    except Exception as e:
        result['status'] = 'error'
        result['error_message'] = str(e)
        
        # Fill missing v·ªõi -1
        missing_keys = [
            'Subdomain_Count', 'Has_Phishing_Keyword', 'Path_Depth',
            'URL_Length', 'Levenshtein_Brand', 'Digit_Ratio',
            'Entropy_Subdomain', 'Is_HTTPS', 'Has_IP_Address',
            'Suspicious_TLD', 'Brand_In_Subdomain', 'Has_At_Symbol',
            'Special_Char_Ratio', 'Prefix_Suffix_Domain',
            'Favicon_Match', 'External_Links_Ratio', 'Has_Popup',
            'Redirect_Count', 'Domain_Age', 'Certificate_Age',
            'Has_External_Form_Submit', 'Has_Submit_Button',
            'Has_Password_Field', 'Total_IFrames', 'Has_Hidden_IFrame',
            'TLS_Issuer_Reputation', 'Is_Top_1M_Domain'
        ]
        for key in missing_keys:
            if key not in result:
                result[key] = -1
    
    return result

# ===================== CHECKPOINT MANAGER =====================

class CheckpointManager:
    def __init__(self, output_dir, input_file):
        self.output_dir = Path(output_dir)
        self.input_file = input_file
        self.checkpoint_file = self.output_dir / f"checkpoint_{self._get_file_hash()}.json"
        self.results_file = self.output_dir / f"results_{self._get_file_hash()}.csv"
    
    def _get_file_hash(self):
        return hashlib.md5(self.input_file.encode()).hexdigest()[:8]
    
    def save_checkpoint(self, processed_indices, results):
        checkpoint_data = {
            'timestamp': datetime.now().isoformat(),
            'processed_indices': list(processed_indices),
            'total_processed': len(processed_indices),
        }
        
        with open(self.checkpoint_file, 'w') as f:
            json.dump(checkpoint_data, f)
        
        if results:
            df_results = pd.DataFrame(results)
            df_results.to_csv(self.results_file, index=False)
        
        return checkpoint_data
    
    def load_checkpoint(self):
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r') as f:
                data = json.load(f)
            
            if self.results_file.exists():
                df_results = pd.read_csv(self.results_file)
                results = df_results.to_dict('records')
            else:
                results = []
            
            return set(data['processed_indices']), results
        
        return set(), []
    
    def clear_checkpoint(self):
        if self.checkpoint_file.exists():
            self.checkpoint_file.unlink()

# ===================== BATCH PROCESSOR =====================

def process_batch(df, checkpoint_manager, skip_dynamic=False):
    """X·ª≠ l√Ω batch v·ªõi checkpoint & resume"""
    
    processed_indices, existing_results = checkpoint_manager.load_checkpoint()
    
    if processed_indices:
        print(f"üìÇ Found checkpoint: {len(processed_indices)} URLs already processed")
        remaining_df = df[~df.index.isin(processed_indices)]
    else:
        remaining_df = df
        existing_results = []
    
    if len(remaining_df) == 0:
        print("‚úÖ All URLs already processed!")
        return pd.DataFrame(existing_results)
    
    print(f"üöÄ Processing {len(remaining_df)} URLs with {CONFIG['max_workers']} workers...")
    if skip_dynamic:
        print("‚ö° FAST MODE: Only extracting static features (14)")
    else:
        print("üî• FULL MODE: Extracting all 26 features")
    
    results = existing_results.copy()
    start_time = time.time()
    processed_count = len(processed_indices)
    
    with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:
        futures = {}
        for idx, row in remaining_df.iterrows():
            # DEBUG: Log first 3 URLs
            if idx < 3:
                url_debug = row.get('url', 'NO_URL_COLUMN') if isinstance(row, pd.Series) else 'NOT_A_SERIES'
                print(f"üîç DEBUG Row {idx}: url='{url_debug}' | type={type(row)}")
            
            if skip_dynamic:
                future = executor.submit(extract_static_features, row.get('url', ''))
            else:
                future = executor.submit(extract_all_features, row)
            futures[future] = (idx, row)
        
        # Real-time metrics tracking
        success_count = 0
        error_count = 0
        checkpoint_count = 0
        
        with tqdm(total=len(remaining_df), desc="Extracting", initial=0) as pbar:
            for future in as_completed(futures):
                idx, row = futures[future]
                
                try:
                    result = future.result(timeout=60)
                    
                    if skip_dynamic:
                        result = {'url': row.get('url', ''), **result}
                    
                    results.append(result)
                    processed_indices.add(idx)
                    processed_count += 1
                    
                    # Track success/error
                    if result.get('status') == 'success':
                        success_count += 1
                    else:
                        error_count += 1
                    
                    # Checkpoint v·ªõi metrics chi ti·∫øt
                    if processed_count % CONFIG['checkpoint_interval'] == 0:
                        checkpoint_count += 1
                        checkpoint_manager.save_checkpoint(processed_indices, results)
                        
                        elapsed = time.time() - start_time
                        rate = processed_count / elapsed
                        remaining = len(df) - processed_count
                        eta_seconds = remaining / rate
                        eta_minutes = eta_seconds / 60
                        eta_hours = eta_minutes / 60
                        
                        success_rate = (success_count / processed_count * 100) if processed_count > 0 else 0
                        
                        print(f"\n")
                        print(f"{'='*70}")
                        print(f"üíæ CHECKPOINT #{checkpoint_count} SAVED")
                        print(f"{'='*70}")
                        print(f"üìä Progress: {processed_count:,}/{len(df):,} URLs ({processed_count/len(df)*100:.1f}%)")
                        print(f"‚úÖ Success: {success_count:,} ({success_rate:.1f}%)")
                        print(f"‚ùå Errors: {error_count:,} ({error_count/processed_count*100:.1f}%)")
                        print(f"‚ö° Current rate: {rate:.2f} URLs/second ({rate*60:.1f} URLs/min)")
                        print(f"‚è±Ô∏è  Elapsed: {elapsed/60:.1f} minutes ({elapsed/3600:.2f} hours)")
                        print(f"‚è∞ ETA: {eta_minutes:.1f} minutes ({eta_hours:.2f} hours)")
                        
                        # Calculate completion time
                        from datetime import timedelta
                        completion_time = datetime.now() + timedelta(seconds=eta_seconds)
                        print(f"üéØ Completion: {completion_time.strftime('%H:%M:%S')}")
                        print(f"{'='*70}\n")
                
                except Exception as e:
                    error_count += 1
                    print(f"\n‚ùå Error processing {row.get('url', 'unknown')}: {e}")
                    results.append({
                        'url': row.get('url', ''),
                        'status': 'timeout',
                        'error_message': str(e)
                    })
                
                pbar.update(1)
                
                # Update progress bar postfix m·ªói 50 URLs
                if processed_count % 50 == 0:
                    elapsed = time.time() - start_time
                    rate = processed_count / elapsed
                    remaining = len(df) - processed_count
                    eta_minutes = remaining / rate / 60
                    success_rate = (success_count / processed_count * 100) if processed_count > 0 else 0
                    
                    pbar.set_postfix({
                        'rate': f'{rate:.1f}/s',
                        'success': f'{success_rate:.0f}%',
                        'ETA': f'{eta_minutes:.0f}m'
                    })
    
    checkpoint_manager.save_checkpoint(processed_indices, results)
    
    elapsed = time.time() - start_time
    total_hours = elapsed / 3600
    total_minutes = elapsed / 60
    
    print(f"\n")
    print(f"{'='*70}")
    print(f"‚úÖ EXTRACTION COMPLETED!")
    print(f"{'='*70}")
    print(f"‚è±Ô∏è  Total time: {total_minutes:.1f} minutes ({total_hours:.2f} hours)")
    print(f"üìà Average rate: {len(df)/elapsed:.2f} URLs/second")
    print(f"üìä Total processed: {len(df):,} URLs")
    
    # Performance comparison
    if not skip_dynamic:
        theoretical_serial = len(df) * 10  # 10s per URL serial
        speedup = theoretical_serial / elapsed
        print(f"üöÄ Speedup vs serial: {speedup:.1f}x faster")
        print(f"   (Would take {theoretical_serial/3600:.1f} hours without parallelization)")
    
    print(f"{'='*70}\n")
    
    return pd.DataFrame(results)

# ===================== MAIN FUNCTION =====================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Feature Extraction v33 - 26 Features')
    parser.add_argument('--input', default='Dataset_v32_Final.csv', help='Input CSV')
    parser.add_argument('--output', default='Dataset_v33_Final.csv', help='Output CSV')
    parser.add_argument('--sample', type=int, help='Test v·ªõi N URLs')
    parser.add_argument('--skip-dynamic', action='store_true', help='Ch·ªâ extract static (14 features)')
    parser.add_argument('--resume', action='store_true', help='Resume t·ª´ checkpoint')
    parser.add_argument('--clear-checkpoint', action='store_true', help='X√≥a checkpoint c≈©')
    
    args = parser.parse_args()
    
    checkpoint_manager = CheckpointManager(CONFIG['output_dir'], args.input)
    
    if args.clear_checkpoint:
        checkpoint_manager.clear_checkpoint()
        print("üóëÔ∏è  Checkpoint cleared")
        return
    
    print(f"üìÇ Loading {args.input}...")
    df = pd.read_csv(args.input)
    print(f"‚úÖ Loaded {len(df)} rows")
    
    # CRITICAL FIX: Normalize column names - handle 'URL' vs 'url'
    print(f"\nüîç Checking columns...")
    print(f"   Original columns: {list(df.columns)}")
    
    # Check for URL column (case-insensitive)
    url_col = None
    for col in df.columns:
        if col.lower() == 'url':
            url_col = col
            break
    
    if url_col is None:
        print(f"\n‚ùå ERROR: No URL column found!")
        print(f"   Available columns: {list(df.columns)}")
        print(f"   Expected: 'url' or 'URL'")
        return
    
    # Rename to lowercase 'url' if needed
    if url_col != 'url':
        print(f"   üìù Renaming '{url_col}' ‚Üí 'url'")
        df.rename(columns={url_col: 'url'}, inplace=True)
    
    # Validate URLs are not empty
    empty_urls = df['url'].isna().sum()
    if empty_urls > 0:
        print(f"   ‚ö†Ô∏è  WARNING: {empty_urls} empty URLs detected - will skip")
        df = df[df['url'].notna()].copy()
    
    print(f"   ‚úÖ URL column ready: {len(df)} valid URLs")
    print(f"   Sample URLs: {df['url'].head(3).tolist()}")
    
    if args.sample:
        df = df.head(args.sample)
        print(f"‚ö†Ô∏è  Sample mode: {args.sample} URLs")
    
    # Estimate time v·ªõi breakdown chi ti·∫øt
    print(f"\n‚è±Ô∏è  TIME ESTIMATION:")
    print(f"   Workers: {CONFIG['max_workers']} parallel threads")
    print(f"   Total URLs: {len(df):,}")
    
    if not args.skip_dynamic:
        # FULL MODE - Chi ti·∫øt t·ª´ng lo·∫°i feature
        print(f"\n   üìä Feature extraction breakdown:")
        print(f"   ‚îú‚îÄ Static features (14):  ~0.001s/URL (instant)")
        print(f"   ‚îú‚îÄ HTML features (8):     ~2-3s/URL (1 request)")
        print(f"   ‚îú‚îÄ Redirect check (1):    ~1-2s/URL")
        print(f"   ‚îú‚îÄ TLS check (2):         ~1-2s/URL")
        print(f"   ‚îî‚îÄ WHOIS check (1):       ~3-5s/URL (slowest)")
        
        # T√≠nh to√°n d·ª±a tr√™n worst-case
        static_time = 0.001  # negligible
        html_time = 2.5      # HTML fetch + parse
        redirect_time = 1.5
        tls_time = 1.5       # TLS_Issuer + Certificate_Age
        whois_time = 4       # Domain_Age (slowest)
        
        total_per_url = static_time + html_time + redirect_time + tls_time + whois_time
        
        # V·ªõi parallelization
        serial_time = len(df) * total_per_url / 3600  # hours n·∫øu ch·∫°y tu·∫ßn t·ª±
        parallel_time = serial_time / CONFIG['max_workers']
        
        # Add overhead (network delays, rate limiting, retries)
        overhead_factor = 1.3
        estimated_hours = parallel_time * overhead_factor
        estimated_minutes = estimated_hours * 60
        
        print(f"\n   üî¢ Calculation:")
        print(f"   Serial time: {serial_time:.1f} hours")
        print(f"   √∑ {CONFIG['max_workers']} workers = {parallel_time:.1f} hours")
        print(f"   √ó {overhead_factor} overhead = {estimated_hours:.1f} hours")
        print(f"\n   ‚è∞ ESTIMATED TOTAL TIME: {estimated_hours:.1f} hours ({estimated_minutes:.0f} minutes)")
        
        # Checkpoint benefit
        checkpoint_interval = CONFIG['checkpoint_interval']
        time_per_checkpoint = (checkpoint_interval * total_per_url) / CONFIG['max_workers'] / 60
        print(f"\n   üíæ Checkpoint every {checkpoint_interval} URLs (~{time_per_checkpoint:.1f} min)")
        print(f"   ‚Üí Resume capability: Safe to interrupt anytime!")
        
        # Rate comparison
        urls_per_hour = (CONFIG['max_workers'] * 3600) / total_per_url
        print(f"\n   üìà Expected rate: ~{urls_per_hour:.0f} URLs/hour")
        print(f"                     ~{urls_per_hour/60:.1f} URLs/minute")
        
        # ETA breakdown
        if len(df) >= 1000:
            eta_1k = (1000 * total_per_url) / CONFIG['max_workers'] / 60
            print(f"\n   üéØ Milestones:")
            print(f"   First 1,000 URLs: ~{eta_1k:.0f} minutes")
            if len(df) >= 5000:
                eta_5k = (5000 * total_per_url) / CONFIG['max_workers'] / 60
                print(f"   First 5,000 URLs: ~{eta_5k:.0f} minutes")
            if len(df) >= 10000:
                eta_10k = (10000 * total_per_url) / CONFIG['max_workers'] / 60
                print(f"   First 10,000 URLs: ~{eta_10k/60:.1f} hours")
    else:
        # FAST MODE - Ch·ªâ static
        static_time = 0.001  # seconds per URL
        parallel_time = (len(df) * static_time) / CONFIG['max_workers'] / 60  # minutes
        
        print(f"\n   ‚ö° FAST MODE (Static only - 14 features):")
        print(f"   Processing time: ~{static_time*1000:.1f}ms/URL")
        print(f"   With {CONFIG['max_workers']} workers: ~{parallel_time:.1f} minutes")
        print(f"\n   ‚è∞ ESTIMATED TOTAL TIME: {parallel_time:.1f} minutes (INSTANT!)")
        print(f"   üìà Expected rate: ~{CONFIG['max_workers']*1000:.0f} URLs/second")
    
    print(f"\nüéØ Features to extract:")
    print(f"   üìå STATIC (instant): 14 features")
    if not args.skip_dynamic:
        print(f"   üîÑ DYNAMIC (slow): 12 features")
    print(f"   üìä TOTAL: {'26' if not args.skip_dynamic else '14'} features")
    
    print(f"\nüìã Feature List:")
    print(f"   STATIC: Is_Top_1M_Domain, Entropy_Subdomain, Is_HTTPS, Subdomain_Count,")
    print(f"           Has_Phishing_Keyword, Path_Depth, URL_Length, Levenshtein_Brand,")
    print(f"           Digit_Ratio, Has_IP_Address, Suspicious_TLD, Brand_In_Subdomain,")
    print(f"           Has_At_Symbol, Special_Char_Ratio, Prefix_Suffix_Domain")
    if not args.skip_dynamic:
        print(f"   DYNAMIC: Has_External_Form_Submit, Has_Submit_Button, Has_Password_Field,")
        print(f"            Total_IFrames, Has_Hidden_IFrame, TLS_Issuer_Reputation,")
        print(f"            Domain_Age, Certificate_Age, Redirect_Count, Favicon_Match,")
        print(f"            External_Links_Ratio, Has_Popup")
    
    input("\nPress ENTER to start (Ctrl+C to cancel)...")
    
    # Process
    df_new_features = process_batch(df, checkpoint_manager, skip_dynamic=args.skip_dynamic)
    
    # Merge v·ªõi DataFrame g·ªëc
    print("\nüîó Merging with original data...")
    
    # Ensure original df also has lowercase 'url'
    if 'url' not in df.columns:
        print("   ‚ö†Ô∏è  WARNING: 'url' column missing from original df")
    
    # Gi·ªØ label t·ª´ dataset g·ªëc
    if 'label' in df.columns:
        df_original = df[['url', 'label']].copy()
        
        # N·∫øu c√≥ Is_Top_1M_Domain trong v32, gi·ªØ l·∫°i
        if 'Is_Top_1M_Domain' in df.columns:
            df_original['Is_Top_1M_Domain'] = df['Is_Top_1M_Domain']
    else:
        df_original = df[['url']].copy()
    
    print(f"   Original data: {len(df_original)} rows")
    print(f"   New features: {len(df_new_features)} rows")
    
    # Merge
    df_final = df_original.merge(df_new_features, on='url', how='left')
    
    print(f"   Merged: {len(df_final)} rows")
    
    # Check for missing URLs after merge
    missing_urls = df_final['url'].isna().sum()
    if missing_urls > 0:
        print(f"   ‚ö†Ô∏è  WARNING: {missing_urls} URLs missing after merge")
    
    # Reorder columns theo th·ª© t·ª±: Static (14) + Dynamic (12) + label
    column_order = [
        'url',
        # STATIC (14)
        'Is_Top_1M_Domain',
        'Entropy_Subdomain',
        'Is_HTTPS',
        'Subdomain_Count',
        'Has_Phishing_Keyword',
        'Path_Depth',
        'URL_Length',
        'Levenshtein_Brand',
        'Digit_Ratio',
        'Has_IP_Address',
        'Suspicious_TLD',
        'Brand_In_Subdomain',
        'Has_At_Symbol',
        'Special_Char_Ratio',
        'Prefix_Suffix_Domain'
    ]
    
    if not args.skip_dynamic:
        column_order.extend([
            # DYNAMIC (12)
            'Has_External_Form_Submit',
            'Has_Submit_Button',
            'Has_Password_Field',
            'Total_IFrames',
            'Has_Hidden_IFrame',
            'TLS_Issuer_Reputation',
            'Domain_Age',
            'Certificate_Age',
            'Redirect_Count',
            'Favicon_Match',
            'External_Links_Ratio',
            'Has_Popup'
        ])
    
    if 'label' in df_final.columns:
        column_order.append('label')
    
    # Keep only existing columns
    column_order = [col for col in column_order if col in df_final.columns]
    df_final = df_final[column_order]
    
    # Save
    df_final.to_csv(args.output, index=False)
    print(f"\nüíæ Results saved to {args.output}")
    
    # Summary
    if 'status' in df_new_features.columns:
        success_count = (df_new_features['status'] == 'success').sum()
        error_count = len(df_new_features) - success_count
        
        print(f"\nüìä Summary:")
        print(f"   ‚úÖ Success: {success_count} ({success_count/len(df)*100:.1f}%)")
        print(f"   ‚ùå Errors: {error_count} ({error_count/len(df)*100:.1f}%)")
    
    # Feature stats
    if not args.skip_dynamic:
        print(f"\nüìà Feature Statistics:")
        for col in ['URL_Length', 'Domain_Age', 'Certificate_Age', 'Digit_Ratio']:
            if col in df_final.columns:
                valid = df_final[df_final[col] != -1][col]
                if len(valid) > 0:
                    print(f"   {col}: mean={valid.mean():.2f}, median={valid.median():.2f}")
    
    # Clear checkpoint
    checkpoint_manager.clear_checkpoint()
    print("\nüóëÔ∏è  Checkpoint cleared (extraction complete)")
    
    print(f"\nüéâ DONE! Dataset v33 v·ªõi {len(column_order)-2} features + label")

if __name__ == '__main__':
    main()
