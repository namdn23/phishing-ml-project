import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright

# =========================================================
# Cáº¤U HÃŒNH Há»† THá»NG
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "processed_urls.log"
TOP_1M_FILE = "top-1m.csv"

CONCURRENT_PAGES = 30 
TIMEOUT = 25000

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

# =========================================================
# HÃ€M Bá»” TRá»¢
# =========================================================
def load_processed_urls():
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, 'r') as f:
            # Chá»‰ láº¥y URL (pháº§n Ä‘áº§u trÆ°á»›c dáº¥u |)
            return set(line.split('|')[0].strip() for line in f if line.strip())
    return set()

try:
    top_1m_df = pd.read_csv(TOP_1M_FILE, header=None)
    top_1m_set = set(top_1m_df[1].astype(str).str.lower().values)
except:
    top_1m_set = set()

def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    return {
        'domainEntropy': calculate_entropy(domain),
        'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
        'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
        'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
        'domainLength': len(domain),
        'Subdomain_Level': domain.count('.'),
        'IsHTTPS': 1 if url.startswith('https') else 0,
        'Is_Top_1M_Domain': 1 if any(d in top_1m_set for d in [domain, '.'.join(domain.split('.')[-2:])]) else 0
    }

async def intercept_route(route):
    if route.request.resource_type in ["image", "media", "font"]:
        await route.abort()
    else:
        try: await route.continue_()
        except: pass

# =========================================================
# TRÃCH XUáº¤T DYNAMIC FEATURES
# =========================================================
async def extract_dynamic(url, label, context, semaphore, log_handle):
    static_data = extract_static(url)
    status = "SKIP"

    if static_data['Is_Top_1M_Domain'] == 1:
        dynamic_data = {
            'Outlink_Ratio': 0.0, 'HasExternalFormSubmit': 0, 'HasPasswordField': 0,
            'DomainTitleMatchScore': 1, 'HasSocialNet': 1, 'HasCopyrightInfo': 1, 'HasDescription': 1,
            'V9_Has_Hidden_IFrame': 0, 'V5_TLS_Issuer_Reputation': 1, 'V4_DNS_Volatility_Count': 0
        }
    else:
        async with semaphore:
            page = await context.new_page()
            await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            await page.route("**/*", intercept_route)
            try:
                await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
                await asyncio.sleep(1)
                domain = urlparse(url).netloc.lower()
                content = (await page.content()).lower()
                title = (await page.title()).lower()

                all_links = await page.query_selector_all('a')
                ext_links = 0
                for l in all_links:
                    try:
                        h = await l.get_attribute('href')
                        if h and 'http' in h and domain not in h: ext_links += 1
                    except: continue

                dynamic_data = {
                    'Outlink_Ratio': ext_links / len(all_links) if len(all_links) > 0 else 0,
                    'HasExternalFormSubmit': 1 if await page.query_selector('form[action^="http"]') else 0,
                    'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                    'DomainTitleMatchScore': 1 if domain.split('.')[0] in title else 0,
                    'HasSocialNet': 1 if any(s in content for s in ['facebook', 'twitter', 'linkedin']) else 0,
                    'HasCopyrightInfo': 1 if ('Â©' in content or 'copyright' in content) else 0,
                    'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
                    'V9_Has_Hidden_IFrame': 1 if any(not await f.is_visible() for f in await page.query_selector_all('iframe')) else 0,
                    'V5_TLS_Issuer_Reputation': 1, 'V4_DNS_Volatility_Count': 0
                }
                status = "SUCCESS"
            except:
                dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                                'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 
                                                'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}
                status = "FAILED"
            finally:
                if not page.is_closed(): await page.close()

    # Ghi log kÃ¨m káº¿t quáº£ tÃ³m táº¯t
    summary = f"Out:{dynamic_data['Outlink_Ratio']:.2f}|Social:{dynamic_data['HasSocialNet']}"
    log_handle.write(f"{url} | {status} | {summary} | {datetime.now().strftime('%H:%M:%S')}\n")
    log_handle.flush()
    
    return {**static_data, **dynamic_data, 'label': label}

# =========================================================
# MAIN
# =========================================================
async def main():
    start_time = time.time()
    processed_urls = load_processed_urls()

    if not os.path.exists(INPUT_FILE): return
    df_all = pd.read_csv(INPUT_FILE)
    url_col = [c for c in df_all.columns if 'url' in c.lower()][0]
    label_col = [c for c in df_all.columns if 'label' in c.lower()][0]
    
    df_to_do = df_all[~df_all[url_col].isin(processed_urls)]
    total_todo = len(df_to_do)
    print(f"ðŸš€ Báº¯t Ä‘áº§u: {total_todo} URL cÃ²n láº¡i")

    semaphore = asyncio.Semaphore(CONCURRENT_PAGES)
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=['--disable-blink-features=AutomationControlled'])
        context = await browser.new_context(user_agent=random.choice(USER_AGENTS), ignore_https_errors=True)

        with open(LOG_FILE, 'a') as log_handle:
            for i in range(0, total_todo, CONCURRENT_PAGES):
                batch = df_to_do.iloc[i:i + CONCURRENT_PAGES]
                tasks = [extract_dynamic(row[url_col], row[label_col], context, semaphore, log_handle) for _, row in batch.iterrows()]
                
                results = await asyncio.gather(*tasks)
                pd.DataFrame(results).to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
                
                # --- TÃNH TOÃN THá»œI GIAN Dá»° ÄOÃN ---
                done_so_far = i + len(batch)
                elapsed = time.time() - start_time
                speed = done_so_far / elapsed # URL per second
                remaining = total_todo - done_so_far
                eta_seconds = remaining / speed if speed > 0 else 0
                eta_str = str(timedelta(seconds=int(eta_seconds)))
                
                print(f"âœ… {done_so_far}/{total_todo} ({ (done_so_far/total_todo*100):.1f}%) | Tá»‘c Ä‘á»™: {speed:.2f} u/s | CÃ²n láº¡i: {eta_str}")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
