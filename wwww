import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import requests
import io
import csv
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# ================= CONFIGURATION =================
URLHAUS_URL = "https://urlhaus.abuse.ch/downloads/csv_online/"
OUTPUT_FILE = "Phishing_Web_24F.csv"
NUM_PROCESSES = 6       
LIMIT_URLS = 15000      
TIMEOUT_MS = 35000      
SAVE_INTERVAL = 50  # Checkpoint m·ªói 50 m·∫´u th√†nh c√¥ng

# Danh s√°ch ƒëu√¥i file c·∫ßn lo·∫°i b·ªè ƒë·ªÉ t·∫≠p trung v√†o trang web
FILE_BLACKLIST = (
    '.exe', '.bin', '.dll', '.msi', '.apk', '.zip', '.rar', '.7z', 
    '.tmp', '.lin', '.sh', '.bat', '.iso', '.dmg', '.gz', '.i', '.dat'
)

COLUMNS = [
    'URL', 'NoOfDegitsInURL', 'IsHTTPS', 'DomainTitleMatchScore', 'HasDescription', 
    'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasPasswordField', 
    'HasCopyrightInfo', 'label', 'V1_PHash_Distance', 'V2_Layout_Similarity', 
    'V6_JS_Entropy', 'V7_Text_Readability_Score', 'V8_Total_IFrames', 
    'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count', 
    'Is_Top_1M_Domain', 'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain', 
    'Trust_Score', 'Digit_Ratio'
]

# ================= LOGIC TR√çCH XU·∫§T =================
def calculate_entropy(text):
    if not text or len(text) == 0: return 0
    prob = [float(text.count(c)) / len(text) for c in dict.fromkeys(list(text))]
    return -sum(p * math.log(p, 2) for p in prob)

async def get_features(page, url):
    try:
        # 1. Truy c·∫≠p v√† ki·ªÉm tra n·∫øu l√† Web (text/html)
        response = await page.goto(url, timeout=TIMEOUT_MS, wait_until="domcontentloaded")
        ctype = response.headers.get('content-type', '').lower()
        if 'text/html' not in ctype:
            return None

        await asyncio.sleep(1.5)
        content = (await page.content()).lower()
        title = (await page.title()) or ""
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        visible_text = await page.inner_text("body")

        digits = sum(c.isdigit() for c in url)
        is_https = 1 if url.startswith('https') else 0
        
        # Ki·ªÉm tra Form submit ra ngo√†i
        ext_form = 0
        forms = await page.query_selector_all('form')
        for form in forms:
            action = await form.get_attribute('action')
            if action and action.startswith('http') and domain not in action:
                ext_form = 1; break

        return {
            'URL': url, 'NoOfDegitsInURL': digits, 'IsHTTPS': is_https,
            'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() else 0,
            'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
            'HasExternalFormSubmit': ext_form,
            'HasSocialNet': 1 if any(s in content for s in ['facebook.com', 'twitter.com', 'instagram.com']) else 0,
            'HasSubmitButton': 1 if await page.query_selector('input[type="submit"], button[type="submit"]') else 0,
            'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
            'HasCopyrightInfo': 1 if 'copyright' in content or '¬©' in content else 0,
            'label': 1, 'V1_PHash_Distance': 0, 'V2_Layout_Similarity': 0,
            'V6_JS_Entropy': calculate_entropy(content),
            'V7_Text_Readability_Score': len(visible_text.split()) / 400,
            'V8_Total_IFrames': len(await page.query_selector_all('iframe')),
            'V9_Has_Hidden_IFrame': 1 if await page.query_selector('iframe[style*="display:none"]') else 0,
            'V5_TLS_Issuer_Reputation': is_https, 'V4_DNS_Volatility_Count': 0,
            'Is_Top_1M_Domain': 0, 'V22_IP_Subdomain_Pattern': 1 if re.match(r'\d+\.\d+\.\d+', domain) else 0,
            'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
            'Trust_Score': is_https, 'Digit_Ratio': digits / len(url) if len(url) > 0 else 0
        }
    except: return None

async def worker(urls, proc_id):
    results = []
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox'])
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        for i, url in enumerate(urls):
            page = await context.new_page()
            data = await get_features(page, url)
            if data:
                results.append(data)
                if len(results) >= SAVE_INTERVAL:
                    pd.DataFrame(results).to_csv(OUTPUT_FILE, mode='a', header=not os.path.exists(OUTPUT_FILE), index=False)
                    results = []
                print(f"‚úÖ [P{proc_id}] Progress: {i+1}/{len(urls)} | Found Web")
            else:
                print(f"‚è© [P{proc_id}] Skip (File/Dead): {url[:30]}")
            await page.close()
        
        await browser.close()
    return results

def start_process(urls, proc_id):
    return asyncio.run(worker(urls, proc_id))

if __name__ == "__main__":
    # 1. Ki·ªÉm tra Checkpoint
    processed_urls = set()
    if os.path.exists(OUTPUT_FILE):
        try:
            old_df = pd.read_csv(OUTPUT_FILE)
            processed_urls = set(old_df['URL'].tolist())
            print(f"üîÑ Checkpoint: B·ªè qua {len(processed_urls)} m·∫´u ƒë√£ c√≥.")
        except: pass

    # 2. T·∫£i v√† l·ªçc Web-only URLs
    print("üì• ƒêang t·∫£i URLHaus v√† l·ªçc link WEB...")
    res = requests.get(URLHAUS_URL, timeout=30)
    reader = csv.reader(io.StringIO(res.text))
    
    phish_urls = []
    for row in reader:
        if not row or row[0].startswith('#'): continue
        try:
            url_val = row[2]
            # L·ªçc Online + Kh√¥ng ph·∫£i ƒëu√¥i file + Ch∆∞a x·ª≠ l√Ω
            if (row[3] == 'online' and 
                not url_val.lower().endswith(FILE_BLACKLIST) and 
                url_val not in processed_urls):
                phish_urls.append(url_val)
        except: continue
        if len(phish_urls) >= LIMIT_URLS: break

    print(f"üì¶ S·ªë l∆∞·ª£ng URL trang web c·∫ßn qu√©t: {len(phish_urls)}")

    # 3. Ch·∫°y ƒëa lu·ªìng
    if phish_urls:
        chunks = np.array_split(phish_urls, NUM_PROCESSES)
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            futures = [executor.submit(start_process, chunks[i].tolist(), i) for i in range(NUM_PROCESSES)]
            [f.result() for f in futures]

    # 4. D·ªçn d·∫πp & T·ªïng k·∫øt
    if os.path.exists(OUTPUT_FILE):
        df_final = pd.read_csv(OUTPUT_FILE)
        df_final.drop_duplicates(subset=['URL'], inplace=True)
        df_final = df_final[[c for c in COLUMNS if c in df_final.columns]]
        df_final.to_csv(OUTPUT_FILE, index=False)
        print(f"üèÅ XONG! ƒê√£ thu ho·∫°ch t·ªïng c·ªông {len(df_final)} m·∫´u Web Phishing.")
