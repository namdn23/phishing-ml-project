import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from playwright.async_api import async_playwright

# =========================================================
# 1. CẤU HÌNH HỆ THỐNG
# =========================================================
INPUT_FILE = "Dataset_Ready_to_Train.csv"
OUTPUT_FILE = "Dataset_18_Features_Final.csv"
LOG_FILE = "processed_urls.log"
TOP_1M_FILE = "top-1m.csv"

CONCURRENT_PAGES = 8  # Giảm xuống 8 luồng để cực kỳ an toàn trước Cloudflare
TIMEOUT = 65000       # 65 giây
MAX_RETRIES = 2

# Danh sách User-Agents thật để xoay vòng
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0"
]

# =========================================================
# 2. STEALTH LOGIC CAO CẤP (Vượt Cloudflare & Anti-Bot)
# =========================================================
async def apply_ultra_stealth(page):
    # Xóa sạch dấu vết Playwright/Automation
    await page.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        window.chrome = { runtime: {} };
        Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']});
        Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
        const getParameter = WebGLRenderingContext.prototype.getParameter;
        WebGLRenderingContext.prototype.getParameter = function(parameter) {
            if (parameter === 37445) return 'Intel Open Source Technology Center';
            if (parameter === 37446) return 'Mesa DRI Intel(R) UHD Graphics 630 (Coffeelake 3x8 GT2)';
            return getParameter(parameter);
        };
    """)

# =========================================================
# 3. TRÍCH XUẤT STATIC FEATURES (8 Cột)
# =========================================================
try:
    top_1m_set = set(pd.read_csv(TOP_1M_FILE, header=None)[1].astype(str).str.lower().values)
except:
    top_1m_set = set()

def calculate_entropy(text):
    if not text: return 0
    p_x = [float(text.count(chr(x))) / len(text) for x in range(256) if text.count(chr(x)) > 0]
    return -sum(p * math.log(p, 2) for p in p_x)

def extract_static(url):
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    return {
        'domainEntropy': calculate_entropy(domain),
        'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
        'hasIp': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', domain) else 0,
        'numHypRatio': domain.count('-') / len(domain) if len(domain) > 0 else 0,
        'domainLength': len(domain),
        'Subdomain_Level': domain.count('.'),
        'IsHTTPS': 1 if url.startswith('https') else 0,
        'Is_Top_1M_Domain': 1 if any(d in top_1m_set for d in [domain, '.'.join(domain.split('.')[-2:])]) else 0
    }

# =========================================================
# 4. TRÍCH XUẤT ĐỘNG (10 Cột) - VƯỚI CHUYỂN ĐỘNG CHUỘT
# =========================================================
async def extract_task(url, label, context, semaphore, log_handle):
    static_data = extract_static(url)
    status = "FAILED"
    dynamic_data = {f: 0.5 for f in ['Outlink_Ratio', 'HasExternalFormSubmit', 'HasPasswordField', 'DomainTitleMatchScore', 
                                    'HasSocialNet', 'HasCopyrightInfo', 'HasDescription', 'V9_Has_Hidden_IFrame', 
                                    'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count']}

    if static_data['Is_Top_1M_Domain'] == 1:
        status = "SKIP"
        dynamic_data = {k: 1.0 if k not in ['Outlink_Ratio', 'V9_Has_Hidden_IFrame', 'V4_DNS_Volatility_Count'] else 0.0 for k in dynamic_data.keys()}
    else:
        async with semaphore:
            for attempt in range(MAX_RETRIES):
                page = await context.new_page()
                await apply_ultra_stealth(page)
                
                try:
                    # Giả lập hành vi: Referer và Delay ngẫu nhiên
                    await page.set_extra_http_headers({"Referer": "https://www.google.com/"})
                    
                    # Truy cập trang
                    await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
                    
                    # --- KỸ THUẬT VƯỢT CLOUDFLARE ---
                    # 1. Di chuyển chuột ngẫu nhiên
                    await page.mouse.move(random.randint(100, 500), random.randint(100, 500))
                    # 2. Cuộn trang nhẹ để giả lập người đọc
                    await page.mouse.wheel(0, 400)
                    # 3. Chờ script bảo mật chạy xong
                    await asyncio.sleep(random.uniform(3, 6))

                    domain = urlparse(url).netloc.lower()
                    content = (await page.content()).lower()
                    title = (await page.title()).lower()

                    all_links = await page.query_selector_all('a')
                    ext_l = 0
                    for l in all_links:
                        try:
                            h = await l.get_attribute('href')
                            if h and 'http' in h and domain not in h: ext_l += 1
                        except: continue

                    dynamic_data = {
                        'Outlink_Ratio': ext_l / len(all_links) if len(all_links) > 0 else 0,
                        'HasExternalFormSubmit': 1 if await page.query_selector('form[action^="http"]') else 0,
                        'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
                        'DomainTitleMatchScore': 1 if domain.split('.')[0] in title else 0,
                        'HasSocialNet': 1 if any(s in content for s in ['facebook', 'twitter', 'linkedin', 'instagram']) else 0,
                        'HasCopyrightInfo': 1 if ('©' in content or 'copyright' in content) else 0,
                        'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
                        'V9_Has_Hidden_IFrame': 1 if any(not await f.is_visible() for f in await page.query_selector_all('iframe')) else 0,
                        'V5_TLS_Issuer_Reputation': 1 if url.startswith('https') else 0,
                        'V4_DNS_Volatility_Count': 0
                    }
                    status = "SUCCESS"
                    await page.close()
                    break
                except Exception:
                    if not page.is_closed(): await page.close()
                    await asyncio.sleep(random.uniform(2, 4))

    log_handle.write(f"{url} | {status} | {datetime.now().strftime('%H:%M:%S')}\n")
    log_handle.flush()
    return {**static_data, **dynamic_data, 'label': label}

# =========================================================
# 5. MAIN
# =========================================================
async def main():
    if not os.path.exists(INPUT_FILE): return
    df_all = pd.read_csv(INPUT_FILE)
    url_col = [c for c in df_all.columns if 'url' in c.lower()][0]
    label_col = [c for c in df_all.columns if 'label' in c.lower()][0]
    
    processed = set()
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, 'r') as f:
            for line in f: processed.add(line.split('|')[0].strip())
    
    df_to_do = df_all[~df_all[url_col].isin(processed)]
    semaphore = asyncio.Semaphore(CONCURRENT_PAGES)

    async with async_playwright() as p:
        # Tắt automation infobar
        browser = await p.chromium.launch(headless=True, args=['--disable-blink-features=AutomationControlled'])
        
        # Tạo context với User-Agent ngẫu nhiên
        context = await browser.new_context(
            user_agent=random.choice(USER_AGENTS),
            viewport={'width': 1920, 'height': 1080}
        )

        with open(LOG_FILE, 'a') as log_h:
            for i in range(0, len(df_to_do), CONCURRENT_PAGES):
                batch = df_to_do.iloc[i : i + CONCURRENT_PAGES]
                tasks = [extract_task(row[url_col], row[label_col], context, semaphore, log_h) for _, row in batch.iterrows()]
                results = await asyncio.gather(*tasks)
                pd.DataFrame(results).to_csv(OUTPUT_FILE, mode='a', index=False, header=not os.path.exists(OUTPUT_FILE))
                print(f"✅ Batch {i//CONCURRENT_PAGES + 1} xong. Tổng: {min(i + CONCURRENT_PAGES, len(df_to_do))}/{len(df_to_do)}")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
