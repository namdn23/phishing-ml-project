import pandas as pd
import numpy as np
import math
import re
import asyncio
import os
import requests
import io
import csv
import time
from urllib.parse import urlparse
from playwright.async_api import async_playwright
from concurrent.futures import ProcessPoolExecutor

# ================= CONFIGURATION =================
URLHAUS_URL = "https://urlhaus.abuse.ch/downloads/csv_online/"
OUTPUT_FILE = "Phishing_Web_24F_Final.csv"
NUM_PROCESSES = 12       # 27GB RAM cháº¡y 12-15 luá»“ng cá»±c mÆ°á»£t
LIMIT_URLS = 15000      
TIMEOUT_MS = 40000      
SAVE_INTERVAL = 30  

FILE_BLACKLIST = ('.exe', '.bin', '.dll', '.msi', '.apk', '.zip', '.rar', '.7z', '.tmp', '.lin', '.sh', '.bat', '.iso', '.dmg', '.gz', '.i', '.dat')

COLUMNS = [
    'URL', 'NoOfDegitsInURL', 'IsHTTPS', 'DomainTitleMatchScore', 'HasDescription', 
    'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasPasswordField', 
    'HasCopyrightInfo', 'label', 'V1_PHash_Distance', 'V2_Layout_Similarity', 
    'V6_JS_Entropy', 'V7_Text_Readability_Score', 'V8_Total_IFrames', 
    'V9_Has_Hidden_IFrame', 'V5_TLS_Issuer_Reputation', 'V4_DNS_Volatility_Count', 
    'Is_Top_1M_Domain', 'V22_IP_Subdomain_Pattern', 'V23_Entropy_Subdomain', 
    'Trust_Score', 'Digit_Ratio'
]

# ================= LOGIC TRÃCH XUáº¤T CHUáº¨N =================
def calculate_entropy(text):
    if not text: return 0
    prob = [float(text.count(c)) / len(text) for c in dict.fromkeys(list(text))]
    return -sum(p * math.log(p, 2) for p in prob)

async def get_features(page, url):
    try:
        # Chá»‘ng cháº·n: Giáº£ láº­p hÃ nh vi tháº­t
        await page.set_extra_http_headers({
            "Accept-Language": "en-US,en;q=0.9",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"
        })

        # Chá»‰ táº£i Script vÃ  HTML, bá» qua rÃ¡c Ä‘á»ƒ nhanh
        await page.route("**/*.{png,jpg,jpeg,gif,svg,css,woff,otf}", lambda route: route.abort())

        # GOTO vá»›i cÆ¡ cháº¿ Ä‘á»£i thÃ´ng minh
        response = await page.goto(url, timeout=TIMEOUT_MS, wait_until="networkidle")
        
        # Kiá»ƒm tra náº¿u lÃ  web (trÃ¡nh link táº£i file ngáº§m)
        ctype = response.headers.get('content-type', '').lower()
        if 'text/html' not in ctype: return None

        await asyncio.sleep(2) # Äá»£i thÃªm 2s cho JS lá»«a Ä‘áº£o render

        content = (await page.content()).lower()
        title = (await page.title()) or ""
        domain = urlparse(url).netloc.lower()
        visible_text = await page.evaluate("() => document.body.innerText")
        
        digits = sum(c.isdigit() for c in url)
        is_https = 1 if url.startswith('https') else 0

        # TrÃ­ch xuáº¥t 24 Ä‘áº·c trÆ°ng
        return {
            'URL': url, 
            'NoOfDegitsInURL': digits, 
            'IsHTTPS': is_https,
            'DomainTitleMatchScore': 1 if domain.split('.')[0] in title.lower() and len(title) > 0 else 0,
            'HasDescription': 1 if await page.query_selector('meta[name="description"]') else 0,
            'HasExternalFormSubmit': 1 if await page.query_selector('form[action^="http"]') else 0,
            'HasSocialNet': 1 if any(s in content for s in ['facebook.com', 'twitter.com', 'instagram.com']) else 0,
            'HasSubmitButton': 1 if await page.query_selector('input[type="submit"], button[type="submit"], button') else 0,
            'HasPasswordField': 1 if await page.query_selector('input[type="password"]') else 0,
            'HasCopyrightInfo': 1 if 'Â©' in content or 'copyright' in content else 0,
            'label': 1, 
            'V1_PHash_Distance': 0, 'V2_Layout_Similarity': 0,
            'V6_JS_Entropy': calculate_entropy(content),
            'V7_Text_Readability_Score': len(visible_text.split()) / 400 if visible_text else 0,
            'V8_Total_IFrames': len(await page.query_selector_all('iframe')),
            'V9_Has_Hidden_IFrame': 1 if await page.query_selector('iframe[style*="display:none"]') else 0,
            'V5_TLS_Issuer_Reputation': is_https, 
            'V4_DNS_Volatility_Count': 0, 'Is_Top_1M_Domain': 0, 
            'V22_IP_Subdomain_Pattern': 1 if re.match(r'\d+\.\d+\.\d+', domain) else 0,
            'V23_Entropy_Subdomain': calculate_entropy(domain.split('.')[0]),
            'Trust_Score': is_https, 
            'Digit_Ratio': digits / len(url) if len(url) > 0 else 0
        }
    except: return None

async def worker(urls, proc_id):
    results = []
    start_worker_time = time.time()
    async with async_playwright() as p:
        # CHá»NG CHáº¶N: ThÃªm args stealth
        browser = await p.chromium.launch(headless=True, args=[
            '--no-sandbox', 
            '--disable-blink-features=AutomationControlled',
            '--disable-infobars'
        ])
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        )
        
        for i, url in enumerate(urls):
            page = await context.new_page()
            data = await get_features(page, url)
            
            if data:
                results.append(data)
                if len(results) >= SAVE_INTERVAL:
                    pd.DataFrame(results).to_csv(OUTPUT_FILE, mode='a', header=not os.path.exists(OUTPUT_FILE), index=False)
                    results = []
                
                # TÃ­nh toÃ¡n thá»i gian
                elapsed = time.time() - start_worker_time
                speed = (i + 1) / elapsed # URLs per second
                rem_urls = len(urls) - (i + 1)
                rem_time = rem_urls / speed if speed > 0 else 0
                print(f"âœ… [P{proc_id}] Xong {i+1}/{len(urls)} | CÃ²n khoáº£ng: {rem_time/60:.1f} phÃºt")
            else:
                print(f"â© [P{proc_id}] Bá» qua (Link cháº¿t/File): {url[:30]}")
            
            await page.close()
        await browser.close()

def start_process(urls, proc_id):
    return asyncio.run(worker(urls, proc_id))

if __name__ == "__main__":
    # --- LOGIC CHECKPOINT ---
    processed_urls = set()
    if os.path.exists(OUTPUT_FILE):
        try:
            old_df = pd.read_csv(OUTPUT_FILE)
            processed_urls = set(old_df['URL'].tolist())
            print(f"ğŸ”„ Checkpoint: ÄÃ£ cÃ³ {len(processed_urls)} máº«u. Tiáº¿p tá»¥c cÃ o...")
        except: pass

    # --- Táº¢I DATA ---
    print("ğŸ“¥ Äang táº£i URLHaus...")
    res = requests.get(URLHAUS_URL, timeout=30)
    reader = csv.reader(io.StringIO(res.text))
    
    phish_urls = []
    for row in reader:
        if len(row) > 3 and row[3] == 'online':
            url = row[2]
            if not url.lower().endswith(FILE_BLACKLIST) and url not in processed_urls:
                phish_urls.append(url)
        if len(phish_urls) >= LIMIT_URLS: break

    # --- CHáº Y ÄA LUá»’NG ---
    print(f"ğŸš€ Tá»•ng cá»™ng {len(phish_urls)} URL. Cháº¡y trÃªn {NUM_PROCESSES} luá»“ng...")
    if phish_urls:
        chunks = np.array_split(phish_urls, NUM_PROCESSES)
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            [executor.submit(start_process, chunks[i].tolist(), i) for i in range(NUM_PROCESSES)]

    print(f"ğŸ Táº¤T Cáº¢ ÄÃƒ XONG! Káº¿t quáº£ táº¡i: {OUTPUT_FILE}")
